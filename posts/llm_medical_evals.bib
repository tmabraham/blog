
@misc{chen_meditron-70b_2023,
	title = {{MEDITRON}-{70B}: {Scaling} {Medical} {Pretraining} for {Large} {Language} {Models}},
	shorttitle = {{MEDITRON}-{70B}},
	url = {http://arxiv.org/abs/2311.16079},
	doi = {10.48550/arXiv.2311.16079},
	abstract = {Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale ({\textless}= 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6\% absolute performance gain over the best public baseline in its parameter class and 3\% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5\% of GPT-4 and 10\% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Chen, Zeming and Cano, Alejandro Hernández and Romanou, Angelika and Bonnet, Antoine and Matoba, Kyle and Salvi, Francesco and Pagliardini, Matteo and Fan, Simin and Köpf, Andreas and Mohtashami, Amirkeivan and Sallinen, Alexandre and Sakhaeirad, Alireza and Swamy, Vinitra and Krawczuk, Igor and Bayazit, Deniz and Marmet, Axel and Montariol, Syrielle and Hartley, Mary-Anne and Jaggi, Martin and Bosselut, Antoine},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16079 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\GKMVF28E\\Chen et al. - 2023 - MEDITRON-70B Scaling Medical Pretraining for Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\FR79F83Z\\2311.html:text/html},
}

@article{singhal_large_2023,
	title = {Large language models encode clinical knowledge},
	volume = {620},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06291-2},
	doi = {10.1038/s41586-023-06291-2},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6\% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17\%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
	language = {en},
	number = {7972},
	urldate = {2025-03-03},
	journal = {Nature},
	author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Babiker, Abubakr and Schärli, Nathanael and Chowdhery, Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and Agüera y Arcas, Blaise and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
	month = aug,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Health care, Medical research},
	pages = {172--180},
	file = {Full Text PDF:C\:\\Users\\tanli\\Zotero\\storage\\S7K83AEN\\Singhal et al. - 2023 - Large language models encode clinical knowledge.pdf:application/pdf},
}

@inproceedings{jin_pubmedqa_2019,
	address = {Hong Kong, China},
	title = {{PubMedQA}: {A} {Dataset} for {Biomedical} {Research} {Question} {Answering}},
	shorttitle = {{PubMedQA}},
	url = {https://aclanthology.org/D19-1259/},
	doi = {10.18653/v1/D19-1259},
	abstract = {We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1\% accuracy, compared to single human performance of 78.0\% accuracy and majority-baseline of 55.2\% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William and Lu, Xinghua},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {2567--2577},
	file = {Full Text PDF:C\:\\Users\\tanli\\Zotero\\storage\\3W2JM82I\\Jin et al. - 2019 - PubMedQA A Dataset for Biomedical Research Question Answering.pdf:application/pdf},
}

@misc{jin_what_2020,
	title = {What {Disease} does this {Patient} {Have}? {A} {Large}-scale {Open} {Domain} {Question} {Answering} {Dataset} from {Medical} {Exams}},
	shorttitle = {What {Disease} does this {Patient} {Have}?},
	url = {http://arxiv.org/abs/2009.13081},
	doi = {10.48550/arXiv.2009.13081},
	abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7{\textbackslash}\%, 42.0{\textbackslash}\%, and 70.1{\textbackslash}\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
	month = sep,
	year = {2020},
	note = {arXiv:2009.13081 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\BN84JJ2H\\Jin et al. - 2020 - What Disease does this Patient Have A Large-scale Open Domain Question Answering Dataset from Medic.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\GXY2SKRP\\2009.html:text/html},
}

@misc{pal_medmcqa_2022,
	title = {{MedMCQA} : {A} {Large}-scale {Multi}-{Subject} {Multi}-{Choice} {Dataset} for {Medical} domain {Question} {Answering}},
	shorttitle = {{MedMCQA}},
	url = {http://arxiv.org/abs/2203.14371},
	doi = {10.48550/arXiv.2203.14371},
	abstract = {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS {\textbackslash}\& NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects {\textbackslash}\& topics. A detailed explanation of the solution, along with the above information, is provided in this study.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.14371 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\PUI2RSUQ\\Pal et al. - 2022 - MedMCQA  A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\PWDSNM2Q\\2203.html:text/html},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	doi = {10.48550/arXiv.2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\4J55FCC3\\Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\5CWCCJ5Y\\2009.html:text/html},
}

@misc{saab_capabilities_2024,
	title = {Capabilities of {Gemini} {Models} in {Medicine}},
	url = {http://arxiv.org/abs/2404.18416},
	doi = {10.48550/arXiv.2404.18416},
	abstract = {Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data. Gemini models, with strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine. Building on these core strengths of Gemini, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly use web search, and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin. On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1\% accuracy, using a novel uncertainty-guided search strategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health \& medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5\%. We demonstrate the effectiveness of Med-Gemini's long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning. Finally, Med-Gemini's performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education. Taken together, our results offer compelling evidence for Med-Gemini's potential, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Saab, Khaled and Tu, Tao and Weng, Wei-Hung and Tanno, Ryutaro and Stutz, David and Wulczyn, Ellery and Zhang, Fan and Strother, Tim and Park, Chunjong and Vedadi, Elahe and Chaves, Juanma Zambrano and Hu, Szu-Yeu and Schaekermann, Mike and Kamath, Aishwarya and Cheng, Yong and Barrett, David G. T. and Cheung, Cathy and Mustafa, Basil and Palepu, Anil and McDuff, Daniel and Hou, Le and Golany, Tomer and Liu, Luyang and Alayrac, Jean-baptiste and Houlsby, Neil and Tomasev, Nenad and Freyberg, Jan and Lau, Charles and Kemp, Jonas and Lai, Jeremy and Azizi, Shekoofeh and Kanada, Kimberly and Man, SiWai and Kulkarni, Kavita and Sun, Ruoxi and Shakeri, Siamak and He, Luheng and Caine, Ben and Webson, Albert and Latysheva, Natasha and Johnson, Melvin and Mansfield, Philip and Lu, Jian and Rivlin, Ehud and Anderson, Jesper and Green, Bradley and Wong, Renee and Krause, Jonathan and Shlens, Jonathon and Dominowska, Ewa and Eslami, S. M. Ali and Chou, Katherine and Cui, Claire and Vinyals, Oriol and Kavukcuoglu, Koray and Manyika, James and Dean, Jeff and Hassabis, Demis and Matias, Yossi and Webster, Dale and Barral, Joelle and Corrado, Greg and Semturs, Christopher and Mahdavi, S. Sara and Gottweis, Juraj and Karthikesalingam, Alan and Natarajan, Vivek},
	month = may,
	year = {2024},
	note = {arXiv:2404.18416 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\JFCCDJNY\\Saab et al. - 2024 - Capabilities of Gemini Models in Medicine.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\9G3WWYNF\\2404.html:text/html},
}

@misc{mcduff_towards_2023,
	title = {Towards {Accurate} {Differential} {Diagnosis} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.00164},
	doi = {10.48550/arXiv.2312.00164},
	abstract = {An accurate differential diagnosis (DDx) is a cornerstone of medical care, often reached through an iterative process of interpretation that combines clinical history, physical examination, investigations and procedures. Interactive interfaces powered by Large Language Models (LLMs) present new opportunities to both assist and automate aspects of this process. In this study, we introduce an LLM optimized for diagnostic reasoning, and evaluate its ability to generate a DDx alone or as an aid to clinicians. 20 clinicians evaluated 302 challenging, real-world medical cases sourced from the New England Journal of Medicine (NEJM) case reports. Each case report was read by two clinicians, who were randomized to one of two assistive conditions: either assistance from search engines and standard medical resources, or LLM assistance in addition to these tools. All clinicians provided a baseline, unassisted DDx prior to using the respective assistive tools. Our LLM for DDx exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1\% vs 33.6\%, [p = 0.04]). Comparing the two assisted study arms, the DDx quality score was higher for clinicians assisted by our LLM (top-10 accuracy 51.7\%) compared to clinicians without its assistance (36.1\%) (McNemar's Test: 45.7, p {\textless} 0.01) and clinicians with search (44.4\%) (4.75, p = 0.03). Further, clinicians assisted by our LLM arrived at more comprehensive differential lists than those without its assistance. Our study suggests that our LLM for DDx has potential to improve clinicians' diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients' access to specialist-level expertise.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {McDuff, Daniel and Schaekermann, Mike and Tu, Tao and Palepu, Anil and Wang, Amy and Garrison, Jake and Singhal, Karan and Sharma, Yash and Azizi, Shekoofeh and Kulkarni, Kavita and Hou, Le and Cheng, Yong and Liu, Yun and Mahdavi, S. Sara and Prakash, Sushant and Pathak, Anupam and Semturs, Christopher and Patel, Shwetak and Webster, Dale R. and Dominowska, Ewa and Gottweis, Juraj and Barral, Joelle and Chou, Katherine and Corrado, Greg S. and Matias, Yossi and Sunshine, Jake and Karthikesalingam, Alan and Natarajan, Vivek},
	month = nov,
	year = {2023},
	note = {arXiv:2312.00164 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\L9SM94H9\\McDuff et al. - 2023 - Towards Accurate Differential Diagnosis with Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\Y2QQS77K\\2312.html:text/html},
}

@article{johri_evaluation_2025,
	title = {An evaluation framework for clinical use of large language models in patient interaction tasks},
	volume = {31},
	copyright = {2025 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-024-03328-5},
	doi = {10.1038/s41591-024-03328-5},
	abstract = {The integration of large language models (LLMs) into clinical diagnostics has the potential to transform doctor–patient interactions. However, the readiness of these models for real-world clinical application remains inadequately tested. This paper introduces the Conversational Reasoning Assessment Framework for Testing in Medicine (CRAFT-MD) approach for evaluating clinical LLMs. Unlike traditional methods that rely on structured medical examinations, CRAFT-MD focuses on natural dialogues, using simulated artificial intelligence agents to interact with LLMs in a controlled environment. We applied CRAFT-MD to assess the diagnostic capabilities of GPT-4, GPT-3.5, Mistral and LLaMA-2-7b across 12 medical specialties. Our experiments revealed critical insights into the limitations of current LLMs in terms of clinical conversational reasoning, history-taking and diagnostic accuracy. These limitations also persisted when analyzing multimodal conversational and visual assessment capabilities of GPT-4V. We propose a comprehensive set of recommendations for future evaluations of clinical LLMs based on our empirical findings. These recommendations emphasize realistic doctor–patient conversations, comprehensive history-taking, open-ended questioning and using a combination of automated and expert evaluations. The introduction of CRAFT-MD marks an advancement in testing of clinical LLMs, aiming to ensure that these models augment medical practice effectively and ethically.},
	language = {en},
	number = {1},
	urldate = {2025-03-03},
	journal = {Nature Medicine},
	author = {Johri, Shreya and Jeong, Jaehwan and Tran, Benjamin A. and Schlessinger, Daniel I. and Wongvibulsin, Shannon and Barnes, Leandra A. and Zhou, Hong-Yu and Cai, Zhuo Ran and Van Allen, Eliezer M. and Kim, David and Daneshjou, Roxana and Rajpurkar, Pranav},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Health care, Medical research},
	pages = {77--86},
}

@misc{tu_towards_2024,
	title = {Towards {Conversational} {Diagnostic} {AI}},
	url = {http://arxiv.org/abs/2401.05654},
	doi = {10.48550/arXiv.2401.05654},
	abstract = {At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue. AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Tu, Tao and Palepu, Anil and Schaekermann, Mike and Saab, Khaled and Freyberg, Jan and Tanno, Ryutaro and Wang, Amy and Li, Brenna and Amin, Mohamed and Tomasev, Nenad and Azizi, Shekoofeh and Singhal, Karan and Cheng, Yong and Hou, Le and Webson, Albert and Kulkarni, Kavita and Mahdavi, S. Sara and Semturs, Christopher and Gottweis, Juraj and Barral, Joelle and Chou, Katherine and Corrado, Greg S. and Matias, Yossi and Karthikesalingam, Alan and Natarajan, Vivek},
	month = jan,
	year = {2024},
	note = {arXiv:2401.05654 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\RRIWQ5VX\\Tu et al. - 2024 - Towards Conversational Diagnostic AI.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\MVXDPWF2\\2401.html:text/html},
}

@misc{schmidgall_agentclinic_2024,
	title = {{AgentClinic}: a multimodal agent benchmark to evaluate {AI} in simulated clinical environments},
	shorttitle = {{AgentClinic}},
	url = {http://arxiv.org/abs/2405.07960},
	doi = {10.48550/arXiv.2405.07960},
	abstract = {Evaluating large language models (LLM) in clinical scenarios is crucial to assessing their potential clinical utility. Existing benchmarks rely heavily on static question-answering, which does not accurately depict the complex, sequential nature of clinical decision-making. Here, we introduce AgentClinic, a multimodal agent benchmark for evaluating LLMs in simulated clinical environments that include patient interactions, multimodal data collection under incomplete information, and the usage of various tools, resulting in an in-depth evaluation across nine medical specialties and seven languages. We find that solving MedQA problems in the sequential decision-making format of AgentClinic is considerably more challenging, resulting in diagnostic accuracies that can drop to below a tenth of the original accuracy. Overall, we observe that agents sourced from Claude-3.5 outperform other LLM backbones in most settings. Nevertheless, we see stark differences in the LLMs' ability to make use of tools, such as experiential learning, adaptive retrieval, and reflection cycles. Strikingly, Llama-3 shows up to 92\% relative improvements with the notebook tool that allows for writing and editing notes that persist across cases. To further scrutinize our clinical simulations, we leverage real-world electronic health records, perform a clinical reader study, perturb agents with biases, and explore novel patient-centric metrics that this interactive environment firstly enables.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Schmidgall, Samuel and Ziaei, Rojin and Harris, Carl and Reis, Eduardo and Jopling, Jeffrey and Moor, Michael},
	month = oct,
	year = {2024},
	note = {arXiv:2405.07960 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\Z49TZGU7\\Schmidgall et al. - 2024 - AgentClinic a multimodal agent benchmark to evaluate AI in simulated clinical environments.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\2PJA788E\\2405.html:text/html},
}

@article{hager_evaluation_2024,
	title = {Evaluation and mitigation of the limitations of large language models in clinical decision-making},
	volume = {30},
	copyright = {2024 The Author(s)},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-024-03097-1},
	doi = {10.1038/s41591-024-03097-1},
	abstract = {Clinical decision-making is one of the most impactful parts of a physician’s responsibilities and stands to benefit greatly from artificial intelligence solutions and large language models (LLMs) in particular. However, while LLMs have achieved excellent performance on medical licensing exams, these tests fail to assess many skills necessary for deployment in a realistic clinical decision-making environment, including gathering information, adhering to guidelines, and integrating into clinical workflows. Here we have created a curated dataset based on the Medical Information Mart for Intensive Care database spanning 2,400 real patient cases and four common abdominal pathologies as well as a framework to simulate a realistic clinical setting. We show that current state-of-the-art LLMs do not accurately diagnose patients across all pathologies (performing significantly worse than physicians), follow neither diagnostic nor treatment guidelines, and cannot interpret laboratory results, thus posing a serious risk to the health of patients. Furthermore, we move beyond diagnostic accuracy and demonstrate that they cannot be easily integrated into existing workflows because they often fail to follow instructions and are sensitive to both the quantity and order of information. Overall, our analysis reveals that LLMs are currently not ready for autonomous clinical decision-making while providing a dataset and framework to guide future studies.},
	language = {en},
	number = {9},
	urldate = {2025-03-03},
	journal = {Nature Medicine},
	author = {Hager, Paul and Jungmann, Friederike and Holland, Robbie and Bhagat, Kunal and Hubrecht, Inga and Knauer, Manuel and Vielhauer, Jakob and Makowski, Marcus and Braren, Rickmer and Kaissis, Georgios and Rueckert, Daniel},
	month = sep,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diagnosis, Health care economics, Translational research},
	pages = {2613--2622},
	file = {Full Text PDF:C\:\\Users\\tanli\\Zotero\\storage\\MUWUV84I\\Hager et al. - 2024 - Evaluation and mitigation of the limitations of large language models in clinical decision-making.pdf:application/pdf},
}

@misc{jiang_medagentbench_2025,
	title = {{MedAgentBench}: {A} {Realistic} {Virtual} {EHR} {Environment} to {Benchmark} {Medical} {LLM} {Agents}},
	shorttitle = {{MedAgentBench}},
	url = {http://arxiv.org/abs/2501.14654},
	doi = {10.48550/arXiv.2501.14654},
	abstract = {Recent large language models (LLMs) have demonstrated significant advancements, particularly in their ability to serve as agents thereby surpassing their traditional role as chatbots. These agents can leverage their planning and tool utilization capabilities to address tasks specified at a high level. However, a standardized dataset to benchmark the agent capabilities of LLMs in medical applications is currently lacking, making the evaluation of LLMs on complex tasks in interactive healthcare environments challenging. To address this gap, we introduce MedAgentBench, a broad evaluation suite designed to assess the agent capabilities of large language models within medical records contexts. MedAgentBench encompasses 300 patient-specific clinically-derived tasks from 10 categories written by human physicians, realistic profiles of 100 patients with over 700,000 data elements, a FHIR-compliant interactive environment, and an accompanying codebase. The environment uses the standard APIs and communication infrastructure used in modern EMR systems, so it can be easily migrated into live EMR systems. MedAgentBench presents an unsaturated agent-oriented benchmark that current state-of-the-art LLMs exhibit some ability to succeed at. The best model (Claude 3.5 Sonnet v2) achieves a success rate of 69.67\%. However, there is still substantial space for improvement which gives the community a next direction to optimize. Furthermore, there is significant variation in performance across task categories. MedAgentBench establishes this and is publicly available at https://github.com/stanfordmlgroup/MedAgentBench , offering a valuable framework for model developers to track progress and drive continuous improvements in the agent capabilities of large language models within the medical domain.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Jiang, Yixing and Black, Kameron C. and Geng, Gloria and Park, Danny and Zou, James and Ng, Andrew Y. and Chen, Jonathan H.},
	month = feb,
	year = {2025},
	note = {arXiv:2501.14654 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\APTML4ZD\\Jiang et al. - 2025 - MedAgentBench A Realistic Virtual EHR Environment to Benchmark Medical LLM Agents.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\T6G5CD9V\\2501.html:text/html},
}

@misc{zuo_medxpertqa_2025,
	title = {{MedXpertQA}: {Benchmarking} {Expert}-{Level} {Medical} {Reasoning} and {Understanding}},
	shorttitle = {{MedXpertQA}},
	url = {http://arxiv.org/abs/2501.18362},
	doi = {10.48550/arXiv.2501.18362},
	abstract = {We introduce MedXpertQA, a highly challenging and comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA includes 4,460 questions spanning 17 specialties and 11 body systems. It includes two subsets, Text for text evaluation and MM for multimodal evaluation. Notably, MM introduces expert-level exam questions with diverse images and rich clinical information, including patient records and examination results, setting it apart from traditional medical multimodal benchmarks with simple QA pairs generated from image captions. MedXpertQA applies rigorous filtering and augmentation to address the insufficient difficulty of existing benchmarks like MedQA, and incorporates specialty board questions to improve clinical relevance and comprehensiveness. We perform data synthesis to mitigate data leakage risk and conduct multiple rounds of expert reviews to ensure accuracy and reliability. We evaluate 16 leading models on MedXpertQA. Moreover, medicine is deeply connected to real-world decision-making, providing a rich and representative setting for assessing reasoning abilities beyond mathematics and code. To this end, we develop a reasoning-oriented subset to facilitate the assessment of o1-like models.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Zuo, Yuxin and Qu, Shang and Li, Yifei and Chen, Zhangren and Zhu, Xuekai and Hua, Ermo and Zhang, Kaiyan and Ding, Ning and Zhou, Bowen},
	month = feb,
	year = {2025},
	note = {arXiv:2501.18362 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\NM7FSMSJ\\Zuo et al. - 2025 - MedXpertQA Benchmarking Expert-Level Medical Reasoning and Understanding.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\XFJTCULA\\2501.html:text/html},
}

@article{griot_large_2025,
	title = {Large {Language} {Models} lack essential metacognition for reliable medical reasoning},
	volume = {16},
	copyright = {2025 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-55628-6},
	doi = {10.1038/s41467-024-55628-6},
	abstract = {Large Language Models have demonstrated expert-level accuracy on medical board examinations, suggesting potential for clinical decision support systems. However, their metacognitive abilities, crucial for medical decision-making, remain largely unexplored. To address this gap, we developed MetaMedQA, a benchmark incorporating confidence scores and metacognitive tasks into multiple-choice medical questions. We evaluated twelve models on dimensions including confidence-based accuracy, missing answer recall, and unknown recall. Despite high accuracy on multiple-choice questions, our study revealed significant metacognitive deficiencies across all tested models. Models consistently failed to recognize their knowledge limitations and provided confident answers even when correct options were absent. In this work, we show that current models exhibit a critical disconnect between perceived and actual capabilities in medical reasoning, posing significant risks in clinical settings. Our findings emphasize the need for more robust evaluation frameworks that incorporate metacognitive abilities, essential for developing reliable Large Language Model enhanced clinical decision support systems.},
	language = {en},
	number = {1},
	urldate = {2025-03-03},
	journal = {Nature Communications},
	author = {Griot, Maxime and Hemptinne, Coralie and Vanderdonckt, Jean and Yuksel, Demet},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Health care, Machine learning, Medical research},
	pages = {642},
	file = {Full Text PDF:C\:\\Users\\tanli\\Zotero\\storage\\RFD9JQPC\\Griot et al. - 2025 - Large Language Models lack essential metacognition for reliable medical reasoning.pdf:application/pdf},
}

@misc{kim_limitations_2025,
	title = {Limitations of {Large} {Language} {Models} in {Clinical} {Problem}-{Solving} {Arising} from {Inflexible} {Reasoning}},
	url = {http://arxiv.org/abs/2502.04381},
	doi = {10.48550/arXiv.2502.04381},
	abstract = {Large Language Models (LLMs) have attained human-level accuracy on medical question-answer (QA) benchmarks. However, their limitations in navigating open-ended clinical scenarios have recently been shown, raising concerns about the robustness and generalizability of LLM reasoning across diverse, real-world medical tasks. To probe potential LLM failure modes in clinical problem-solving, we present the medical abstraction and reasoning corpus (M-ARC). M-ARC assesses clinical reasoning through scenarios designed to exploit the Einstellung effect -- the fixation of thought arising from prior experience, targeting LLM inductive biases toward inflexible pattern matching from their training data rather than engaging in flexible reasoning. We find that LLMs, including current state-of-the-art o1 and Gemini models, perform poorly compared to physicians on M-ARC, often demonstrating lack of commonsense medical reasoning and a propensity to hallucinate. In addition, uncertainty estimation analyses indicate that LLMs exhibit overconfidence in their answers, despite their limited accuracy. The failure modes revealed by M-ARC in LLM medical reasoning underscore the need to exercise caution when deploying these models in clinical settings.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Kim, Jonathan and Podlasek, Anna and Shidara, Kie and Liu, Feng and Alaa, Ahmed and Bernardo, Danilo},
	month = feb,
	year = {2025},
	note = {arXiv:2502.04381 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\B9ZZIVXZ\\Kim et al. - 2025 - Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\GVPEKMSH\\2502.html:text/html},
}

@misc{nori_medprompt_2024,
	title = {From {Medprompt} to o1: {Exploration} of {Run}-{Time} {Strategies} for {Medical} {Challenge} {Problems} and {Beyond}},
	shorttitle = {From {Medprompt} to o1},
	url = {http://arxiv.org/abs/2411.03590},
	doi = {10.48550/arXiv.2411.03590},
	abstract = {Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI's o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1's performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Nori, Harsha and Usuyama, Naoto and King, Nicholas and McKinney, Scott Mayer and Fernandes, Xavier and Zhang, Sheng and Horvitz, Eric},
	month = nov,
	year = {2024},
	note = {arXiv:2411.03590 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\RBYDRMHQ\\Nori et al. - 2024 - From Medprompt to o1 Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\YCGNX4VB\\2411.html:text/html},
}

@misc{brodeur_superhuman_2024,
	title = {Superhuman performance of a large language model on the reasoning tasks of a physician},
	url = {http://arxiv.org/abs/2412.10849},
	doi = {10.48550/arXiv.2412.10849},
	abstract = {Performance of large language models (LLMs) on medical tasks has traditionally been evaluated using multiple choice question benchmarks. However, such benchmarks are highly constrained, saturated with repeated impressive performance by LLMs, and have an unclear relationship to performance in real clinical scenarios. Clinical reasoning, the process by which physicians employ critical thinking to gather and synthesize clinical data to diagnose and manage medical problems, remains an attractive benchmark for model performance. Prior LLMs have shown promise in outperforming clinicians in routine and complex diagnostic scenarios. We sought to evaluate OpenAI's o1-preview model, a model developed to increase run-time via chain of thought processes prior to generating a response. We characterize the performance of o1-preview with five experiments including differential diagnosis generation, display of diagnostic reasoning, triage differential diagnosis, probabilistic reasoning, and management reasoning, adjudicated by physician experts with validated psychometrics. Our primary outcome was comparison of the o1-preview output to identical prior experiments that have historical human controls and benchmarks of previous LLMs. Significant improvements were observed with differential diagnosis generation and quality of diagnostic and management reasoning. No improvements were observed with probabilistic reasoning or triage differential diagnosis. This study highlights o1-preview's ability to perform strongly on tasks that require complex critical thinking such as diagnosis and management while its performance on probabilistic reasoning tasks was similar to past models. New robust benchmarks and scalable evaluation of LLM capabilities compared to human physicians are needed along with trials evaluating AI in real clinical settings.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Brodeur, Peter G. and Buckley, Thomas A. and Kanjee, Zahir and Goh, Ethan and Ling, Evelyn Bin and Jain, Priyank and Cabral, Stephanie and Abdulnour, Raja-Elie and Haimovich, Adrian and Freed, Jason A. and Olson, Andrew and Morgan, Daniel J. and Hom, Jason and Gallo, Robert and Horvitz, Eric and Chen, Jonathan and Manrai, Arjun K. and Rodman, Adam},
	month = dec,
	year = {2024},
	note = {arXiv:2412.10849 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tanli\\Zotero\\storage\\QUXSK47T\\Brodeur et al. - 2024 - Superhuman performance of a large language model on the reasoning tasks of a physician.pdf:application/pdf;Snapshot:C\:\\Users\\tanli\\Zotero\\storage\\9BABY9NN\\2412.html:text/html},
}
