<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tanishq Mathew Abraham, Ph.D.">
<meta name="dcterms.date" content="2023-09-22">
<meta name="description" content="Implementing the DDPO algorithm">

<title>Dr.&nbsp;Tanishq Abraham’s blog - Reinforcement Learning for Diffusion Models from Scratch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-LRXD97FB1E"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-LRXD97FB1E', { 'anonymize_ip': true});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Dr.&nbsp;Tanishq Abraham’s blog - Reinforcement Learning for Diffusion Models from Scratch">
<meta name="twitter:description" content="Implementing the DDPO algorithm">
<meta name="twitter:image" content="https://www.tanishq.ai/blog/posts/image-3.png">
<meta name="twitter:image-height" content="1473">
<meta name="twitter:image-width" content="1608">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Dr.&nbsp;Tanishq Abraham’s blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tmabraham"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/iScienceLuvr"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/tanishq-abraham-iscienceluvr/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Reinforcement Learning for Diffusion Models from Scratch</h1>
                  <div>
        <div class="description">
          Implementing the DDPO algorithm
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep learning</div>
                <div class="quarto-category">diffusion models</div>
                <div class="quarto-category">reinforcement learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tanishq Mathew Abraham, Ph.D. </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 22, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#what-if-we-could-optimize-the-aesthetic-score" id="toc-what-if-we-could-optimize-the-aesthetic-score" class="nav-link" data-scroll-target="#what-if-we-could-optimize-the-aesthetic-score">What if we could optimize the aesthetic score?</a>
  <ul class="collapse">
  <li><a href="#diffusion-model-refresher" id="toc-diffusion-model-refresher" class="nav-link" data-scroll-target="#diffusion-model-refresher">Diffusion model refresher</a></li>
  </ul></li>
  <li><a href="#the-ddpo-algorithm" id="toc-the-ddpo-algorithm" class="nav-link" data-scroll-target="#the-ddpo-algorithm">The DDPO algorithm</a></li>
  <li><a href="#complete-training-loop" id="toc-complete-training-loop" class="nav-link" data-scroll-target="#complete-training-loop">Complete training loop</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#drlx---a-library-for-performing-rlhf-training-with-diffusion-models" id="toc-drlx---a-library-for-performing-rlhf-training-with-diffusion-models" class="nav-link" data-scroll-target="#drlx---a-library-for-performing-rlhf-training-with-diffusion-models">DRLX - A library for performing RLHF training with diffusion models</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements">Acknowledgements</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Over the past year we have seen the rise of generative AI that has mainly come in two forms:</p>
<ol type="1">
<li><p>Text-to-image generation powered by diffusion models like Stable Diffusion and DALL-E 2</p></li>
<li><p>Language models like ChatGPT and LLaMA-2</p></li>
</ol>
<p>It turns out one of the key ingredients for the mainstream success of language models is the use of Reinforcement Learning from Human Feedback (RLHF) where language models are trained with human feedback to produce outputs that users are more likely to prefer. This has enabled these language models to more easily follow instructions, making these models significantly more accessible. Therefore the question arises if RLHF can be applied to diffusion models. This is a natural question to ask, since text-to-image diffusion models also struggle to follow prompts and tend to need prompt engineering skills in order to get desired results. A <a href="https://arxiv.org/abs/2305.13301">paper in May 2023</a> by the Levine Lab at UC Berkeley explored how the RLHF paradigm can be applied to diffusion models, resulting in an algorithm called DDPO. Here we’ll walk through a simple implementation of this DDPO algorithm. Let’s get started!</p>
<p>First let’s start with some basic imports:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> clip <span class="co"># pip install git+https://github.com/openai/CLIP.git</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> wandb</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusers <span class="im">import</span> StableDiffusionPipeline, DDIMScheduler</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastprogress <span class="im">import</span> progress_bar, master_bar</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s load our Stable Diffusion model. Let’s also enable some performance optimizations (TF32 support, attention slicing, memory-efficient xformers attention) that will make it faster to work with our Stable Diffusion model for training.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> StableDiffusionPipeline.from_pretrained(<span class="st">"CompVis/stable-diffusion-v1-4"</span>).to(<span class="st">"cuda"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>pipe.enable_attention_slicing()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>pipe.enable_xformers_memory_efficient_attention()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>pipe.text_encoder.requires_grad_(<span class="va">False</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>pipe.vae.requires_grad_(<span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4402b527944547f082da48072c7ae1cf","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.</code></pre>
</div>
</div>
<p>We’re using the <a href="https://huggingface.co/docs/diffusers">diffusers</a> library, which provides a simple-to-use interface for sampling the Stable Diffusion model using their pipeline:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a photograph of an astronaut riding a horse"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> pipe(prompt).images[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0d36ab4a0f7e49c7b6fd161f9020fce8","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<p><img src="ddpo_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Okay then, we want to improve the images coming out of our model. In order to do so we should have some sort of score for the image that we can later optimize for. This score could represent how aesthetic the image is. This is frankly something that is quite subjective, and there is no mathematical equation for the aestheticness of an image. Instead we will use <a href="https://github.com/LAION-AI/aesthetic-predictor">LAION’s aesthetic predictor</a>, which was trained on thousands of human aesthetic ratings of AI-generated images and is a linear model on top of CLIP features. Below is the standard inference code for the aesthetic predictor model:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, xcol<span class="op">=</span><span class="st">'emb'</span>, ycol<span class="op">=</span><span class="st">'avg_rating'</span>):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_size <span class="op">=</span> input_size</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.xcol <span class="op">=</span> xcol</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ycol <span class="op">=</span> ycol</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>.input_size, <span class="dv">1024</span>),</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>            <span class="co">#nn.ReLU(),</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1024</span>, <span class="dv">128</span>),</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>            <span class="co">#nn.ReLU(),</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>),</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            <span class="co">#nn.ReLU(),</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(<span class="fl">0.1</span>),</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">64</span>, <span class="dv">16</span>),</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>            <span class="co">#nn.ReLU(),</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">16</span>, <span class="dv">1</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layers(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_aesthetic_model_weights(cache<span class="op">=</span><span class="st">"."</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    weights_fname <span class="op">=</span> <span class="st">"sac+logos+ava1-l14-linearMSE.pth"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    loadpath <span class="op">=</span> os.path.join(cache, weights_fname)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.exists(loadpath):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        url <span class="op">=</span> (</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"https://github.com/christophschuhmann/"</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"improved-aesthetic-predictor/blob/main/</span><span class="sc">{</span>weights_fname<span class="sc">}</span><span class="ss">?raw=true"</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> requests.get(url)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(loadpath, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>            f.write(r.content)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> torch.load(loadpath, map_location<span class="op">=</span>torch.device(<span class="st">"cpu"</span>))</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> aesthetic_model_normalize(a, axis<span class="op">=-</span><span class="dv">1</span>, order<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    l2 <span class="op">=</span> np.atleast_1d(np.linalg.norm(a, order, axis))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    l2[l2 <span class="op">==</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">/</span> np.expand_dims(l2, axis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We need the CLIP model, whose features will be passed into our aesthetic predictor:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>clip_model, preprocess <span class="op">=</span> clip.load(<span class="st">"ViT-L/14"</span>, device<span class="op">=</span><span class="st">"cuda"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>aesthetic_model <span class="op">=</span> MLP(<span class="dv">768</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>aesthetic_model.load_state_dict(load_aesthetic_model_weights())</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>aesthetic_model.cuda()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>MLP(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): Dropout(p=0.2, inplace=False)
    (2): Linear(in_features=1024, out_features=128, bias=True)
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=64, bias=True)
    (5): Dropout(p=0.1, inplace=False)
    (6): Linear(in_features=64, out_features=16, bias=True)
    (7): Linear(in_features=16, out_features=1, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> preprocess(img).unsqueeze(<span class="dv">0</span>).cuda()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad(): image_features <span class="op">=</span> clip_model.encode_image(image)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>im_emb_arr <span class="op">=</span> aesthetic_model_normalize(image_features.cpu().detach().numpy())</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> aesthetic_model(torch.from_numpy(im_emb_arr).<span class="bu">float</span>().cuda())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Aesthetic score: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Aesthetic score: tensor([[5.6835]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>Just like, that, we get the aesthetic score given with this predictor. Let’s package this code into a function:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> preprocess(img).unsqueeze(<span class="dv">0</span>).cuda()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad(): image_features <span class="op">=</span> clip_model.encode_image(image)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    im_emb_arr <span class="op">=</span> aesthetic_model_normalize(image_features.cpu().detach().numpy())</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> aesthetic_model(torch.from_numpy(im_emb_arr).<span class="bu">float</span>().cuda())</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prediction</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a horse"</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> pipe(prompt).images[<span class="dv">0</span>]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Aesthetic score: </span><span class="sc">{</span>aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"df80acff5bb441a298742e19bb3f26b5","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Aesthetic score: 5.228103160858154</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="16">
<p><img src="ddpo_files/figure-html/cell-16-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a beautiful, exquisite portrait of a horse, 4k, unreal engine"</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> pipe(prompt).images[<span class="dv">0</span>]</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Aesthetic score: </span><span class="sc">{</span>aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b74b9e0df7ab473280e3b845e9367c36","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Aesthetic score: 5.3236589431762695</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="17">
<p><img src="ddpo_files/figure-html/cell-17-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a very ugly photograph of a donkey"</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> pipe(prompt).images[<span class="dv">0</span>]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Aesthetic score: </span><span class="sc">{</span>aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"20e0f7920f41400b8d5c52e1c7c68596","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Aesthetic score: 5.207968711853027</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="18">
<p><img src="ddpo_files/figure-html/cell-18-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>You can see if you prompt for something “ugly” we do get an image with a lower score. But the variation of the score doesn’t tend to be much there seems to be a sort of average aesthetic score that most images fluctuate around (~5.3). Can we increase the average aesthetic score of the images produced by Stable Diffusion? That is what we will attempt to do using reinforcement learning (RL).</p>
</section>
<section id="what-if-we-could-optimize-the-aesthetic-score" class="level2">
<h2 class="anchored" data-anchor-id="what-if-we-could-optimize-the-aesthetic-score">What if we could optimize the aesthetic score?</h2>
<p>Now that we have some sort of measure of quality of our image, our aesthetic score, we can optimize for it. In the RL literature, this measure of quality that we are optimizing for is referred to as the <strong>reward</strong>. The goal of RL algorithms is to optimize the reward. We will see how DDPO does this for diffusion models.</p>
<p>Before we go down the RL route though, it is worth examining if there are alternative approaches. Diffusion models, after all, are an extremely versatile framework, and people have been incorporating different constraints and forms of guidance during sampling in order to achieve desired results. Let’s do a quick refresher about diffusion models and how guidance is applied.</p>
<section id="diffusion-model-refresher" class="level3">
<h3 class="anchored" data-anchor-id="diffusion-model-refresher">Diffusion model refresher</h3>
<p>A diffusion model is described by a forward and reverse process. The forward process is when we start out with a clean image <span class="math inline">\(\mathbf{x}_0\)</span> and repeatedly add Gaussian noise <span class="math inline">\(\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> to give use noisier and noisier images <span class="math inline">\(\mathbf{x}_t\)</span>. This is described by the following:</p>
<p><span class="math display">\[ q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I}) \]</span></p>
<p>where <span class="math inline">\(\beta_t\)</span> is a predefined monotonically increasing variance schedule. The forward process runs for a total of <span class="math inline">\(T\)</span> timesteps and finally ends with pure noise <span class="math inline">\(\mathbf{x}_T\)</span>. The reverse process starts with pure noise <span class="math inline">\(\mathbf{x}_T\)</span> and uses a neural network to repeatedly denoise the image giving us <span class="math inline">\(\mathbf{x}_t\)</span>. The end of the reverse process gives us back our samples <span class="math inline">\(\mathbf{x}_0\)</span>. This is described as follows:</p>
<p><span class="math display">\[ p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \tilde{\beta}_t \mathbf{I}) \]</span></p>
<p>where <span class="math inline">\(\tilde{\beta}_t\)</span> is the variance schedule for the reverse schedule and <span class="math inline">\(\mu_\theta(\mathbf{x}_t, t)\)</span> is the denoising neural network. Note that the denoising neural network can be reparameterized to predict the noise <span class="math inline">\(\epsilon_t\)</span> in the image. So instead of predicting the denoised image <span class="math inline">\(\hat{\mathbf{x}}_0\)</span> directly, we can predict the noise in the image and subtract it out to get <span class="math inline">\(\hat{\mathbf{x}}_0\)</span>. We train the reparameterized denoising neural network <span class="math inline">\(\epsilon_\theta(\mathbf{x}_t, t)\)</span> in the reverse diffusion process with a simple MSE loss:</p>
<p><span class="math display">\[ L_{simple} = \mathbb{E}_{t,\mathbf{x}_t, \epsilon_t} || \epsilon_t - \epsilon_\theta(\mathbf{x}_t, t) || \]</span></p>
<p>In practice, training and sampling is quite simple. During each training step, a random image <span class="math inline">\(\mathbf{x}_0\)</span> and timestep <span class="math inline">\(t\)</span> is selected, the forward process starts from <span class="math inline">\(\mathbf{x}_0\)</span> till timestep <span class="math inline">\(t\)</span> to get <span class="math inline">\(\mathbf{x}_t\)</span> using the noise <span class="math inline">\(\epsilon_t\)</span>, this is passed into our denoising model, and the MSE between the <span class="math inline">\(\epsilon_t\)</span> used to calculate <span class="math inline">\(\mathbf{x}_t\)</span> and the predicted <span class="math inline">\(\epsilon_\theta(\mathbf{x}_t, t)\)</span> is optimized. During sampling, we start out with random Gaussian noise <span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> and the denoising neural network is repeatedly applied to give us <span class="math inline">\(\mathbf{x}_{t-1}\)</span> until we reach our sample <span class="math inline">\(\mathbf{x}_0\)</span>.</p>
<p>It is also worth noting the score matching intuition behind diffusion models. Specifically, <span class="math inline">\(\epsilon_\theta(\mathbf(x)_t, t)\)</span> turns out to actually be an estimate (up to a factor) of the “score function” <span class="math inline">\(\nabla_\mathbf{x} \log p(\mathbf{x})\)</span>. Basically, what this means is that when we sample from a diffusion model, we are iteratively taking steps in the direction of this score function, which is this gradient of the likelihood. So the sampling is very much like an optimization problem.</p>
<p>If any of this is unfamiliar to you, I recommend checking out the <a href="https://course.fast.ai/Lessons/part2.html">fast.ai course</a> on the subject (I am somewhat biased though given I co-taught the class!).</p>
<p>Let’s now discuss how additional constraints and guidance can be added during diffusion sampling. Basically, we want to model <span class="math inline">\(p(\mathbf{x} | \mathbf{y})\)</span> where <span class="math inline">\(\mathbf{y}\)</span> is some sort of condition or constraint (for example a class condition). In diffusion models, we could instead try to estimate <span class="math inline">\(\nabla_\mathbf{x} \log p(\mathbf{x} | \mathbf{y})\)</span> and use this in sampling. This can be expressed differently using Bayes’ Rule:</p>
<p><span class="math display">\[ \begin{gathered}
p(\mathbf{x} \mid  \mathbf{y})=\frac{p( \mathbf{y} \mid \mathbf{x}) \cdot p(\mathbf{x})}{p( \mathbf{y})} \\
\Longrightarrow \log p(\mathbf{x} \mid  \mathbf{y})=\log p( \mathbf{y} \mid \mathbf{x})+\log p(\mathbf{x})-\log p( \mathbf{y}) \\
\Longrightarrow \nabla_\mathbf{x} \log p(\mathbf{x} \mid  \mathbf{y})=\nabla_\mathbf{x} \log p( \mathbf{y} \mid \mathbf{x})+\nabla_\mathbf{x} \log p(\mathbf{x})
\end{gathered} \]</span></p>
<p>The second term is our score function that is already being estimated by our diffusion model <span class="math inline">\(\epsilon_\theta(\mathbf{x}_t, t)\)</span>. The first term, however, is the gradient of the log likelihood of a classifier <span class="math inline">\(p(\mathbf{x} \mid \mathbf{y})\)</span> with respect to <span class="math inline">\(\mathbf{x}\)</span>. What this means is that during diffusion model sampling, if we use a modified <span class="math inline">\(\hat{\epsilon}_\theta(\mathbf{x}_t,t)\)</span> with the classifier gradient added to it, we can get samples that adhere to the desired condition.</p>
<p>More broadly, losses can be applied to the noisy images and its gradient can be added to the score function/denoising network output to try to obtain images that better adhere to some desired constraints. This is the idea behind CLIP-guided diffusion, for example, where the similarity between the CLIP text embedding of a prompt and the CLIP image embedding of the images during sampling are maximized.</p>
<p>Can we use guidance to get diffusion models to generate more aesthetic images that better adhere to user prompts? Potentially yes, but there are many challenges that may make it undesirable.</p>
<p>Strictly speaking, the proper way to perform classifier guidance is to use a classifier trained on either the noisy images <span class="math inline">\(\mathbf{x}_t\)</span> or the predicted denoised images <span class="math inline">\(\hat{\mathbf{x}}_0\)</span> (which tend to be blurry, especially early on in sampling), which is what the original classifier guidance paper demonstrates. Note, it is possible to use classifiers and other models trained on regular images and get reasonable results for guidance, even though noisy or blurry images will likely be out-of-distribution. For example CLIP isn’t trained on noisy images but is used in CLIP-guided diffusion. But often to get reasonable results, various hacks and tricks are required (in the case of CLIP-guided diffusion, the use of this technique fell out of popularity once diffusion models properly conditioned on CLIP features like Stable Diffusion were developed, and CLIP guidance applied on top of models like Stable Diffusion often showed minimal benefit).</p>
<p>Additionally, note that guidance requires the calculation of whatever guidance loss we have and autograd of that loss at each step in the sampling process. This can add a significant overhead to the sampling time compared to guidance-free sampling. The situation is made worse with latent diffusion models like Stable Diffusion, where the latents often need to be decoded to full images in order to apply the classifier/loss, resulting in additional computational overhead.</p>
<p>For these reasons, we will instead apply reinforcement learning to obtain a diffusion model after optimizing arbitrary constraints (the reward function), such as aesthetic scores. As we will see, the generated images from the starting diffusion model are passed into the reward function, so there is no concern of images being out-of-distribution. Additionally, we will obtain a diffusion model that provides higher scoring images directly, not through sampling changes like guidance.</p>
<p>Okay let’s proceed with trying to apply RL to diffusion models. First we’ll create a dataset generator - Stable Diffusion generated images given some prompts. We’ll use animal prompts:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget https:<span class="op">//</span>raw.githubusercontent.com<span class="op">/</span>formigone<span class="op">/</span>tf<span class="op">-</span>imagenet<span class="op">/</span>master<span class="op">/</span>LOC_synset_mapping.txt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--2023-06-13 09:54:06--  https://raw.githubusercontent.com/formigone/tf-imagenet/master/LOC_synset_mapping.txt
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 31675 (31K) [text/plain]
Saving to: ‘LOC_synset_mapping.txt.14’

LOC_synset_mapping. 100%[===================&gt;]  30.93K  --.-KB/s    in 0.006s  

2023-06-13 09:54:06 (5.24 MB/s) - ‘LOC_synset_mapping.txt.14’ saved [31675/31675]
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>synsets <span class="op">=</span> {k:v <span class="cf">for</span> k,v <span class="kw">in</span> [o.split(<span class="st">','</span>)[<span class="dv">0</span>].split(<span class="st">' '</span>, maxsplit<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> o <span class="kw">in</span> Path(<span class="st">'LOC_synset_mapping.txt'</span>).read_text().splitlines()]}</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>imagenet_classes <span class="op">=</span> <span class="bu">list</span>(synsets.values())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> imagenet_animal_prompts():</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    animal <span class="op">=</span> random.choice(imagenet_classes[:<span class="dv">397</span>])</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    prompts <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>animal<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prompts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>imagenet_animal_prompts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>'sea urchin'</code></pre>
</div>
</div>
<p>Put into a dataset class:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PromptDataset(torch.utils.data.Dataset):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, prompt_fn, num):</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prompt_fn <span class="op">=</span> prompt_fn</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num <span class="op">=</span> num</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>): <span class="cf">return</span> <span class="va">self</span>.num</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, x): <span class="cf">return</span> <span class="va">self</span>.prompt_fn()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next let’s set up our sampling loop. For simplicity, we’ll just use the DDIM scheduler:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>pipe.scheduler <span class="op">=</span> DDIMScheduler(</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    num_train_timesteps<span class="op">=</span>pipe.scheduler.num_train_timesteps,</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    beta_start<span class="op">=</span>pipe.scheduler.beta_start,</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    beta_end<span class="op">=</span>pipe.scheduler.beta_end,</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    beta_schedule<span class="op">=</span>pipe.scheduler.beta_schedule,</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    trained_betas<span class="op">=</span>pipe.scheduler.trained_betas,</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    clip_sample<span class="op">=</span>pipe.scheduler.clip_sample,</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    set_alpha_to_one<span class="op">=</span>pipe.scheduler.set_alpha_to_one,</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    steps_offset<span class="op">=</span>pipe.scheduler.steps_offset,</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    prediction_type<span class="op">=</span>pipe.scheduler.prediction_type</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Below we have a sampling function, that also gives us intermediate timesteps. Again this is a pretty standard diffusion sampling loop, check out <a href="https://huggingface.co/blog/stable_diffusion">HuggingFace blog post</a> for more information.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sd_sample(prompts, pipe, height, width, guidance_scale, num_inference_steps, eta, device):</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    scheduler <span class="op">=</span> pipe.scheduler</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    unet <span class="op">=</span> pipe.unet</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    text_embeddings <span class="op">=</span> pipe._encode_prompt(prompts,device, <span class="dv">1</span>, do_classifier_free_guidance<span class="op">=</span>guidance_scale <span class="op">&gt;</span> <span class="fl">1.0</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    scheduler.set_timesteps(num_inference_steps, device<span class="op">=</span>device)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    latents <span class="op">=</span> torch.randn((<span class="bu">len</span>(prompts), unet.in_channels, height<span class="op">//</span><span class="dv">8</span>, width<span class="op">//</span><span class="dv">8</span>)).to(device)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    all_step_preds <span class="op">=</span> []</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(progress_bar(scheduler.timesteps)):</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> torch.cat([latents] <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> scheduler.scale_model_input(<span class="bu">input</span>, t)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predict the noise residual</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> unet(<span class="bu">input</span>, t, encoder_hidden_states<span class="op">=</span>text_embeddings).sample</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># perform guidance</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>        pred_uncond, pred_text <span class="op">=</span> pred.chunk(<span class="dv">2</span>)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> pred_uncond <span class="op">+</span> guidance_scale <span class="op">*</span> (pred_text <span class="op">-</span> pred_uncond)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute the "previous" noisy sample</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>        scheduler_output <span class="op">=</span> scheduler.step(pred, t, latents, eta)</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        all_step_preds.append(scheduler_output)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        latents <span class="op">=</span> scheduler_output.prev_sample</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> latents, all_step_preds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>preds, all_step_preds <span class="op">=</span> sd_sample([prompt]<span class="op">*</span><span class="dv">2</span>, pipe, <span class="dv">512</span>, <span class="dv">512</span>, <span class="fl">7.5</span>, <span class="dv">50</span>, <span class="dv">1</span>, <span class="st">'cuda'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="50" style="width:300px; height:20px; vertical-align: middle;"></progress>
      0.00% [0/50 00:00&lt;?]
    </div>
    
</div>
</div>
<p>The sampling function only gives us latents, and they need to be decoded by a VAE to get the final images:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decoding_fn(latents,pipe):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> pipe.vae.decode(<span class="dv">1</span> <span class="op">/</span> <span class="fl">0.18215</span> <span class="op">*</span> latents.cuda()).sample</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> (images <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span>).clamp(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> images.detach().cpu().permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>).numpy()</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> (images <span class="op">*</span> <span class="dv">255</span>).<span class="bu">round</span>().astype(<span class="st">"uint8"</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> images</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>Image.fromarray(decoding_fn(preds,pipe)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<p><img src="ddpo_files/figure-html/cell-28-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s again calculate the aesthetic score. We have to make a slight modification to our <code>aesthetic_scoring</code> function so it can handle batches.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model):    </span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> torch.stack([preprocess(Image.fromarray(img)).cuda() <span class="cf">for</span> img <span class="kw">in</span> imgs])</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad(): image_features <span class="op">=</span> clip_model.encode_image(imgs)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    im_emb_arr <span class="op">=</span> aesthetic_model_normalize(image_features.cpu().detach().numpy())</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> aesthetic_model(torch.from_numpy(im_emb_arr).<span class="bu">float</span>().cuda())</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prediction</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>imgs <span class="op">=</span> decoding_fn(preds,pipe)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>tensor([[5.1635],
        [5.4475]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>Now we can initialize our dataset generator, which provides prompts that we sample with, generate images, and pass into the aesthetic predictor in just a few lines of code:</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>train_set <span class="op">=</span> PromptDataset(imagenet_animal_prompts, <span class="dv">1000</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> torch.utils.data.DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">2</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>prompts <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dl))</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>preds, all_step_preds <span class="op">=</span> sd_sample(prompts, pipe, <span class="dv">512</span>, <span class="dv">512</span>, <span class="fl">7.5</span>, <span class="dv">50</span>, <span class="dv">1</span>, <span class="st">'cuda'</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>imgs <span class="op">=</span> decoding_fn(preds,pipe)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>rewards <span class="op">=</span> aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="50" style="width:300px; height:20px; vertical-align: middle;"></progress>
      0.00% [0/50 00:00&lt;?]
    </div>
    
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> torch.where(rewards <span class="op">==</span> rewards.<span class="bu">min</span>())[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prompts[index])</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>Image.fromarray(imgs[index])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>polecat</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="33">
<p><img src="ddpo_files/figure-html/cell-33-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> torch.where(rewards <span class="op">==</span> rewards.<span class="bu">max</span>())[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prompts[index])</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>Image.fromarray(imgs[index])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>garter snake</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="34">
<p><img src="ddpo_files/figure-html/cell-34-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Once again, the aesthetic predictor provides <em>rewards</em> and these are used in optimization with RL. Our goal is to maximize the reward!</p>
<p>For stability during RL training, the rewards are usually normalized. There are different ways of doing this, but the DDPO paper utilizes a simple approach, which is to normalize based on unique prompts. Basically, a queue is set up with prompts and the corresponding rewards, which some given buffer size. When a new (prompt, reward) pair is obtained, it is added to the queue. If the queue for that unique prompt is not long enough, the reward is just normalized over the whole batch. Else, it is normalized based on the statistics for that specific prompt. This is implemented below:</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PerPromptStatTracker:</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, buffer_size, min_count):</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buffer_size <span class="op">=</span> buffer_size</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.min_count <span class="op">=</span> min_count</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stats <span class="op">=</span> {}</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, prompts, rewards):</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        unique <span class="op">=</span> np.unique(prompts)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        advantages <span class="op">=</span> np.empty_like(rewards)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> prompt <span class="kw">in</span> unique:</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>            prompt_rewards <span class="op">=</span> rewards[prompts <span class="op">==</span> prompt]</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> prompt <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.stats:</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.stats[prompt] <span class="op">=</span> deque(maxlen<span class="op">=</span><span class="va">self</span>.buffer_size)</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.stats[prompt].extend(prompt_rewards)</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.stats[prompt]) <span class="op">&lt;</span> <span class="va">self</span>.min_count:</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>                mean <span class="op">=</span> np.mean(rewards)</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>                std <span class="op">=</span> np.std(rewards) <span class="op">+</span> <span class="fl">1e-6</span></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>                mean <span class="op">=</span> np.mean(<span class="va">self</span>.stats[prompt])</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>                std <span class="op">=</span> np.std(<span class="va">self</span>.stats[prompt]) <span class="op">+</span> <span class="fl">1e-6</span></span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>            advantages[prompts <span class="op">==</span> prompt] <span class="op">=</span> (prompt_rewards <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> advantages</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>per_prompt_stat_tracker <span class="op">=</span> PerPromptStatTracker(buffer_size<span class="op">=</span><span class="dv">32</span>, min_count<span class="op">=</span><span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>rewards.squeeze().cpu()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>tensor([5.4786, 5.5764], grad_fn=&lt;ToCopyBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> per_prompt_stat_tracker.update(np.array(prompts), rewards.squeeze().cpu().detach().numpy())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>advantages</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>array([-0.99998444,  0.9999747 ], dtype=float32)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>per_prompt_stat_tracker.stats</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>{'garter snake': deque([5.576433], maxlen=32),
 'polecat': deque([5.478563], maxlen=32)}</code></pre>
</div>
</div>
<p>We have the reward normalization set up. Let’s look at some of the intermediate timesteps:</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>Image.fromarray(decoding_fn(all_step_preds[<span class="dv">0</span>].prev_sample, pipe)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<p><img src="ddpo_files/figure-html/cell-41-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>Image.fromarray(decoding_fn(all_step_preds[<span class="dv">30</span>].prev_sample, pipe)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<p><img src="ddpo_files/figure-html/cell-42-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As expected, we get pure noise (in the latent space, it looks somewhat different when decoded by the Stable Diffusion VAE) at the beginning of sampling, and as sampling progresses, the images starts to take shape.</p>
</section>
</section>
<section id="the-ddpo-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="the-ddpo-algorithm">The DDPO algorithm</h2>
<p>Okay let’s now dig into how reinforcement learning works and derive the DDPO objective. A reminder that what we want to do is to maximize the reward signal. We can mathematically express this as follows:</p>
<p><span class="math display">\[ \theta^\star = \arg \max_\theta \mathbb{E}_{\mathbf{x}_0 \sim p_\theta(\cdot \mid \mathbf{c})} [r(\mathbf{x}_0 \mid \mathbf{c})] \]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the weights of our diffusion model, <span class="math inline">\(\mathbf{c}\)</span> is some conditioning for the diffusion model, and <span class="math inline">\(r(\cdot)\)</span> is our reward function.</p>
<p>It would be nice to directly maximize for <span class="math inline">\(r(\cdot)\)</span> and if our model was a single evaluation of a neural network, we could simply backpropagate through the neural network and use an optimizer to update the weights. But that’s not what happens with a diffusion model! We have multiple timesteps for which we apply our denoising neural network. This constructs a <strong>trajectory</strong> as its known in the RL literature. In standard RL literature, our trajectory is composed of <strong>states</strong> and <strong>actions</strong>. A model that we are optimizing provides the next action given the current state, and this model is referred to as the <strong>policy</strong>. This framework is known as a <strong>Markov Decision Process (MDP)</strong>. Note that in the general MDP framework, a reward is usually given after each action, and we optimize over the sum of rewards over the whole trajectory.</p>
<p>We can easily describe diffusion models as an MDP, which will allow us to use standard results in RL for diffusion model optimization.</p>
<p><span class="math display">\[ \begin{array}{rrr}
\mathbf{s}_t \triangleq\left(\mathbf{c}, t, \mathbf{x}_t\right) &amp; \pi\left(\mathbf{a}_t \mid \mathbf{s}_t\right) \triangleq p^{\text{diffusion}}_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{c}\right) &amp; p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t\right) \triangleq\left(\delta_{\mathbf{c}}, \delta_{t-1}, \delta_{\mathbf{x}_{t-1}}\right) \\
\mathbf{a}_t \triangleq \mathbf{x}_{t-1} &amp; \rho_0\left(\mathbf{s}_0\right) \triangleq\left(p(\mathbf{c}), \delta_T, \mathcal{N}(\mathbf{0}, \mathbf{I})\right) &amp; R\left(\mathbf{s}_t, \mathbf{a}_t\right) \triangleq \begin{cases}r\left(\mathbf{x}_0, \mathbf{c}\right) &amp; \text { if } t=0 \\
0 &amp; \text { otherwise }\end{cases}
\end{array} \]</span></p>
<p><span class="math display">\[ \mathcal{J}(\theta)=\mathbb{E}_{\tau \sim p(\cdot \mid \pi)}\left[\sum_{t=0}^T R\left(\mathbf{s}_t, \mathbf{a}_t\right)\right] = \mathbb{E}_{\mathbf{x}_0 \sim p_\theta(\cdot \mid \mathbf{c})} \left[r(\mathbf{x}_0 \mid \mathbf{c})\right] \]</span></p>
<p><span class="math inline">\(\mathbf{s}_t\)</span> is the state, which is just the current noisy image <span class="math inline">\(\mathbf{x}_t\)</span> (along with the timestep and condition info). <span class="math inline">\(\mathbf{a}_t\)</span> is the action, which is the slightly less noisy image <span class="math inline">\(\mathbf{x}_{t-1}\)</span>. <span class="math inline">\(\pi\left(\mathbf{a}_t \mid \mathbf{s}_t\right)\)</span> is the policy that takes the current state and provides the next action, which is our diffusion model <span class="math inline">\(p^{\text{diffusion}}_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{c}\right)\)</span>. <span class="math inline">\(\rho_0\left(\mathbf{s}_0\right)\)</span> is the distribution of the initial states, which in this case is the same distribution for <span class="math inline">\(\mathbf{x}_T\)</span>, a standard isotropic normal distribution, with the timestep always being <span class="math inline">\(T\)</span> and the conditioning having whatever prior distribution as in the dataset. <span class="math inline">\(p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t\right)\)</span> is giving <span class="math inline">\(\mathbf{s}_{t+1}\)</span> given the current state and action, and is basically just saying it always goes to the state associated with <span class="math inline">\(\mathbf{x}_{t-1}\)</span>. <span class="math inline">\(R\left(\mathbf{s}_t, \mathbf{a}_t\right)\)</span> is the reward after each action/state (which is zero until the very last step when the generation is complete).The entire trajectory can be represented as <span class="math inline">\(\tau\)</span> and the probability density for trajectories as <span class="math inline">\(p_\theta(\mathbf{s}_0, \mathbf{a}_0, \cdot \cdot \cdot, \mathbf{s}_T, \mathbf{a}_T) = p_\theta(\tau)\)</span> Note this <span class="math inline">\(p_\theta\)</span> is different from the diffusion model <span class="math inline">\(p_\theta\)</span> as it is a function of <span class="math inline">\(\tau\)</span>, a bit confusing! So going forward the diffusion model is referred to as <span class="math inline">\(p^\text{diffusion}_\theta\)</span>.</p>
<p><img src="image.png" class="img-fluid"></p>
<p>Okay, with this framework in place it becomes trivial to apply standard <strong>policy gradient</strong> optimization methods like <strong>REINFORCE</strong> and <strong>proximal policy optimization (PPO)</strong>. Let’s go over these two algorithms now. I have also written a more principled from-scratch derivation over [here]</p>
<p>We’ll start by looking at the general case of taking the gradient of the expectation over <span class="math inline">\(p_\theta(\mathbf{x})\)</span> of some function <span class="math inline">\(f(\mathbf{x})\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\nabla_\theta \mathbb{E}_{p_{\theta}}[f(\mathbf{x})]
&amp;= \nabla_\theta \int p_{\theta}(\mathbf{x}) f(\mathbf{x}) d\mathbf{x} \\
&amp;= \int \nabla_\theta p_{\theta}(\mathbf{x}) f(\mathbf{x}) d\mathbf{x} \\
&amp;= \int \frac{p_{\theta}(\mathbf{x})}{p_{\theta}(\mathbf{x})} \nabla_\theta p_{\theta}(\mathbf{x}) f(\mathbf{x}) d\mathbf{x} \\
&amp;= \int p_{\theta}(\mathbf{x}) \frac{\nabla_\theta p_{\theta}(\mathbf{x})}{p_{\theta}(\mathbf{x})} f(\mathbf{x}) d\mathbf{x} \\
&amp;= \int p_{\theta}(\mathbf{x}) \nabla_\theta \log p_{\theta}(\mathbf{x}) f(\mathbf{x}) d\mathbf{x} \\
&amp;= \mathbb{E}_{p_{\theta}} \big[ \nabla_\theta \log p_{\theta}(\mathbf{x})f(\mathbf{x}) \big]
\end{align}
\]</span></p>
<p>This is referred to as the <em>score function gradient estimator</em>.</p>
<p>Let’s think more about what this is doing. We want to calculate the gradient of <span class="math inline">\(\mathbb{E}_{p_{\theta}}[f(\mathbf{x})]\)</span> with respect to <span class="math inline">\(\theta\)</span>. That is, we want to know how we can change <span class="math inline">\(\theta\)</span> such that we get samples from <span class="math inline">\(p_\theta(\mathbf{x}\)</span> that on average give higher <span class="math inline">\(f(\mathbf{x})\)</span> values. What this estimator says is that we can take the gradient <span class="math inline">\(\nabla_\theta \log p_{\theta}(\mathbf{x})\)</span> (which tells you how to change <span class="math inline">\(\theta\)</span> to increase the likelihood of <span class="math inline">\(\mathbf{x}\)</span> under your distribtuion <span class="math inline">\(p_\theta(\mathbf{x})\)</span>), and weight it with <span class="math inline">\(f(\mathbf{x})\)</span>. So if this is being used in gradient ascent for example, we are placing more weight on updates that make high-scoring <span class="math inline">\(\mathbf{x}\)</span> samples more likely under our model <span class="math inline">\(p_\theta(\mathbf{x})\)</span>.</p>
<p>When we apply this to the MDP framework and simplify, we can get our <strong>policy gradient</strong>:</p>
<p><span class="math display">\[ \nabla_\theta \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[\left(\sum^T_{t=0} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right) \right) \left(\sum^T_{t=0}R(\mathbf{s}_t, \mathbf{a}_t) \right) \right] \]</span></p>
<p>This gradient is referred to as the <strong>REINFORCE</strong> gradient and is only one type of policy gradient that could be used. Of course, this policy gradient is then used to update the weights of our model using gradient ascent:</p>
<p><span class="math display">\[ \theta \leftarrow \theta + \alpha \nabla_\theta \mathcal{J}(\theta) \]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is some learning rate.</p>
<p>One implementation point is that the expectation is over the trajectories but of course we can’t take and sum over all possible trajectories. The expectation is estimated with just the sampled trajectories in the currenty batch. One other implementation point to mention: we could calculate our gradient and then pass that gradient to our optimizer, or we could let autograd handle the calculation of the gradient by constructing a loss function and treating it as a standard training loop. The latter is what is done in practice even though it is not explicitly mentioned often in the papers. So our loss function is:</p>
<p><span class="math display">\[ \mathcal{L}(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ - \left(\sum^T_{t=0} \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right) \right) \left(\sum^T_{t=0}R(\mathbf{s}_t, \mathbf{a}_t) \right) \right] \]</span></p>
<p>Again, let’s reinforce the intuition behind REINFORCE gradient/loss function (pun definitely intended). You can see the loss looks very much like a negative log-likelihood loss, with the actions as our target. The diference here is that it is weighted by the reward. What this loss is doing is trying to make high-reward trajectories more likely and low-reward trajectories less likely.</p>
<p><img src="1 zkOBQ9Izq28yXCANTmdKtA.webp" class="img-fluid"></p>
<p>Okay so we can simply plug in our diffusion model terms based on how it fits into the MDP framework, which we described earlier. We get:</p>
<p><span class="math display">\[ \nabla_\theta \mathcal{J}(\theta) = \mathbb{E} \left[\left(\sum^T_{t=0} \nabla_\theta \log p^\text{diffusion}_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{c}, t, \mathbf{x}_t\right) \right) r(\mathbf{x}_0, \mathbf{c}) \right] \]</span></p>
<p>This objective and gradient estimator is referred in the paper as <strong>DDPO<sub>SF</sub></strong>.</p>
<p>One challenge with this approach is that for each optimization step, the sampling from the current iteration of the model needs to be performed, we need to re-calculate <span class="math inline">\(\mathbf{x}_t\)</span> as it comes from the current version of the model. This can be computationally expensive and wasteful, as the samples collected with previous iterations of the model cannot be used to learn.</p>
<p>One trick to address this is known as importance sampling. This relies on the following identity (trivial to demonstrate based on the expectation definition):</p>
<p><span class="math display">\[ \mathbb{E}_{x\sim p(x)} \left[f(x)\right] = \mathbb{E}_{x\sim q(x)} \left[\frac{p(x)}{q(x)}f(x)\right] \]</span></p>
<p>Applying importance sampling gives us the following:</p>
<p><span class="math display">\[ \nabla_\theta \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim p_{\theta_{old}} \left(\tau \right)} \left[\left(\sum^T_{t=0} \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right)}{\pi_{\theta_{old}}\left(\mathbf{a}_t \mid \mathbf{s}_t\right)} \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right) \right) \left(\sum^T_{t=0}R(\mathbf{s}_t, \mathbf{a}_t) \right) \right] \]</span></p>
<p>Again this can be written down as a loss function that we perform gradient descent on (sometimes referred to as the <strong>surrogate loss</strong>):</p>
<p><span class="math display">\[ \mathcal{L}(\theta) = \mathbb{E}_{\tau \sim p_{\theta_{old}} \left(\tau \right)} \left[-\left(\sum^T_{t=0} \frac{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right)}{\pi_{\theta_{old}}\left(\mathbf{a}_t \mid \mathbf{s}_t\right)} \right) \left(\sum^T_{t=0}R(\mathbf{s}_t, \mathbf{a}_t) \right) \right] \]</span></p>
<p>Again, we can plug in the diffusion model terms based on the MDP framework and get this loss function:</p>
<p><span class="math display">\[ L(\theta) = \mathbb{E} \left[ - \sum^T_{t=0} \frac{p^\text{diffusion}_\theta(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t)}{p^\text{diffusion}_{\theta_{old}}(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t)} r(\mathbf{x}_0,\mathbf{c}) \right] \]</span></p>
<p>Minimization of this loss function is equivalent to gradient with the following gradient:</p>
<p><span class="math display">\[ \hat g = \mathbb{E} \left[\sum^T_{t=0} \frac{p^\text{diffusion}_\theta(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t)}{p^\text{diffusion}_{\theta_{old}}(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t)} \nabla_\theta p^\text{diffusion}_\theta(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t) r(\mathbf{x}_0,\mathbf{c}) \right] \]</span></p>
<p>Note that the reward <span class="math inline">\(r(\mathbf{x}_0,\mathbf{c})\)</span> is usually normalized, and the normalized reward is referred to the advantage <span class="math inline">\(A(\mathbf{x}_0,\mathbf{c})\)</span>. So the advantage can be negative if it’s less than average.</p>
<p>Note that we don’t want current policy diverge too much from the previous policy, otherwise we may diverge and get a bad policy. To help address this, we can apply clipping to the importance sampling ratio to the loss function:</p>
<p><span class="math display">\[ L(\theta) = \mathbb{E} \left[ - \sum^T_{t=0} \min \left(\frac{p^\text{diffusion}_\theta(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t)}{p^\text{diffusion}_{\theta_{old}}(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t)} A(\mathbf{x}_0,\mathbf{c}), \mathrm{clip} \left( \frac{p^\text{diffusion}_\theta(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t)}{p^\text{diffusion}_{\theta_{old}}(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t)}, 1-\epsilon, 1+\epsilon  \right) A(\mathbf{x}_0,\mathbf{c}) \right) \right] \]</span></p>
<p>So if the policy diverges too much (the ratio is either much larger or much smaller than 1) the loss function is clipped to a certain value and the gradient will be zero and no updates will be made. The below diagram (taken from <a href="https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl">here</a>) clarifies this further:</p>
<p><img src="https://i.stack.imgur.com/gasbI.png" class="img-fluid"></p>
<p>Note that DDPO also clips the advantages themselves $ A(_0,) $ in its implementation but this is not described in the paper, so I have not included it in the loss function.</p>
<p>The loss function can be written in a way that’s numerically easier to calculate/more stable (using logs, ignoring the clipping for now):</p>
<p><span class="math display">\[ L(\theta) = \mathbb{E} \left[ - \sum^T_{t=0} \exp{\left(\log p^\text{diffusion}_\theta(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t) -\log p^\text{diffusion}_{\theta_{old}}(\mathbf{x}_{t-1} | \mathbf{c},t,\mathbf{x}_t) \right)} A(\mathbf{x}_0,\mathbf{c}) \right] \]</span></p>
<p>The objective and gradient estimator described here is referred in the paper as <strong>DDPO<sub>IS</sub></strong>. It is pretty much the same as <strong>proximal policy optimization (PPO)</strong>, applied to diffusion models.</p>
<p>For a more complete derivation of the DDPO objective from scratch, see <a href="ddpo_derivation.pdf">here</a>.</p>
<p>In order to start implementing this loss function, let’s calculate the log probs, which is easy for a normal distribution:</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_log_probs(prev_sample, prev_sample_mean, std_dev_t):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    std_dev_t <span class="op">=</span> torch.clip(std_dev_t, <span class="fl">1e-6</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    log_probs <span class="op">=</span> <span class="op">-</span>((prev_sample.detach() <span class="op">-</span> prev_sample_mean) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> std_dev_t <span class="op">**</span> <span class="dv">2</span>) <span class="op">-</span> torch.log(std_dev_t) <span class="op">-</span> math.log(math.sqrt(<span class="dv">2</span> <span class="op">*</span> math.pi))</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We need to get those log probs of the original model so our sampling function should return that. Let’s update our sampling function to do that:</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sd_sample(prompts, pipe, height, width, guidance_scale, num_inference_steps, eta, device):</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    scheduler <span class="op">=</span> pipe.scheduler</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    unet <span class="op">=</span> pipe.unet</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    text_embeddings <span class="op">=</span> pipe._encode_prompt(prompts,device, <span class="dv">1</span>, do_classifier_free_guidance<span class="op">=</span>guidance_scale <span class="op">&gt;</span> <span class="fl">1.0</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    scheduler.set_timesteps(num_inference_steps, device<span class="op">=</span>device)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    latents <span class="op">=</span> torch.randn((<span class="bu">len</span>(prompts), unet.in_channels, height<span class="op">//</span><span class="dv">8</span>, width<span class="op">//</span><span class="dv">8</span>)).to(device)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    all_step_preds, log_probs <span class="op">=</span> [latents], []</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(progress_bar(scheduler.timesteps)):</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> torch.cat([latents] <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> scheduler.scale_model_input(<span class="bu">input</span>, t)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predict the noise residual</span></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> unet(<span class="bu">input</span>, t, encoder_hidden_states<span class="op">=</span>text_embeddings).sample</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># perform guidance</span></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>        pred_uncond, pred_text <span class="op">=</span> pred.chunk(<span class="dv">2</span>)</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> pred_uncond <span class="op">+</span> guidance_scale <span class="op">*</span> (pred_text <span class="op">-</span> pred_uncond)</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute the "previous" noisy sample mean and variance, and get log probs</span></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>        scheduler_output <span class="op">=</span> scheduler.step(pred, t, latents, eta, variance_noise<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>        t_1 <span class="op">=</span> t <span class="op">-</span> scheduler.config.num_train_timesteps <span class="op">//</span> num_inference_steps</span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>        variance <span class="op">=</span> scheduler._get_variance(t, t_1)</span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>        std_dev_t <span class="op">=</span> eta <span class="op">*</span> variance <span class="op">**</span> (<span class="fl">0.5</span>)</span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>        prev_sample_mean <span class="op">=</span> scheduler_output.prev_sample <span class="co"># this is the mean and not full sample since variance is 0</span></span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>        prev_sample <span class="op">=</span> prev_sample_mean <span class="op">+</span> torch.randn_like(prev_sample_mean) <span class="op">*</span> std_dev_t <span class="co"># get full sample by adding noise</span></span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a>        log_probs.append(calculate_log_probs(prev_sample, prev_sample_mean, std_dev_t).mean(dim<span class="op">=</span><span class="bu">tuple</span>(<span class="bu">range</span>(<span class="dv">1</span>, prev_sample_mean.ndim))))</span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a>        all_step_preds.append(prev_sample)</span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a>        latents <span class="op">=</span> prev_sample</span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> latents, torch.stack(all_step_preds), torch.stack(log_probs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can get everything we need for the loss function now (intermediate timesteps, log_probs, rewards):</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>per_prompt_stat_tracker <span class="op">=</span> PerPromptStatTracker(buffer_size<span class="op">=</span><span class="dv">32</span>, min_count<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>prompts <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dl))</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>pipe.text_encoder.to(<span class="st">'cuda'</span>)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>pipe.vae.to(<span class="st">'cuda'</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>preds, all_step_preds, log_probs <span class="op">=</span> sd_sample(prompts, pipe, <span class="dv">512</span>, <span class="dv">512</span>, <span class="fl">7.5</span>, <span class="dv">50</span>, <span class="dv">1</span>, <span class="st">'cuda'</span>)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>imgs <span class="op">=</span> decoding_fn(preds,pipe)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>rewards <span class="op">=</span> aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> torch.from_numpy(per_prompt_stat_tracker.update(np.array(prompts), rewards.squeeze().cpu().detach().numpy())).<span class="bu">float</span>().to(<span class="st">'cuda'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="50" style="width:300px; height:20px; vertical-align: middle;"></progress>
      0.00% [0/50 00:00&lt;?]
    </div>
    
</div>
</div>
<p>Here’s a function to compute the loss function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_loss(x_t, original_log_probs, advantages, clip_advantages, clip_ratio, prompts, pipe, num_inference_steps, guidance_scale, eta, device):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    scheduler <span class="op">=</span> pipe.scheduler</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    unet <span class="op">=</span> pipe.unet</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    text_embeddings <span class="op">=</span> pipe._encode_prompt(prompts,device, <span class="dv">1</span>, do_classifier_free_guidance<span class="op">=</span>guidance_scale <span class="op">&gt;</span> <span class="fl">1.0</span>).detach()</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    scheduler.set_timesteps(num_inference_steps, device<span class="op">=</span>device)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    loss_value <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(progress_bar(scheduler.timesteps)):</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        clipped_advantages <span class="op">=</span> torch.clip(advantages, <span class="op">-</span>clip_advantages, clip_advantages).detach()</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> torch.cat([x_t[i].detach()] <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> scheduler.scale_model_input(<span class="bu">input</span>, t)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predict the noise residual</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> unet(<span class="bu">input</span>, t, encoder_hidden_states<span class="op">=</span>text_embeddings).sample</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># perform guidance</span></span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a>        pred_uncond, pred_text <span class="op">=</span> pred.chunk(<span class="dv">2</span>)</span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> pred_uncond <span class="op">+</span> guidance_scale <span class="op">*</span> (pred_text <span class="op">-</span> pred_uncond)</span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute the "previous" noisy sample mean and variance, and get log probs</span></span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>        scheduler_output <span class="op">=</span> scheduler.step(pred, t, x_t[i].detach(), eta, variance_noise<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>        t_1 <span class="op">=</span> t <span class="op">-</span> scheduler.config.num_train_timesteps <span class="op">//</span> num_inference_steps</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a>        variance <span class="op">=</span> scheduler._get_variance(t, t_1)</span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a>        std_dev_t <span class="op">=</span> eta <span class="op">*</span> variance <span class="op">**</span> (<span class="fl">0.5</span>)</span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>        prev_sample_mean <span class="op">=</span> scheduler_output.prev_sample</span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a>        current_log_probs <span class="op">=</span> calculate_log_probs(x_t[i<span class="op">+</span><span class="dv">1</span>].detach(), prev_sample_mean, std_dev_t).mean(dim<span class="op">=</span><span class="bu">tuple</span>(<span class="bu">range</span>(<span class="dv">1</span>, prev_sample_mean.ndim)))</span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate loss</span></span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a>        ratio <span class="op">=</span> torch.exp(current_log_probs <span class="op">-</span> original_log_probs[i].detach()) <span class="co"># this is the importance ratio of the new policy to the old policy</span></span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a>        unclipped_loss <span class="op">=</span> <span class="op">-</span>clipped_advantages <span class="op">*</span> ratio <span class="co"># this is the surrogate loss</span></span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a>        clipped_loss <span class="op">=</span> <span class="op">-</span>clipped_advantages <span class="op">*</span> torch.clip(ratio, <span class="fl">1.</span> <span class="op">-</span> clip_ratio, <span class="fl">1.</span> <span class="op">+</span> clip_ratio) <span class="co"># this is the surrogate loss, but with artificially clipped ratios</span></span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> torch.<span class="bu">max</span>(unclipped_loss, clipped_loss).mean() <span class="co"># we take the max of the clipped and unclipped surrogate losses, and take the mean over the batch</span></span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a>        loss.backward() <span class="co"># perform backward here, gets accumulated for all the timesteps</span></span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a>        loss_value <span class="op">+=</span> loss.item()</span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss_value</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> compute_loss(all_step_preds, log_probs, advantages, <span class="dv">10</span>, <span class="fl">1e-4</span>, prompts, pipe, <span class="dv">50</span>, <span class="fl">7.5</span>, <span class="dv">1</span>, <span class="st">'cuda'</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="50" class="" max="50" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [50/50 00:29&lt;00:00]
    </div>
    
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2.6168066263198853</code></pre>
</div>
</div>
</section>
<section id="complete-training-loop" class="level2">
<h2 class="anchored" data-anchor-id="complete-training-loop">Complete training loop</h2>
<p>Now that we can calculate the loss function, we can construct the full training loop. For a single epoch we:</p>
<ol type="1">
<li><p>Sample from the diffusion model <code>num_samples_per_epoch</code>times, collecting the intermediate noisy images and log probs.</p></li>
<li><p>Pass the samples to the reward model and get reward, which we normalize to get advantage.</p></li>
<li><p>For <code>num_inner_epochs</code> times, we go over each sample compute the loss, backpropagate, and update our diffusion model.</p></li>
</ol>
<p>Let’s define all our hyperparameters. We will be training Stable Diffusion v1.4 on ImageNet animal prompts as defined earlier, using the LAION Aesthetic classifier as our reward model.</p>
<p>Note that if we set <code>num_inner_epochs</code> to a high amount, this would be very data-efficient since we would be repeatedly using the previously generated trajectories, but we would probably significantly diverge from the original policy that we used to get those trajectories (or it would at least get clipped frequently in the loss). So we set <code>num_inner_epochs=1</code>. This is still pretty efficient using DDPO<sub>IS</sub> because otherwise with DDPO<sub>SF</sub> you’d need to resample after every iteration (when model is updated) instead of after <code>num_samples_per_epoch=128</code> that we have here.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>num_samples_per_epoch <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>num_inner_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>num_timesteps <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>img_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">5e-6</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>clip_advantages <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>clip_ratio <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>cfg <span class="op">=</span> <span class="fl">5.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Okay let’s set everything up:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># group all reward function stuff</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reward_fn(imgs, device):</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    clip_model.to(device)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    aesthetic_model.to(device)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    rewards <span class="op">=</span> aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    clip_model.to(<span class="st">'cpu'</span>)</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    aesthetic_model.to(<span class="st">'cpu'</span>)</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rewards</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a function to sample from the model and calculate rewards</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_and_calculate_rewards(prompts, pipe, image_size, cfg, num_timesteps, decoding_fn, reward_fn, device):</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>    preds, all_step_preds, log_probs <span class="op">=</span> sd_sample(prompts, pipe, image_size, image_size, cfg, num_timesteps, <span class="dv">1</span>, device)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> decoding_fn(preds,pipe)    </span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>    rewards <span class="op">=</span> reward_fn(imgs, device)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> imgs, rewards, all_step_preds, log_probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here we create our dataset, which is just randomly chosen prompts:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>train_set <span class="op">=</span> PromptDataset(imagenet_animal_prompts, num_samples_per_epoch)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> torch.utils.data.DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>per_prompt_stat_tracker <span class="op">=</span> PerPromptStatTracker(buffer_size<span class="op">=</span><span class="dv">32</span>, min_count<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>sample_prompts <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dl)) <span class="co"># sample a batch of prompts to use for visualization</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>pipe.unet.enable_gradient_checkpointing() <span class="co"># more performance optimization</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(pipe.unet.parameters(), lr<span class="op">=</span>lr, weight_decay<span class="op">=</span><span class="fl">1e-4</span>) <span class="co"># optimizer</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we are set up, let’s start training! You can see the training loop is quite simple!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> master_bar(<span class="bu">range</span>(num_epochs)):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    all_step_preds, log_probs, advantages, all_prompts, all_rewards <span class="op">=</span> [], [], [], [], []</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sampling `num_samples_per_epoch` images and calculating rewards</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, prompts <span class="kw">in</span> <span class="bu">enumerate</span>(progress_bar(train_dl)):</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>        batch_imgs, rewards, batch_all_step_preds, batch_log_probs <span class="op">=</span> sample_and_calculate_rewards(prompts, pipe, img_size, cfg, num_timesteps, decoding_fn, reward_fn, <span class="st">'cuda'</span>)</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>        batch_advantages <span class="op">=</span> torch.from_numpy(per_prompt_stat_tracker.update(np.array(prompts), rewards.squeeze().cpu().detach().numpy())).<span class="bu">float</span>().to(<span class="st">'cuda'</span>)</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>        all_step_preds.append(batch_all_step_preds)</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>        log_probs.append(batch_log_probs)</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>        advantages.append(batch_advantages)</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>        all_prompts <span class="op">+=</span> prompts</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>        all_rewards.append(rewards)</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>    all_step_preds <span class="op">=</span> torch.cat(all_step_preds, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>    log_probs <span class="op">=</span> torch.cat(log_probs, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>    advantages <span class="op">=</span> torch.cat(advantages)</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>    all_rewards <span class="op">=</span> torch.cat(all_rewards)</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># inner loop</span></span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inner_epoch <span class="kw">in</span> progress_bar(<span class="bu">range</span>(num_inner_epochs)):</span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># chunk them into batches</span></span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a>        all_step_preds_chunked <span class="op">=</span> torch.chunk(all_step_preds, num_samples_per_epoch <span class="op">//</span> batch_size, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a>        log_probs_chunked <span class="op">=</span> torch.chunk(log_probs, num_samples_per_epoch <span class="op">//</span> batch_size, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a>        advantages_chunked <span class="op">=</span> torch.chunk(advantages, num_samples_per_epoch <span class="op">//</span> batch_size, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># chunk the prompts (list of strings) into batches</span></span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a>        all_prompts_chunked <span class="op">=</span> [all_prompts[i:i <span class="op">+</span> batch_size] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(all_prompts), batch_size)]</span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> progress_bar(<span class="bu">range</span>(<span class="bu">len</span>(all_step_preds_chunked))):</span>
<span id="cb68-30"><a href="#cb68-30" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb68-31"><a href="#cb68-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-32"><a href="#cb68-32" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> compute_loss(all_step_preds_chunked[i], log_probs_chunked[i], </span>
<span id="cb68-33"><a href="#cb68-33" aria-hidden="true" tabindex="-1"></a>                                advantages_chunked[i], clip_advantages, clip_ratio, all_prompts_chunked[i], pipe, num_timesteps, cfg, <span class="dv">1</span>, <span class="st">'cuda'</span></span>
<span id="cb68-34"><a href="#cb68-34" aria-hidden="true" tabindex="-1"></a>                                ) <span class="co"># loss.backward happens inside</span></span>
<span id="cb68-35"><a href="#cb68-35" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb68-36"><a href="#cb68-36" aria-hidden="true" tabindex="-1"></a>            torch.nn.utils.clip_grad_norm_(pipe.unet.parameters(), <span class="fl">1.0</span>) <span class="co"># gradient clipping</span></span>
<span id="cb68-37"><a href="#cb68-37" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>That’s it! Let’s see what results we get with this code.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>(These results were obtained with <a href="https://github.com/tmabraham/ddpo-pytorch/blob/main/main.py">this script</a> that included W&amp;B tracking)</p>
<p>Here is the loss curve from training Stable Diffusion v1.4 with the LAION Aesthetic classifier reward model on ImageNet animal prompts: <img src="image-1.png" class="img-fluid"></p>
<p>As you can see, it’s quite noisy but clearly does decrease. That said, I’ve observed that typically the loss curve can be quite uninformative. Instead, as expected, the reward curve is a better indicator of the performance:</p>
<p><img src="image-2.png" class="img-fluid"></p>
<p>Here you can see a clear increase in average reward over training, which is what we want! So what do the samples look like? Let’s see:</p>
<p><img src="image-3.png" class="img-fluid"></p>
<p>I’d argue these images are definitely more aesthetic! It works! A few observations:</p>
<ol type="1">
<li><p>Sometimes the prompt isn’t being followed, as seen with the wolf spider example. This is since the reward model does not take into consideration the prompt and only looks at the image, so if generating something that is slightly unrelated to the prompt gives a better reward score then the model will do so. A reward model explicitly taking in the prompt as well and ensuring prompt alignment would be needed. One such model that could be used is <a href="https://arxiv.org/abs/2305.01569">PickScore</a>.</p></li>
<li><p>Sometimes the RL-trained diffusion model generates pencil/charcoal drawings of the animals, which was observed in the original DDPO paper as well.</p></li>
<li><p>Another common pattern that the RL-trained diffusion model demonstrates is the generation of narrow depth-of-focus images, which of course clearly looks more artistic and aesthetic.</p></li>
</ol>
</section>
<section id="drlx---a-library-for-performing-rlhf-training-with-diffusion-models" class="level2">
<h2 class="anchored" data-anchor-id="drlx---a-library-for-performing-rlhf-training-with-diffusion-models">DRLX - A library for performing RLHF training with diffusion models</h2>
<p>In order to make RLHF for diffusion models easy-to-use and accessible, I have co-developed a library with Shahbuland Matiana at <a href="https://carper.ai">CarperAI</a> called <a href="https://github.com/carperai/drlx">DRLX</a>. It implements DDPO, complete with W&amp;B experiment tracking, distributed GPU training support, and other features. We also will be implementing other RL algorithms and adding more features in the coming weeks. Here I provide a quick overview, but check out our <a href="https://carper.ai">blog post</a> for more information!</p>
<p>Let’s see how to do the same DDPO training of Stable Diffusion 1.4 with LAION aesthetic classifier on ImageNet prompts. First we’ll do our imports:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> drlx.trainer.ddpo_trainer <span class="im">import</span> DDPOTrainer</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> drlx.configs <span class="im">import</span> DRLXConfig</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> drlx.reward_modelling.aesthetics <span class="im">import</span> Aesthetics</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> drlx.pipeline.imagenet_animal_prompts <span class="im">import</span> ImagenetAnimalPrompts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>DRLX has a simple-to-use config system, where the model information and hyperparameters are described in a YAML file. See the config file for this example <a href="https://github.com/CarperAI/DRLX/blob/main/configs/ddpo_sd_imagenet.yml">here</a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> DRLXConfig.load_yaml(<span class="st">"configs/ddpo_sd_imagenet.yml"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>DRLX has <code>PromptPipeline</code> to implement the prompts we pass into the diffusion model. We already have a prompt pipeline for ImageNet animal prompts implemented in the library:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> ImagenetAnimalPrompts(prefix<span class="op">=</span><span class="st">''</span>, postfix<span class="op">=</span><span class="st">''</span>, num<span class="op">=</span>config.train.num_samples_per_epoch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>All we have to do is instantiate our <code>DDPOTrainer</code> and call <code>train()</code>, passing in our reward model. We have a <code>RewardModel</code> class that can be subclassed to implement the desired reward function, and the LAION aesthetic classifier is already provided as the <code>Aesthetics</code> lass:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> DDPOTrainer(config)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>trainer.train(pipe, Aesthetics())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It’s that simple!</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Note that much of what we discuss here for how RL is applied to diffusion models also applies to language models. Specifically, the autoregressive generation of text from a language model can be viewed as a trajectory from an MDP. The state is the previous tokens, the action is the next token to be predicted, and the policy is of course the language model. The PPO algorithm that we described is the most common RL algorithm for RLHF training of language models. Overall, this hints to a deeper connection between diffusion models and autoregressive language models and how ideas can be transferred from one domain to another. For example, recently it was demonstrated how classifier-free guidance <a href="https://arxiv.org/abs/2306.17806">could be applied</a> to language models. There may continue to be interesting ideas to apply from diffusion models to language models or vice versa.</p>
<p>This paper only is the start of applying RL to diffusion models. DDPO<sub>IS</sub> is only a baseline, and there are changes that can easily be made to improve the performance, such as value function baselines, reward discounting, KL regularization, etc. Additionally, alternative RLHF algorithms like <a href="https://arxiv.org/abs/2305.18290">direct preference optimization</a> could be explored. We plan to explore these directions further via DRLX.</p>
</section>
<section id="acknowledgements" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h2>
<p>Thank you to <a href="https://twitter.com/shah_bu_land">Shahbuland Matiana</a>, … for providing feedback on my blog post. Thank you to <a href="https://costa.sh/">Costa Huang</a> who helped me debug my code.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="tmabraham/new_blog" issue-term="title" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>