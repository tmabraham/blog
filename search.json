[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! To learn more about me, check out my website here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr.¬†Tanishq Abraham‚Äôs blog",
    "section": "",
    "text": "Reinforcement Learning for Diffusion Models from Scratch\n\n\n\n\n\n\n\ndeep learning\n\n\ndiffusion models\n\n\nreinforcement learning\n\n\n\n\nImplementing the DDPO algorithm\n\n\n\n\n\n\nSep 22, 2023\n\n\nTanishq Mathew Abraham, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nGradio + HuggingFace Spaces: A Tutorial\n\n\n\n\n\n\n\ndeep learning\n\n\n\n\nLearn about easy ML app development\n\n\n\n\n\n\nNov 16, 2021\n\n\nTanishq Mathew Abraham, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nCoding with GitHub Copilot\n\n\n\n\n\n\n\nprogramming\n\n\n\n\nMy thoughts and experience on the new GitHub Copilot tool.\n\n\n\n\n\n\nJul 14, 2021\n\n\nTanishq Mathew Abraham, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Noisy Imagenette\n\n\n\n\n\n\n\ndeep learning\n\n\nimagenette\n\n\n\n\nA noisy version of fastai‚Äôs Imagenette/Imagewoof datasets\n\n\n\n\n\n\nMar 2, 2021\n\n\nTanishq Mathew Abraham, Ph.D.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-03-02-noisy-imagenette.html",
    "href": "posts/2021-03-02-noisy-imagenette.html",
    "title": "Introducing Noisy Imagenette",
    "section": "",
    "text": "TL;DR: We introduce a dataset, Noisy Imagenette, which is a version of the Imagenette dataset with noisy labels. We hope this dataset is useful for rapid experimentation and testing of methods to address noisy label training."
  },
  {
    "objectID": "posts/2021-03-02-noisy-imagenette.html#dataset-have-noisy-labels",
    "href": "posts/2021-03-02-noisy-imagenette.html#dataset-have-noisy-labels",
    "title": "Introducing Noisy Imagenette",
    "section": "Dataset have noisy labels!",
    "text": "Dataset have noisy labels!\nDeep learning has led to impressive results on datasets of all types, but its success often shines when models are trained with large datasets with human-annotated labels (extreme example: GPT-3 and more recently CLIP/ALIGN/DALL-E). A major challenge when constructing these datasets is obtaining enough labels to train a neural network model. There is an inherent tradeoff between the quality of the annotations and the cost of annotation (in the form of time or money). For example, while using sources like Amazon Mechanical Turk provide cheap labeling, the use of these non-expert labeling services will often produce unreliable labels. This is what is referred to as noisy labels, as these unreliable labels are not necessarily ground truth. Unfortunately, neural networks are known to be susceptible to overfitting to noisy labels (see here) which means alternative approaches are needed to achieve good generalization in the presence of noisy labels."
  },
  {
    "objectID": "posts/2021-03-02-noisy-imagenette.html#prior-research-on-noisy-labels",
    "href": "posts/2021-03-02-noisy-imagenette.html#prior-research-on-noisy-labels",
    "title": "Introducing Noisy Imagenette",
    "section": "Prior research on noisy labels",
    "text": "Prior research on noisy labels\nRecently, many techniques have been presented in order to address label noise. These include novel loss functions like Bi-Tempered Logistic LossTaylor Cross Entropy Loss, or Symmetric Cross Entropy. Additionally, there are many novel training techniques that have been recently developed like MentorMix, DivideMix, Early-Learning Regularization and Noise-Robust Contrastive Learning.\nMost of these papers are using MNIST, SVHN, CIFAR10 or related datasets with synthetically-added noise. Other common datasets are the WebVision and Clothing1M datasets, which are real-world noisy, large-scale datasets with millions of images. Therefore there is an opportunity to develop a mid-scale dataset that allows for rapid prototyping but is complex enough to provide useful results when it comes to noisy label training."
  },
  {
    "objectID": "posts/2021-03-02-noisy-imagenette.html#fastais-imagenette---a-dataset-for-rapid-prototyping",
    "href": "posts/2021-03-02-noisy-imagenette.html#fastais-imagenette---a-dataset-for-rapid-prototyping",
    "title": "Introducing Noisy Imagenette",
    "section": "fastai‚Äôs Imagenette - a dataset for rapid prototyping",
    "text": "fastai‚Äôs Imagenette - a dataset for rapid prototyping\nThe idea of mid-scale datasets for rapid prototyping has been explored in the past. For example, in 2019, fast.ai released the Imagenette and Imagewoof datasets (subsequently updated in 2020), subsets of Imagenet for rapid experimentation and prototyping. It can serve as a small dataset proxy for the ImageNet, or a dataset with more complexity than MNIST or CIFAR10 but still small and simple enough for benchmarking and rapid experimentation. This dataset has been used to test and establish new training techniques like Mish activation function and Ranger optimizer (see here). The dataset also has been used in various papers (see here, here, here, here, here, and here). Clearly, this dataset has been quite useful to machine learning researchers and practitioners for testing and comparing new methods. We believe that an analogous dataset could be useful to researchers with modest compute for testing and comparing new methods for addressing label noise."
  },
  {
    "objectID": "posts/2021-07-14-GitHub-Copilot.html",
    "href": "posts/2021-07-14-GitHub-Copilot.html",
    "title": "Coding with GitHub Copilot",
    "section": "",
    "text": "On July 1st, I was able to obtain access to GitHub Copilot, thanks to Hamel Husain. I wanted to share my experience and discoveries about this new tool. Much of the findings was demonstrated with the help of Mazen Alotaibi, Ryan Panwar, and Mark Saroufim.\n\n\n\n\nIf you haven‚Äôt logged onto Twitter or Hacker News in the last couple weeks, you might not know about GitHub Copilot. Developed out of a partnership between OpenAI and Microsoft (GitHub‚Äôs parent company), it‚Äôs an AI-based autocomplete tool that helps you to write code faster. The GitHub team has termed it ‚Äúyour AI pair programmer‚Äù. OpenAI CTO Greg Brockman has explained that it utilizes the currently-unreleased Codex model, which is apparently a successor to the (in)famous GPT-3 language model. It has been trained on billions of lines of code available on GitHub 1.\nBased on the demos that GitHub Copilot provided and favorable reviews from beta-testers, I was eager to give it a try, but I was also skeptical if it really was as life-changing as people claimed it was. To my surprise, it was much better than I expected.\nHere is a demo of GitHub Copilot in action (specifically for an ML-related task):\n\n{{ tweet iScienceLuvr 1411074516411764743 }}\n\nIt‚Äôs clear that GitHub Copilot understands the general PyTorch training workflow, and understands intricacies like what are the appropriate augmentations for images (resizing, random crop, normalization, etc.), making sure to put model into evaluation mode and with torch.no_grad() during validation, etc. These are things that sometimes we may forget to do, so it‚Äôs great that GitHub Copilot can help prevent us from making these common mistakes.\nGitHub Copilot performs best when you provide it with comments describing what you are trying to do. It then uses the comments to generate a list of possible completions. This is highlighted in the example above, where I wrote a few lines about what I wanted to do (fine-tuning a pretrained ResNet50 on a custom dataset) and how I wanted to do that, and it mostly completed the rest of the code for me. I think this is great, because it changes the way we code. It now drives code development to focus on documentation, since writing good documentation often results in better Copilot suggestions.\nOn a related note, some have hypothesized that GitHub Copilot might also lead to more test-driven development:\n\n\n\nI also want to point out that while most demos directly use GitHub Copilot in the editor, it‚Äôs also possible to open GitHub Copilot in a separate tab and have it generate and present multiple suggestions for you. Here‚Äôs an example:\n\n\n\n\n\nI quite like this feature, because it provides various approaches for solving a particular task, and I can select which approach I want to use. For instance, in the above example, it shows various approaches for defining a ResNet50 model for fine-tuning. I typically prefer defining a class for the ResNet50, so I select that option.\nThere is another unintended consequence of GitHub Copilot that I find interesting. GitHub Copilot actually makes a pretty good autocomplete tool for regular writing. I actually discovered this when I started writing this blog post in a Markdown file in the VS Code editor. Of course, this is likely GitHub Copilot learning from README files and other documentation in various repositories, and there could be some residual general knowledge from the underlying GPT-3 model (if that is indeed the base model used) 2. But I would genuinely consider writing more in Markdown files with VS Code + GitHub Copilot because some of the autocomplete suggestions are actually quite helpful.\n\n\n\n\nThere are several challenges that I think could preclude widespread use of GitHub Copilot:\n\nLeaking of personal information\nLimited multi-lingual capabilities\nCopyright/licensing issues\nUsage of outdated APIs\n\nLet‚Äôs dive into each of these issues further.\n\n\nOne aspect we discovered was that GitHub Copilot would inadvertantly share information that would be considered personal, such as people‚Äôs names, phone numbers, emails, etc. This was something Mazen and I explored further. Here are a few examples of this.\nIn a Python file, simply asking it to create a function to list author names indeed gives the name of a person that exist: \nMark demonstrated an example when writing a bash script when an actual person‚Äôs name was suggested in an autocompletion here.\nInterestingly, this method did not work for returning other types of information like phone numbers:\n\n\nBut if we just ask GitHub Copilot to autocomplete phone number in a comment at the beginning of a Python file, it does work:\n\nMazen looked more into this number, and found out it was used in several GitHub repositories, including a programming example problem here.\nMark also discovered that working API keys were provided by GitHub Copilot: \nInterestingly, from my experiments, I was not able to get GitHub Copilot to leak any e-mail addresses.\nOn their website, GitHub Copilot has the following information:\n\nSo this confirms that indeed private information was available in the training set that allows GitHub Copilot to leak this information. I was unable to easily get email addresses because of the rudimentary filtering that GitHub Copilot performed.\n\n\n\nAs we mentioned before, GitHub Copilot performs best when you provide it with comments explaining your intent. Therefore, Mazen and I wanted to explore how well GitHub Copilot can perform with comments in various languages. I have used Google Translate to translate my English comments to various languages and observe how well it performed. Let‚Äôs go over an example. Below, I give GitHub Copilot the prompt to ‚ÄúAdd two numbers‚Äù and see what Python code it suggests:\n\n\n</tr></tr></tr></tr>\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMandarin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpanish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArabic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf course, if you comment with English, GitHub Copilot provides a good suggestion. It gives us an adding function as well as some use-cases. But as demonstrated in these experiments, the quality of GitHub Copilot suggestions when given comments in other languages likely is correlated with the overall frequency of these languages in the training data. It‚Äôs likely that Mandarin and Spanish is more common than Arabic in the training set, so GitHub Copilot performs better with Mandarin and Spanish comments. Of course, this is a single example (although I observed similar results with other prompts). However, given that it‚Äôs well-established that biases in the training data are reflected in the output of any ML algorithm (unless it is appropriately counteracted), I think it is safe to assume that GitHub Copilot will likely be less useful for non-English-speaking users.\n\n\n\nLet‚Äôs move on to the elephant in the room: copyright/licensing issues. GitHub Copilot/Codex was trained on all public GitHub code, regardless of license (confirmed by GitHub). While some argue that training on copyrighted code is not an issue, it becomes much more challenging to argue that when Copilot is regurgitating public code verbatim 3. According to GitHub, Copilot repeats code snippets verbatim about 0.1% of the time. They have also provided a more in-depth study here. Thankfully they are currently developing origin tracker that tells where the verbatim code is coming from and allows you to decide whether to include proper attribution or not use that code altogether.\nIn my opinion, because of these copyright issues, GitHub Copilot in its current state is not usable for commericial purposes. I think that once the origin tracker is released, copyright issues will be resolved, although it puts the onus on the user to make sure that code is properly attributed. Of course, the easier solution would have been to avoid training on copyrighted and GPL-licensed code altogether, which would have likely prevented the significant controversy that arose, and I wonder what led to the decision to train on all public GitHub code instead of further curating the dataset.\n\n\n\nAs an ML researcher and developer, I am typically working with the latest ML frameworks and tools. However, GitHub Copilot is trained on older codebases and does not have knowledge of these cutting-edge tools and is often unable to provide relevant suggestions.\nI first discovered this issue when trying to write fastai-related code and get GitHub Copilot to provide relevant suggestions. However, since the latest version of fastai was only released in August 2020, GitHub Copilot was not able to provide any relevant suggestions and instead provided code for using older versions of fastai. This indicates that the codebases that GitHub Copilot is trained on must be at least before August 2020, if not earlier. Similarly, I discovered that GitHub Copilot was unable to provide any suggestions regarding the usage of the timm library, which is one of the leading deep learning+computer vision libraries.\nHere is a video that demonstrates this issue:\n\n\n\n\n\nTo me, this is a major concern regarding the current usability of GitHub Copilot 4. If we are using cutting edge tools like PyTorch XLA, JAX, fastai, timm, GitHub Copilot has no knowledge of this and cannot provide useful suggestions. Somehow, the GitHub team needs to keep Copilot updated on newer codebases. Given that telemetry of GitHub Copilot usage is being sent to GitHub, it‚Äôs possible that the GitHub team can further train their model on the usage of these newer codebases. Indeed, it is mentioned in the documentation that the telemetry data is used for ‚Äúimproving the underlying code generation models, e.g.¬†by providing positive and negative examples (but always so that your private code is not used as input to suggest code for other users of GitHub Copilot)‚Äù. Additionally, a GitHub Developer Advocate has mentioned that ‚Äúthe model is being trained everyday, so the more people use it, Copilot will learn that these suggestions need to be updated‚Äù.\nI wonder if the GitHub team might also develop a way of perhaps fine-tuning GitHub Copilot to specific use-cases. For example, there may be a specific GitHub Copilot models for fastai, JAX, etc. They would be fine-tuned on the source code of of these libraries and codebases that use these libraries. But making sure that the tool does not provide outdated suggestions would still be a challenge. I don‚Äôt think it would be possible to provide suggestions for a brand-new library that does not have enough codebases using it to train on. Additionally, for situations like fastai where there are older APIs and newer APIs, when fine-tuning a model, the codebases using the older APIs would have to be filtered out.\nAll in all, I personally think that for practical applications, it is necessary for GitHub Copilot to provide suggestions for new codebases, and doing so might be a difficult but potentially solvable challenge.\n\n\n\n\nWhile it is currently available for free to the beta-testers, the GitHub team has already mentioned they plan to commercialize this product. There are several ways that GitHub Copilot could be commercialized:\n\nA monthly fee for personal use of a generic GitHub Copilot model\nEnterprises paying for a model fine-tuned to their specific, private codebases\nSeparate fees for domain-specific models (ex: a GitHub Copilot model for writing machine learning code, or a GitHub Copilot model for web development)\n\n\n\n\nIn conclusion, GitHub Copilot, is a mind-blowing and extremely powerful tool. Additionally, it is a very interesting and practical application of AI. With the domains that it is most familiar, GitHub Copilot works exceptionally well and can write most of the code for you! It may very well change the approach and workflow many programmers have and lead to documentation-driven and test-driven development.\nBut it‚Äôs not yet ready for prime time. There are clear issues with leaking of personal information copyright/licensing issues, accessibility to foreign-language users, and its use on more cutting-edge projects. Thankfully, the GitHub team is working on these issues and I‚Äôm excited by the future of AI-augmented programming!"
  },
  {
    "objectID": "posts/2021-11-16-gradio-huggingface.html",
    "href": "posts/2021-11-16-gradio-huggingface.html",
    "title": "Gradio + HuggingFace Spaces: A Tutorial",
    "section": "",
    "text": "Introduction\nAfter you train a machine learning model, the next thing to do is showcase it to the world by making a demo. Currently, the easiest way to do so is with Gradio, hosting on HuggingFace Spaces. With the Gradio framework deployed on Spaces, it takes <10 minutes to deploy a model! Let‚Äôs see how we can easily deploy a model for the world to try out with these platforms. We will use a classic CNN pet classifier as an example.\n\n\nPreliminaries: Training a pet classifier\nBefore we make a demo, we need to have a model to actually demo! Let‚Äôs quickly train a simple ResNet50 pet classifier on the Oxford Pets dataset using fastai.\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\ndls = ImageDataLoaders.from_name_re(path, get_image_files(path/'images'), pat='(.+)_\\d+.jpg', item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75))\nlearn = vision_learner(dls, models.resnet50, metrics=accuracy)\nlearn.fine_tune(1)\nlearn.path = Path('.')\nlearn.export()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.973277\n      0.309940\n      0.905954\n      00:32\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.420781\n      0.260167\n      0.910690\n      00:34\n    \n  \n\n\n\nAnd with fastai, it‚Äôs that simple! Learn more about fastai, a simple and flexible PyTorch training framework, over here.\n\n\nUsing Gradio\nLet‚Äôs see how to make a demo web app with Gradio. First let‚Äôs load our model:\n\nlearn = load_learner('export.pkl')\n\nNext, let‚Äôs define a prediction function our model:\n\nlabels = learn.dls.vocab\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\nFinally, let‚Äôs import Gradio and use it‚Äôs functionality to make an interface and launch it. Note that if you are doing this from a notebook, the Gradio demo will also show up within the notebook for you to try interactively (here I just show screenshots).\n\nimport gradio as gr\ngr.Interface(fn=predict, inputs=gr.inputs.Image(shape=(512, 512)), outputs=gr.outputs.Label(num_top_classes=3)).launch(share=True)\n\nRunning on local URL:  http://127.0.0.1:7860/\nRunning on public URL: https://10290.gradio.app\n\nThis share link will expire in 72 hours. To get longer links, send an email to: support@gradio.app\n\n(<Flask 'gradio.networking'>, 'http://127.0.0.1:7860/', 'https://10290.gradio.app')\nThat‚Äôs it! The actual creation of the demo takes one line!1\nAll Gradio interfaces are created by constructing a gradio.Interface() object. As you can see in this example, the Interface object takes in the function that we want to make an interface for (usually an ML model inference function), Gradio input components (the number of input components should match the number of parameters of the provided function), and Gradio output components (the number of output components should match the number of values returned by the provided function). Gradio provides components for various types of input and output types. This includes: images (upload, draw, or webcam), video, audio (upload or microphone), textboxes, dataframes, timeseries, generic files, and more! So you should be able to create a Gradio demo for virtually any type of ML task you can think of!\nAfter the gradio.Interface() object is defined, the interface is launched with the launch method.\n\n\nOptional: customizing our Gradio app\nGradio has lots of features that we can use to customize our app. Let‚Äôs go over a few of these features and add them to our demo. All of these features are arguments for the instantiation of the Interface class.\nFirst of all, we can pass in a title and description for our app which goes at the top before our input and output components:\n\ntitle = \"Pet Breed Classifier\"\ndescription = \"A pet breed classifier trained on the Oxford Pets dataset with fastai. Created as a demo for Gradio and HuggingFace Spaces.\"\n\nWe can also put a link at the bottom of our demo. Here I will link to this blog post:\n\narticle=\"<p style='text-align: center'><a href='https://tmabraham.github.io/blog/gradio_hf_spaces_tutorial' target='_blank'>Blog post</a></p>\"\n\nWe can also provide some example inputs that people can try out. Here I have provided an example Siamese cat image, which is in the same directory as my code:\n\nexamples = ['siamese.jpg']\n\nAnother interesting feature that Gradio has is the ability for interpretation so that users can understand what parts of the input are responsible for the output. We‚Äôll use the default interpretation function provided by Gradio but you can use your own as well:\n\ninterpretation='default'\n\nNote that the default interpretation function needs scikit-image to be installed. More information on the interpretation feature is provided here.\nGradio also provides a screenshotting feature that can make it really easy to share your examples and results with others. It is enabled by default.\nFinally, Gradio also supports serving of inference requests with a queue. This can be helpful when your app receives a significant amount of traffic. We‚Äôll enable a queue here:\n\nenable_queue=True\n\nYou can also add custom CSS for your Gradio app but we‚Äôll not do that here (my CSS skills are essentially non-existent! üòÇ). Additionally, you can set live=True so that it will automatically submit when you make a change to the input, but removes the Submit button so I won‚Äôt use it for now.\nLet‚Äôs put it all together and make our interface with these additional features:\n\ngr.Interface(fn=predict,inputs=gr.inputs.Image(shape=(512, 512)),outputs=gr.outputs.Label(num_top_classes=3),title=title,description=description,article=article,examples=examples,interpretation=interpretation,enable_queue=enable_queue).launch(share=True)\n\nRunning on local URL:  http://127.0.0.1:7861/\nRunning on public URL: https://30513.gradio.app\n\nThis share link will expire in 72 hours. To get longer links, send an email to: support@gradio.app\n\n(<Flask 'gradio.networking'>,\n 'http://127.0.0.1:7861/',\n 'https://30513.gradio.app')\nCheck the Gradio documentation for more information on how to customize your interface.\nLet‚Äôs put it all into one file which we name app.py:\nimport gradio as gr\nfrom fastai.vision.all import *\nimport skimage\n\nlearn = load_learner('export.pkl')\n\nlabels = learn.dls.vocab\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\ntitle = \"Pet Breed Classifier\"\ndescription = \"A pet breed classifier trained on the Oxford Pets dataset with fastai. Created as a demo for Gradio and HuggingFace Spaces.\"\narticle=\"<p style='text-align: center'><a href='https://tmabraham.github.io/blog/gradio_hf_spaces_tutorial' target='_blank'>Blog post</a></p>\"\nexamples = ['siamese.jpg']\ninterpretation='default'\nenable_queue=True\n\ngr.Interface(fn=predict,inputs=gr.inputs.Image(shape=(512, 512)),outputs=gr.outputs.Label(num_top_classes=3),title=title,description=description,article=article,examples=examples,interpretation=interpretation,enable_queue=enable_queue).launch()\nLet‚Äôs also make a requirements.txt file which will allow us to install the packages that we need in whatever environment we need:\nfastai\nscikit-image\nNow that we have our self-contained web app, we could deploy this on any webserver or cloud platform that we want. But let‚Äôs see how we can use HuggingFace Spaces to deploy it.\n\n\nUsing HuggingFace Spaces\nHuggingFace Spaces is a free-to-use platform for hosting machine learning demos and apps. The Spaces environment provided is a CPU environment with 16 GB RAM and 8 cores. It currently supports the Gradio and Streamlit platforms. Here we will make a Space for our Gradio demo.\nIn order to be able to create a HuggingFace Space, you need to have a HuggingFace account. You can sign up for free here. After signing up, you can create a Space by clicking ‚ÄúNew Space‚Äù on the navigation menu (press on your profile image).\n\nNow you will be shown instructions on how to add your code to this Space from the command line to prepare the demo. Spaces are essentially git repositories (like GitHub) with an app.py file from which the demo is prepared.\nSo we can clone the repository to a local directory,\ngit clone https://huggingface.co/spaces/tmabraham/fastai_pet_classifier\nadd the app.py, requirements.txt, export.pkl, and siamese.jpg files,\ncp app.py fastai_pet_classifier/app.py\ncp requirements.txt fastai_pet_classifier/requirements.txt\ncp export.pkl fastai_pet_classifier/export.pkl\ncp siamese.jpg fastai_pet_classifier/siamese.jpg\nNow before we commit our files, there is something we need to pay attention to. Our model file export.pkl is too big to be handled by git. So instead we need to use git-lfs which you first need to install. If you are on Debian or Ubuntu, you can directly use apt-get install git-lfs (which installs an older version but that‚Äôs not really an issue). For other Linux distros, you can use this script which Jeremy Howard has prepared. For Windows, you can download and run the installer from here. For MacOS, you can do brew install git-lfs.\nOnce you have installed git-lfs, you can then initialize git-lfs in the repository for the app in the following way:\ngit lfs install\ngit lfs track \"*.pkl\"\ngit add .gitattributes\ngit commit -m \"update .gitattributes so git lfs will track .pkl files\"\nNow, we can commit and push the changes to the Space.\ngit commit -am \"let's deploy to huggingface spaces\"\ngit push\nAlternatively, the files can be uploaded via the Spaces UI. When you go to your Space, under ‚ÄúFiles and versions‚Äù, there is an ‚ÄúAdd files‚Äù button which you can use to upload your app files.\nAfter a few moments, during which the app is being built, our demo should show up on the HuggingFace Space.\nThat‚Äôs it! In a few minutes, you trained a pet classifier model with fastai, made a demo interface with Gradio, and hosted it for free on a HuggingFace Space! You can try it out right below or you can try it out on HuggingFace Spaces here. All the files described in this post located here).2\n\n\n\n        \n        \n\n\nIf you are a more advanced user with expertise in web development, you might be interested to know that there is an API available for any Gradio interface (there is a ‚Äúview the api‚Äù link at the bottom of the interface). For example, here is a link to the API docs for my interface. This provides much more flexibility, like interacting with your model very easily in code. For example, here I can take any image URL and get a pet breed prediction with my model.\n\nimport requests\nimport gradio as gr\nfrom IPython.display import Image\nfrom IPython.core.display import HTML \nimage_url = 'https://petkeen.com/wp-content/uploads/2021/05/grey-cat.jpeg'\ndata = gr.processing_utils.encode_url_or_file_to_base64(image_url)\nr = requests.post(url='https://hf.space/embed/tmabraham/fastai_pet_classifier/+/api/predict/', json={\"data\":[data]})\n\n\nprint(f\"The breed of this pet is a {(' '.join(r.json()['data'][0]['label'].split('_')))}:\")\ndisplay(Image(url=image_url, width=475))\nprint('Original JSON returned from the request: ', json.dumps(r.json(), indent=2))\n\nThe breed of this pet is a British Shorthair:\n\n\n\n\n\nOriginal JSON returned from the request:  {\n  \"data\": [\n    {\n      \"label\": \"British_Shorthair\",\n      \"confidences\": [\n        {\n          \"label\": \"British_Shorthair\",\n          \"confidence\": 0.9997965693473816\n        },\n        {\n          \"label\": \"Russian_Blue\",\n          \"confidence\": 0.00019805884221568704\n        },\n        {\n          \"label\": \"Sphynx\",\n          \"confidence\": 2.037774265772896e-06\n        }\n      ]\n    }\n  ],\n  \"flag_index\": null,\n  \"updated_state\": null,\n  \"durations\": [\n    0.09037947654724121\n  ],\n  \"avg_durations\": [\n    0.13969146820806688\n  ]\n}\n\n\nSome examples of using the API in custom websites is provided here (put together by Jeremy Howard and members of the fast.ai community).\nFor more information on Gradio and HuggingFace Spaces, check the relevant docs and forums:\n\nGradio documentation\nHuggingFace Spaces documentation\nGradio Guides\nHuggingFace Forums (for Spaces and Gradio Q&A)\n\nThere are so many features of Gradio and Spaces that I haven‚Äôt mentioned here (like multiple models per demo, the Blocks feature, etc.). Additionally, both Gradio and HuggingFace Spaces are in active development and new, amazing features afe always being added by tje Gradio and HuggingFace teams! For this reason, I also recommend following HuggingFace and Gradio on Twitter to hear about the latest updates and newest features.\nI‚Äôll end by sharing a quick example prediction by my pet classifier of our new kitten! Her name is Mimi and, as predicted by my classifier here, she is indeed a Ragdoll kitten!:\n\n\n\nAcknowledgements\nThanks to Zach Mueller, Ahsen Khaliq, Abhishek Thakur, and Jeremy Howard for reviewing my blog post.\n\n\nFootnotes\n\n\n\n\n\nFootnotes\n\n\nOne of the developers of Gradio created a simple Python module to easily create Gradio demos for fastai Learner objects. Check it out here. It currently only supports image-to-label interfaces but it could likely be expanded to other tasks fairly easily.‚Ü©Ô∏é\nRecently, HuggingFace added direct support for pushing and loading fastai models to the HuggingFace Hub with the push_to_hub_fastai and from_pretrained_fastai functions, respectively. This can make creating Spaces much easier, since you can just load it in the Space and not have to add it to the repository with git-lfs. See an example of this over here.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/ddpo.html",
    "href": "posts/ddpo.html",
    "title": "Reinforcement Learning for Diffusion Models from Scratch",
    "section": "",
    "text": "Over the past year we have seen the rise of generative AI that has mainly come in two forms:\n\nText-to-image generation powered by diffusion models like Stable Diffusion and DALL-E 2\nLanguage models like ChatGPT and LLaMA-2\n\nIt turns out one of the key ingredients for the mainstream success of language models is the use of Reinforcement Learning from Human Feedback (RLHF) where language models are trained with human feedback to produce outputs that users are more likely to prefer. This has enabled these language models to more easily follow instructions, making these models significantly more accessible. Therefore the question arises if RLHF can be applied to diffusion models. This is a natural question to ask, since text-to-image diffusion models also struggle to follow prompts and tend to need prompt engineering skills in order to get desired results. A paper in May 2023 by the Levine Lab at UC Berkeley explored how the RLHF paradigm can be applied to diffusion models, resulting in an algorithm called DDPO. Here we‚Äôll walk through a simple implementation of this DDPO algorithm. Let‚Äôs get started!\nFirst let‚Äôs start with some basic imports:\n\nimport os\nimport requests\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport clip # pip install git+https://github.com/openai/CLIP.git\nimport torch\nimport random\nimport math\nimport wandb\nfrom torch import nn\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\nfrom PIL import Image\nfrom fastprogress import progress_bar, master_bar\n\nLet‚Äôs load our Stable Diffusion model. Let‚Äôs also enable some performance optimizations (TF32 support, attention slicing, memory-efficient xformers attention) that will make it faster to work with our Stable Diffusion model for training.\n\ntorch.backends.cuda.matmul.allow_tf32 = True\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(\"cuda\")\npipe.enable_attention_slicing()\npipe.enable_xformers_memory_efficient_attention()\npipe.text_encoder.requires_grad_(False)\npipe.vae.requires_grad_(False)\n\n\n\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n\n\nWe‚Äôre using the diffusers library, which provides a simple-to-use interface for sampling the Stable Diffusion model using their pipeline:\n\nprompt = \"a photograph of an astronaut riding a horse\"\nimg = pipe(prompt).images[0]\n\n\n\n\n\nimg\n\n\n\n\nOkay then, we want to improve the images coming out of our model. In order to do so we should have some sort of score for the image that we can later optimize for. This score could represent how aesthetic the image is. This is frankly something that is quite subjective, and there is no mathematical equation for the aestheticness of an image. Instead we will use LAION‚Äôs aesthetic predictor, which was trained on thousands of human aesthetic ratings of AI-generated images and is a linear model on top of CLIP features. Below is the standard inference code for the aesthetic predictor model:\n\nclass MLP(nn.Module):\n    def __init__(self, input_size, xcol='emb', ycol='avg_rating'):\n        super().__init__()\n        self.input_size = input_size\n        self.xcol = xcol\n        self.ycol = ycol\n        self.layers = nn.Sequential(\n            nn.Linear(self.input_size, 1024),\n            #nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(1024, 128),\n            #nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            #nn.ReLU(),\n            nn.Dropout(0.1),\n\n            nn.Linear(64, 16),\n            #nn.ReLU(),\n\n            nn.Linear(16, 1)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n\ndef load_aesthetic_model_weights(cache=\".\"):\n    weights_fname = \"sac+logos+ava1-l14-linearMSE.pth\"\n    loadpath = os.path.join(cache, weights_fname)\n\n    if not os.path.exists(loadpath):\n        url = (\n            \"https://github.com/christophschuhmann/\"\n            f\"improved-aesthetic-predictor/blob/main/{weights_fname}?raw=true\"\n        )\n        r = requests.get(url)\n\n        with open(loadpath, \"wb\") as f:\n            f.write(r.content)\n\n    weights = torch.load(loadpath, map_location=torch.device(\"cpu\"))\n    return weights\n\n\ndef aesthetic_model_normalize(a, axis=-1, order=2):\n    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n    l2[l2 == 0] = 1\n    return a / np.expand_dims(l2, axis)\n\nWe need the CLIP model, whose features will be passed into our aesthetic predictor:\n\nclip_model, preprocess = clip.load(\"ViT-L/14\", device=\"cuda\")\n\n\naesthetic_model = MLP(768)\n\n\naesthetic_model.load_state_dict(load_aesthetic_model_weights())\naesthetic_model.cuda()\n\nMLP(\n  (layers): Sequential(\n    (0): Linear(in_features=768, out_features=1024, bias=True)\n    (1): Dropout(p=0.2, inplace=False)\n    (2): Linear(in_features=1024, out_features=128, bias=True)\n    (3): Dropout(p=0.2, inplace=False)\n    (4): Linear(in_features=128, out_features=64, bias=True)\n    (5): Dropout(p=0.1, inplace=False)\n    (6): Linear(in_features=64, out_features=16, bias=True)\n    (7): Linear(in_features=16, out_features=1, bias=True)\n  )\n)\n\n\n\nimage = preprocess(img).unsqueeze(0).cuda()\nwith torch.no_grad(): image_features = clip_model.encode_image(image)\n\n\nim_emb_arr = aesthetic_model_normalize(image_features.cpu().detach().numpy())\nprediction = aesthetic_model(torch.from_numpy(im_emb_arr).float().cuda())\n\n\nprint(f'Aesthetic score: {prediction}')\n\nAesthetic score: tensor([[5.6835]], device='cuda:0', grad_fn=<AddmmBackward0>)\n\n\nJust like, that, we get the aesthetic score given with this predictor. Let‚Äôs package this code into a function:\n\ndef aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model):\n    image = preprocess(img).unsqueeze(0).cuda()\n    with torch.no_grad(): image_features = clip_model.encode_image(image)\n    im_emb_arr = aesthetic_model_normalize(image_features.cpu().detach().numpy())\n    prediction = aesthetic_model(torch.from_numpy(im_emb_arr).float().cuda())\n    return prediction\n\n\nprompt = \"a horse\"\nimg = pipe(prompt).images[0]\nprint(f'Aesthetic score: {aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[0][0]}')\nimg\n\n\n\n\nAesthetic score: 5.228103160858154\n\n\n\n\n\n\nprompt = \"a beautiful, exquisite portrait of a horse, 4k, unreal engine\"\nimg = pipe(prompt).images[0]\nprint(f'Aesthetic score: {aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[0][0]}')\nimg\n\n\n\n\nAesthetic score: 5.3236589431762695\n\n\n\n\n\n\nprompt = \"a very ugly photograph of a donkey\"\nimg = pipe(prompt).images[0]\nprint(f'Aesthetic score: {aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[0][0]}')\nimg\n\n\n\n\nAesthetic score: 5.207968711853027\n\n\n\n\n\nYou can see if you prompt for something ‚Äúugly‚Äù we do get an image with a lower score. But the variation of the score doesn‚Äôt tend to be much there seems to be a sort of average aesthetic score that most images fluctuate around (~5.3). Can we increase the average aesthetic score of the images produced by Stable Diffusion? That is what we will attempt to do using reinforcement learning (RL)."
  },
  {
    "objectID": "posts/ddpo.html#what-if-we-could-optimize-the-aesthetic-score",
    "href": "posts/ddpo.html#what-if-we-could-optimize-the-aesthetic-score",
    "title": "Reinforcement Learning for Diffusion Models from Scratch",
    "section": "What if we could optimize the aesthetic score?",
    "text": "What if we could optimize the aesthetic score?\nNow that we have some sort of measure of quality of our image, our aesthetic score, we can optimize for it. In the RL literature, this measure of quality that we are optimizing for is referred to as the reward. The goal of RL algorithms is to optimize the reward. We will see how DDPO does this for diffusion models.\nBefore we go down the RL route though, it is worth examining if there are alternative approaches. Diffusion models, after all, are an extremely versatile framework, and people have been incorporating different constraints and forms of guidance during sampling in order to achieve desired results. Let‚Äôs do a quick refresher about diffusion models and how guidance is applied.\n\nDiffusion model refresher\nA diffusion model is described by a forward and reverse process. The forward process is when we start out with a clean image \\(\\mathbf{x}_0\\) and repeatedly add Gaussian noise \\(\\epsilon_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) to give use noisier and noisier images \\(\\mathbf{x}_t\\). This is described by the following:\n\\[ q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\]\nwhere \\(\\beta_t\\) is a predefined monotonically increasing variance schedule. The forward process runs for a total of \\(T\\) timesteps and finally ends with pure noise \\(\\mathbf{x}_T\\). The reverse process starts with pure noise \\(\\mathbf{x}_T\\) and uses a neural network to repeatedly denoise the image giving us \\(\\mathbf{x}_t\\). The end of the reverse process gives us back our samples \\(\\mathbf{x}_0\\). This is described as follows:\n\\[ p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_t, t), \\tilde{\\beta}_t \\mathbf{I}) \\]\nwhere \\(\\tilde{\\beta}_t\\) is the variance schedule for the reverse schedule and \\(\\mu_\\theta(\\mathbf{x}_t, t)\\) is the denoising neural network. Note that the denoising neural network can be reparameterized to predict the noise \\(\\epsilon_t\\) in the image. So instead of predicting the denoised image \\(\\hat{\\mathbf{x}}_0\\) directly, we can predict the noise in the image and subtract it out to get \\(\\hat{\\mathbf{x}}_0\\). We train the reparameterized denoising neural network \\(\\epsilon_\\theta(\\mathbf{x}_t, t)\\) in the reverse diffusion process with a simple MSE loss:\n\\[ L_{simple} = \\mathbb{E}_{t,\\mathbf{x}_t, \\epsilon_t} || \\epsilon_t - \\epsilon_\\theta(\\mathbf{x}_t, t) || \\]\nIn practice, training and sampling is quite simple. During each training step, a random image \\(\\mathbf{x}_0\\) and timestep \\(t\\) is selected, the forward process starts from \\(\\mathbf{x}_0\\) till timestep \\(t\\) to get \\(\\mathbf{x}_t\\) using the noise \\(\\epsilon_t\\), this is passed into our denoising model, and the MSE between the \\(\\epsilon_t\\) used to calculate \\(\\mathbf{x}_t\\) and the predicted \\(\\epsilon_\\theta(\\mathbf{x}_t, t)\\) is optimized. During sampling, we start out with random Gaussian noise \\(\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) and the denoising neural network is repeatedly applied to give us \\(\\mathbf{x}_{t-1}\\) until we reach our sample \\(\\mathbf{x}_0\\).\nIt is also worth noting the score matching intuition behind diffusion models. Specifically, \\(\\epsilon_\\theta(\\mathbf(x)_t, t)\\) turns out to actually be an estimate (up to a factor) of the ‚Äúscore function‚Äù \\(\\nabla_\\mathbf{x} \\log p(\\mathbf{x})\\). Basically, what this means is that when we sample from a diffusion model, we are iteratively taking steps in the direction of this score function, which is this gradient of the likelihood. So the sampling is very much like an optimization problem.\nIf any of this is unfamiliar to you, I recommend checking out the fast.ai course on the subject (I am somewhat biased though given I co-taught the class!).\nLet‚Äôs now discuss how additional constraints and guidance can be added during diffusion sampling. Basically, we want to model \\(p(\\mathbf{x} | \\mathbf{y})\\) where \\(\\mathbf{y}\\) is some sort of condition or constraint (for example a class condition). In diffusion models, we could instead try to estimate \\(\\nabla_\\mathbf{x} \\log p(\\mathbf{x} | \\mathbf{y})\\) and use this in sampling. This can be expressed differently using Bayes‚Äô Rule:\n\\[ \\begin{gathered}\np(\\mathbf{x} \\mid  \\mathbf{y})=\\frac{p( \\mathbf{y} \\mid \\mathbf{x}) \\cdot p(\\mathbf{x})}{p( \\mathbf{y})} \\\\\n\\Longrightarrow \\log p(\\mathbf{x} \\mid  \\mathbf{y})=\\log p( \\mathbf{y} \\mid \\mathbf{x})+\\log p(\\mathbf{x})-\\log p( \\mathbf{y}) \\\\\n\\Longrightarrow \\nabla_\\mathbf{x} \\log p(\\mathbf{x} \\mid  \\mathbf{y})=\\nabla_\\mathbf{x} \\log p( \\mathbf{y} \\mid \\mathbf{x})+\\nabla_\\mathbf{x} \\log p(\\mathbf{x})\n\\end{gathered} \\]\nThe second term is our score function that is already being estimated by our diffusion model \\(\\epsilon_\\theta(\\mathbf{x}_t, t)\\). The first term, however, is the gradient of the log likelihood of a classifier \\(p(\\mathbf{x} \\mid \\mathbf{y})\\) with respect to \\(\\mathbf{x}\\). What this means is that during diffusion model sampling, if we use a modified \\(\\hat{\\epsilon}_\\theta(\\mathbf{x}_t,t)\\) with the classifier gradient added to it, we can get samples that adhere to the desired condition.\nMore broadly, losses can be applied to the noisy images and its gradient can be added to the score function/denoising network output to try to obtain images that better adhere to some desired constraints. This is the idea behind CLIP-guided diffusion, for example, where the similarity between the CLIP text embedding of a prompt and the CLIP image embedding of the images during sampling are maximized.\nCan we use guidance to get diffusion models to generate more aesthetic images that better adhere to user prompts? Potentially yes, but there are many challenges that may make it undesirable.\nStrictly speaking, the proper way to perform classifier guidance is to use a classifier trained on either the noisy images \\(\\mathbf{x}_t\\) or the predicted denoised images \\(\\hat{\\mathbf{x}}_0\\) (which tend to be blurry, especially early on in sampling), which is what the original classifier guidance paper demonstrates. Note, it is possible to use classifiers and other models trained on regular images and get reasonable results for guidance, even though noisy or blurry images will likely be out-of-distribution. For example CLIP isn‚Äôt trained on noisy images but is used in CLIP-guided diffusion. But often to get reasonable results, various hacks and tricks are required (in the case of CLIP-guided diffusion, the use of this technique fell out of popularity once diffusion models properly conditioned on CLIP features like Stable Diffusion were developed, and CLIP guidance applied on top of models like Stable Diffusion often showed minimal benefit).\nAdditionally, note that guidance requires the calculation of whatever guidance loss we have and autograd of that loss at each step in the sampling process. This can add a significant overhead to the sampling time compared to guidance-free sampling. The situation is made worse with latent diffusion models like Stable Diffusion, where the latents often need to be decoded to full images in order to apply the classifier/loss, resulting in additional computational overhead.\nFor these reasons, we will instead apply reinforcement learning to obtain a diffusion model after optimizing arbitrary constraints (the reward function), such as aesthetic scores. As we will see, the generated images from the starting diffusion model are passed into the reward function, so there is no concern of images being out-of-distribution. Additionally, we will obtain a diffusion model that provides higher scoring images directly, not through sampling changes like guidance.\nOkay let‚Äôs proceed with trying to apply RL to diffusion models. First we‚Äôll create a dataset generator - Stable Diffusion generated images given some prompts. We‚Äôll use animal prompts:\n\n!wget https://raw.githubusercontent.com/formigone/tf-imagenet/master/LOC_synset_mapping.txt\n\n--2023-06-13 09:54:06--  https://raw.githubusercontent.com/formigone/tf-imagenet/master/LOC_synset_mapping.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 31675 (31K) [text/plain]\nSaving to: ‚ÄòLOC_synset_mapping.txt.14‚Äô\n\nLOC_synset_mapping. 100%[===================>]  30.93K  --.-KB/s    in 0.006s  \n\n2023-06-13 09:54:06 (5.24 MB/s) - ‚ÄòLOC_synset_mapping.txt.14‚Äô saved [31675/31675]\n\n\n\n\nsynsets = {k:v for k,v in [o.split(',')[0].split(' ', maxsplit=1) for o in Path('LOC_synset_mapping.txt').read_text().splitlines()]}\n\nimagenet_classes = list(synsets.values())\n\n\ndef imagenet_animal_prompts():\n    animal = random.choice(imagenet_classes[:397])\n    prompts = f'{animal}'\n    return prompts\n\n\nimagenet_animal_prompts()\n\n'sea urchin'\n\n\nPut into a dataset class:\n\nclass PromptDataset(torch.utils.data.Dataset):\n    def __init__(self, prompt_fn, num):\n        super().__init__()\n        self.prompt_fn = prompt_fn\n        self.num = num\n        \n    def __len__(self): return self.num\n    def __getitem__(self, x): return self.prompt_fn()\n\nNext let‚Äôs set up our sampling loop. For simplicity, we‚Äôll just use the DDIM scheduler:\n\npipe.scheduler = DDIMScheduler(\n    num_train_timesteps=pipe.scheduler.num_train_timesteps,\n    beta_start=pipe.scheduler.beta_start,\n    beta_end=pipe.scheduler.beta_end,\n    beta_schedule=pipe.scheduler.beta_schedule,\n    trained_betas=pipe.scheduler.trained_betas,\n    clip_sample=pipe.scheduler.clip_sample,\n    set_alpha_to_one=pipe.scheduler.set_alpha_to_one,\n    steps_offset=pipe.scheduler.steps_offset,\n    prediction_type=pipe.scheduler.prediction_type\n)\n\nBelow we have a sampling function, that also gives us intermediate timesteps. Again this is a pretty standard diffusion sampling loop, check out HuggingFace blog post for more information.\n\n@torch.no_grad()\ndef sd_sample(prompts, pipe, height, width, guidance_scale, num_inference_steps, eta, device):\n    scheduler = pipe.scheduler\n    unet = pipe.unet\n    text_embeddings = pipe._encode_prompt(prompts,device, 1, do_classifier_free_guidance=guidance_scale > 1.0)\n\n    scheduler.set_timesteps(num_inference_steps, device=device)\n    latents = torch.randn((len(prompts), unet.in_channels, height//8, width//8)).to(device)\n\n    all_step_preds = []\n\n    for i, t in enumerate(progress_bar(scheduler.timesteps)):\n        input = torch.cat([latents] * 2)\n        input = scheduler.scale_model_input(input, t)\n\n        # predict the noise residual\n        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n        # perform guidance\n        pred_uncond, pred_text = pred.chunk(2)\n        pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n        # compute the \"previous\" noisy sample\n        scheduler_output = scheduler.step(pred, t, latents, eta)\n\n        all_step_preds.append(scheduler_output)\n        latents = scheduler_output.prev_sample\n    \n    return latents, all_step_preds\n\n\npreds, all_step_preds = sd_sample([prompt]*2, pipe, 512, 512, 7.5, 50, 1, 'cuda')\n\n\n\n\n\n\n    \n      \n      0.00% [0/50 00:00<?]\n    \n    \n\n\nThe sampling function only gives us latents, and they need to be decoded by a VAE to get the final images:\n\n@torch.no_grad()\ndef decoding_fn(latents,pipe):\n    images = pipe.vae.decode(1 / 0.18215 * latents.cuda()).sample\n    images = (images / 2 + 0.5).clamp(0, 1)\n    images = images.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (images * 255).round().astype(\"uint8\")\n    return images\n\n\nImage.fromarray(decoding_fn(preds,pipe)[0])\n\n\n\n\nLet‚Äôs again calculate the aesthetic score. We have to make a slight modification to our aesthetic_scoring function so it can handle batches.\n\ndef aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model):    \n    imgs = torch.stack([preprocess(Image.fromarray(img)).cuda() for img in imgs])\n    with torch.no_grad(): image_features = clip_model.encode_image(imgs)\n    im_emb_arr = aesthetic_model_normalize(image_features.cpu().detach().numpy())\n    prediction = aesthetic_model(torch.from_numpy(im_emb_arr).float().cuda())\n    return prediction\n\n\nimgs = decoding_fn(preds,pipe)\naesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)\n\ntensor([[5.1635],\n        [5.4475]], device='cuda:0', grad_fn=<AddmmBackward0>)\n\n\nNow we can initialize our dataset generator, which provides prompts that we sample with, generate images, and pass into the aesthetic predictor in just a few lines of code:\n\ntrain_set = PromptDataset(imagenet_animal_prompts, 1000)\ntrain_dl = torch.utils.data.DataLoader(train_set, batch_size=2, shuffle=True, num_workers=0)\n\n\nprompts = next(iter(train_dl))\npreds, all_step_preds = sd_sample(prompts, pipe, 512, 512, 7.5, 50, 1, 'cuda')\nimgs = decoding_fn(preds,pipe)\nrewards = aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)\n\n\n\n\n\n\n    \n      \n      0.00% [0/50 00:00<?]\n    \n    \n\n\n\nindex = torch.where(rewards == rewards.min())[0][0]\nprint(prompts[index])\nImage.fromarray(imgs[index])\n\npolecat\n\n\n\n\n\n\nindex = torch.where(rewards == rewards.max())[0][0]\nprint(prompts[index])\nImage.fromarray(imgs[index])\n\ngarter snake\n\n\n\n\n\nOnce again, the aesthetic predictor provides rewards and these are used in optimization with RL. Our goal is to maximize the reward!\nFor stability during RL training, the rewards are usually normalized. There are different ways of doing this, but the DDPO paper utilizes a simple approach, which is to normalize based on unique prompts. Basically, a queue is set up with prompts and the corresponding rewards, which some given buffer size. When a new (prompt, reward) pair is obtained, it is added to the queue. If the queue for that unique prompt is not long enough, the reward is just normalized over the whole batch. Else, it is normalized based on the statistics for that specific prompt. This is implemented below:\n\nfrom collections import deque\nclass PerPromptStatTracker:\n    def __init__(self, buffer_size, min_count):\n        self.buffer_size = buffer_size\n        self.min_count = min_count\n        self.stats = {}\n\n    def update(self, prompts, rewards):\n        unique = np.unique(prompts)\n        advantages = np.empty_like(rewards)\n        for prompt in unique:\n            prompt_rewards = rewards[prompts == prompt]\n            if prompt not in self.stats:\n                self.stats[prompt] = deque(maxlen=self.buffer_size)\n            self.stats[prompt].extend(prompt_rewards)\n\n            if len(self.stats[prompt]) < self.min_count:\n                mean = np.mean(rewards)\n                std = np.std(rewards) + 1e-6\n            else:\n                mean = np.mean(self.stats[prompt])\n                std = np.std(self.stats[prompt]) + 1e-6\n            advantages[prompts == prompt] = (prompt_rewards - mean) / std\n\n        return advantages\n\n\nper_prompt_stat_tracker = PerPromptStatTracker(buffer_size=32, min_count=16)\n\n\nrewards.squeeze().cpu()\n\ntensor([5.4786, 5.5764], grad_fn=<ToCopyBackward0>)\n\n\n\nadvantages = per_prompt_stat_tracker.update(np.array(prompts), rewards.squeeze().cpu().detach().numpy())\n\n\nadvantages\n\narray([-0.99998444,  0.9999747 ], dtype=float32)\n\n\n\nper_prompt_stat_tracker.stats\n\n{'garter snake': deque([5.576433], maxlen=32),\n 'polecat': deque([5.478563], maxlen=32)}\n\n\nWe have the reward normalization set up. Let‚Äôs look at some of the intermediate timesteps:\n\nImage.fromarray(decoding_fn(all_step_preds[0].prev_sample, pipe)[0])\n\n\n\n\n\nImage.fromarray(decoding_fn(all_step_preds[30].prev_sample, pipe)[0])\n\n\n\n\nAs expected, we get pure noise (in the latent space, it looks somewhat different when decoded by the Stable Diffusion VAE) at the beginning of sampling, and as sampling progresses, the images starts to take shape."
  },
  {
    "objectID": "posts/ddpo.html#the-ddpo-algorithm",
    "href": "posts/ddpo.html#the-ddpo-algorithm",
    "title": "Reinforcement Learning for Diffusion Models from Scratch",
    "section": "The DDPO algorithm",
    "text": "The DDPO algorithm\nOkay let‚Äôs now dig into how reinforcement learning works and derive the DDPO objective. A reminder that what we want to do is to maximize the reward signal. We can mathematically express this as follows:\n\\[ \\theta^\\star = \\arg \\max_\\theta \\mathbb{E}_{\\mathbf{x}_0 \\sim p_\\theta(\\cdot \\mid \\mathbf{c})} [r(\\mathbf{x}_0 \\mid \\mathbf{c})] \\]\nwhere \\(\\theta\\) is the weights of our diffusion model, \\(\\mathbf{c}\\) is some conditioning for the diffusion model, and \\(r(\\cdot)\\) is our reward function.\nIt would be nice to directly maximize for \\(r(\\cdot)\\) and if our model was a single evaluation of a neural network, we could simply backpropagate through the neural network and use an optimizer to update the weights. But that‚Äôs not what happens with a diffusion model! We have multiple timesteps for which we apply our denoising neural network. This constructs a trajectory as its known in the RL literature. In standard RL literature, our trajectory is composed of states and actions. A model that we are optimizing provides the next action given the current state, and this model is referred to as the policy. This framework is known as a Markov Decision Process (MDP). Note that in the general MDP framework, a reward is usually given after each action, and we optimize over the sum of rewards over the whole trajectory.\nWe can easily describe diffusion models as an MDP, which will allow us to use standard results in RL for diffusion model optimization.\n\\[ \\begin{array}{rrr}\n\\mathbf{s}_t \\triangleq\\left(\\mathbf{c}, t, \\mathbf{x}_t\\right) & \\pi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right) \\triangleq p^{\\text{diffusion}}_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{c}\\right) & p\\left(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t\\right) \\triangleq\\left(\\delta_{\\mathbf{c}}, \\delta_{t-1}, \\delta_{\\mathbf{x}_{t-1}}\\right) \\\\\n\\mathbf{a}_t \\triangleq \\mathbf{x}_{t-1} & \\rho_0\\left(\\mathbf{s}_0\\right) \\triangleq\\left(p(\\mathbf{c}), \\delta_T, \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\right) & R\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right) \\triangleq \\begin{cases}r\\left(\\mathbf{x}_0, \\mathbf{c}\\right) & \\text { if } t=0 \\\\\n0 & \\text { otherwise }\\end{cases}\n\\end{array} \\]\n\\[ \\mathcal{J}(\\theta)=\\mathbb{E}_{\\tau \\sim p(\\cdot \\mid \\pi)}\\left[\\sum_{t=0}^T R\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)\\right] = \\mathbb{E}_{\\mathbf{x}_0 \\sim p_\\theta(\\cdot \\mid \\mathbf{c})} \\left[r(\\mathbf{x}_0 \\mid \\mathbf{c})\\right] \\]\n\\(\\mathbf{s}_t\\) is the state, which is just the current noisy image \\(\\mathbf{x}_t\\) (along with the timestep and condition info). \\(\\mathbf{a}_t\\) is the action, which is the slightly less noisy image \\(\\mathbf{x}_{t-1}\\). \\(\\pi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)\\) is the policy that takes the current state and provides the next action, which is our diffusion model \\(p^{\\text{diffusion}}_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{c}\\right)\\). \\(\\rho_0\\left(\\mathbf{s}_0\\right)\\) is the distribution of the initial states, which in this case is the same distribution for \\(\\mathbf{x}_T\\), a standard isotropic normal distribution, with the timestep always being \\(T\\) and the conditioning having whatever prior distribution as in the dataset. \\(p\\left(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t\\right)\\) is giving \\(\\mathbf{s}_{t+1}\\) given the current state and action, and is basically just saying it always goes to the state associated with \\(\\mathbf{x}_{t-1}\\). \\(R\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)\\) is the reward after each action/state (which is zero until the very last step when the generation is complete).The entire trajectory can be represented as \\(\\tau\\) and the probability density for trajectories as \\(p_\\theta(\\mathbf{s}_0, \\mathbf{a}_0, \\cdot \\cdot \\cdot, \\mathbf{s}_T, \\mathbf{a}_T) = p_\\theta(\\tau)\\) Note this \\(p_\\theta\\) is different from the diffusion model \\(p_\\theta\\) as it is a function of \\(\\tau\\), a bit confusing! So going forward the diffusion model is referred to as \\(p^\\text{diffusion}_\\theta\\).\n\nOkay, with this framework in place it becomes trivial to apply standard policy gradient optimization methods like REINFORCE and proximal policy optimization (PPO). Let‚Äôs go over these two algorithms now. I have also written a more principled from-scratch derivation over [here]\nWe‚Äôll start by looking at the general case of taking the gradient of the expectation over \\(p_\\theta(\\mathbf{x})\\) of some function \\(f(\\mathbf{x})\\).\n\\[\n\\begin{align}\n\\nabla_\\theta \\mathbb{E}_{p_{\\theta}}[f(\\mathbf{x})]\n&= \\nabla_\\theta \\int p_{\\theta}(\\mathbf{x}) f(\\mathbf{x}) d\\mathbf{x} \\\\\n&= \\int \\nabla_\\theta p_{\\theta}(\\mathbf{x}) f(\\mathbf{x}) d\\mathbf{x} \\\\\n&= \\int \\frac{p_{\\theta}(\\mathbf{x})}{p_{\\theta}(\\mathbf{x})} \\nabla_\\theta p_{\\theta}(\\mathbf{x}) f(\\mathbf{x}) d\\mathbf{x} \\\\\n&= \\int p_{\\theta}(\\mathbf{x}) \\frac{\\nabla_\\theta p_{\\theta}(\\mathbf{x})}{p_{\\theta}(\\mathbf{x})} f(\\mathbf{x}) d\\mathbf{x} \\\\\n&= \\int p_{\\theta}(\\mathbf{x}) \\nabla_\\theta \\log p_{\\theta}(\\mathbf{x}) f(\\mathbf{x}) d\\mathbf{x} \\\\\n&= \\mathbb{E}_{p_{\\theta}} \\big[ \\nabla_\\theta \\log p_{\\theta}(\\mathbf{x})f(\\mathbf{x}) \\big]\n\\end{align}\n\\]\nThis is referred to as the score function gradient estimator.\nLet‚Äôs think more about what this is doing. We want to calculate the gradient of \\(\\mathbb{E}_{p_{\\theta}}[f(\\mathbf{x})]\\) with respect to \\(\\theta\\). That is, we want to know how we can change \\(\\theta\\) such that we get samples from \\(p_\\theta(\\mathbf{x}\\) that on average give higher \\(f(\\mathbf{x})\\) values. What this estimator says is that we can take the gradient \\(\\nabla_\\theta \\log p_{\\theta}(\\mathbf{x})\\) (which tells you how to change \\(\\theta\\) to increase the likelihood of \\(\\mathbf{x}\\) under your distribtuion \\(p_\\theta(\\mathbf{x})\\)), and weight it with \\(f(\\mathbf{x})\\). So if this is being used in gradient ascent for example, we are placing more weight on updates that make high-scoring \\(\\mathbf{x}\\) samples more likely under our model \\(p_\\theta(\\mathbf{x})\\).\nWhen we apply this to the MDP framework and simplify, we can get our policy gradient:\n\\[ \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)} \\left[\\left(\\sum^T_{t=0} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right) \\right) \\left(\\sum^T_{t=0}R(\\mathbf{s}_t, \\mathbf{a}_t) \\right) \\right] \\]\nThis gradient is referred to as the REINFORCE gradient and is only one type of policy gradient that could be used. Of course, this policy gradient is then used to update the weights of our model using gradient ascent:\n\\[ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\mathcal{J}(\\theta) \\]\nwhere \\(\\alpha\\) is some learning rate.\nOne implementation point is that the expectation is over the trajectories but of course we can‚Äôt take and sum over all possible trajectories. The expectation is estimated with just the sampled trajectories in the currenty batch. One other implementation point to mention: we could calculate our gradient and then pass that gradient to our optimizer, or we could let autograd handle the calculation of the gradient by constructing a loss function and treating it as a standard training loop. The latter is what is done in practice even though it is not explicitly mentioned often in the papers. So our loss function is:\n\\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ - \\left(\\sum^T_{t=0} \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right) \\right) \\left(\\sum^T_{t=0}R(\\mathbf{s}_t, \\mathbf{a}_t) \\right) \\right] \\]\nAgain, let‚Äôs reinforce the intuition behind REINFORCE gradient/loss function (pun definitely intended). You can see the loss looks very much like a negative log-likelihood loss, with the actions as our target. The diference here is that it is weighted by the reward. What this loss is doing is trying to make high-reward trajectories more likely and low-reward trajectories less likely.\n\nOkay so we can simply plug in our diffusion model terms based on how it fits into the MDP framework, which we described earlier. We get:\n\\[ \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E} \\left[\\left(\\sum^T_{t=0} \\nabla_\\theta \\log p^\\text{diffusion}_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{c}, t, \\mathbf{x}_t\\right) \\right) r(\\mathbf{x}_0, \\mathbf{c}) \\right] \\]\nThis objective and gradient estimator is referred in the paper as DDPOSF.\nOne challenge with this approach is that for each optimization step, the sampling from the current iteration of the model needs to be performed, we need to re-calculate \\(\\mathbf{x}_t\\) as it comes from the current version of the model. This can be computationally expensive and wasteful, as the samples collected with previous iterations of the model cannot be used to learn.\nOne trick to address this is known as importance sampling. This relies on the following identity (trivial to demonstrate based on the expectation definition):\n\\[ \\mathbb{E}_{x\\sim p(x)} \\left[f(x)\\right] = \\mathbb{E}_{x\\sim q(x)} \\left[\\frac{p(x)}{q(x)}f(x)\\right] \\]\nApplying importance sampling gives us the following:\n\\[ \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta_{old}} \\left(\\tau \\right)} \\left[\\left(\\sum^T_{t=0} \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)}{\\pi_{\\theta_{old}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)} \\nabla_\\theta \\log \\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right) \\right) \\left(\\sum^T_{t=0}R(\\mathbf{s}_t, \\mathbf{a}_t) \\right) \\right] \\]\nAgain this can be written down as a loss function that we perform gradient descent on (sometimes referred to as the surrogate loss):\n\\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta_{old}} \\left(\\tau \\right)} \\left[-\\left(\\sum^T_{t=0} \\frac{\\pi_\\theta\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)}{\\pi_{\\theta_{old}}\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)} \\right) \\left(\\sum^T_{t=0}R(\\mathbf{s}_t, \\mathbf{a}_t) \\right) \\right] \\]\nAgain, we can plug in the diffusion model terms based on the MDP framework and get this loss function:\n\\[ L(\\theta) = \\mathbb{E} \\left[ - \\sum^T_{t=0} \\frac{p^\\text{diffusion}_\\theta(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t)}{p^\\text{diffusion}_{\\theta_{old}}(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t)} r(\\mathbf{x}_0,\\mathbf{c}) \\right] \\]\nMinimization of this loss function is equivalent to gradient with the following gradient:\n\\[ \\hat g = \\mathbb{E} \\left[\\sum^T_{t=0} \\frac{p^\\text{diffusion}_\\theta(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t)}{p^\\text{diffusion}_{\\theta_{old}}(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t)} \\nabla_\\theta p^\\text{diffusion}_\\theta(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t) r(\\mathbf{x}_0,\\mathbf{c}) \\right] \\]\nNote that the reward \\(r(\\mathbf{x}_0,\\mathbf{c})\\) is usually normalized, and the normalized reward is referred to the advantage \\(A(\\mathbf{x}_0,\\mathbf{c})\\). So the advantage can be negative if it‚Äôs less than average.\nNote that we don‚Äôt want current policy diverge too much from the previous policy, otherwise we may diverge and get a bad policy. To help address this, we can apply clipping to the importance sampling ratio to the loss function:\n\\[ L(\\theta) = \\mathbb{E} \\left[ - \\sum^T_{t=0} \\min \\left(\\frac{p^\\text{diffusion}_\\theta(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t)}{p^\\text{diffusion}_{\\theta_{old}}(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t)} A(\\mathbf{x}_0,\\mathbf{c}), \\mathrm{clip} \\left( \\frac{p^\\text{diffusion}_\\theta(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t)}{p^\\text{diffusion}_{\\theta_{old}}(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t)}, 1-\\epsilon, 1+\\epsilon  \\right) A(\\mathbf{x}_0,\\mathbf{c}) \\right) \\right] \\]\nSo if the policy diverges too much (the ratio is either much larger or much smaller than 1) the loss function is clipped to a certain value and the gradient will be zero and no updates will be made. The below diagram (taken from here) clarifies this further:\n\nNote that DDPO also clips the advantages themselves $ A(_0,) $ in its implementation but this is not described in the paper, so I have not included it in the loss function.\nThe loss function can be written in a way that‚Äôs numerically easier to calculate/more stable (using logs, ignoring the clipping for now):\n\\[ L(\\theta) = \\mathbb{E} \\left[ - \\sum^T_{t=0} \\exp{\\left(\\log p^\\text{diffusion}_\\theta(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t) -\\log p^\\text{diffusion}_{\\theta_{old}}(\\mathbf{x}_{t-1} | \\mathbf{c},t,\\mathbf{x}_t) \\right)} A(\\mathbf{x}_0,\\mathbf{c}) \\right] \\]\nThe objective and gradient estimator described here is referred in the paper as DDPOIS. It is pretty much the same as proximal policy optimization (PPO), applied to diffusion models.\nFor a more complete derivation of the DDPO objective from scratch, see here.\nIn order to start implementing this loss function, let‚Äôs calculate the log probs, which is easy for a normal distribution:\n\ndef calculate_log_probs(prev_sample, prev_sample_mean, std_dev_t):\n    std_dev_t = torch.clip(std_dev_t, 1e-6)\n    log_probs = -((prev_sample.detach() - prev_sample_mean) ** 2) / (2 * std_dev_t ** 2) - torch.log(std_dev_t) - math.log(math.sqrt(2 * math.pi))\n    return log_probs\n\nWe need to get those log probs of the original model so our sampling function should return that. Let‚Äôs update our sampling function to do that:\n\n@torch.no_grad()\ndef sd_sample(prompts, pipe, height, width, guidance_scale, num_inference_steps, eta, device):\n    scheduler = pipe.scheduler\n    unet = pipe.unet\n    text_embeddings = pipe._encode_prompt(prompts,device, 1, do_classifier_free_guidance=guidance_scale > 1.0)\n\n    scheduler.set_timesteps(num_inference_steps, device=device)\n    latents = torch.randn((len(prompts), unet.in_channels, height//8, width//8)).to(device)\n\n    all_step_preds, log_probs = [latents], []\n\n\n    for i, t in enumerate(progress_bar(scheduler.timesteps)):\n        input = torch.cat([latents] * 2)\n        input = scheduler.scale_model_input(input, t)\n\n        # predict the noise residual\n        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n        # perform guidance\n        pred_uncond, pred_text = pred.chunk(2)\n        pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n        # compute the \"previous\" noisy sample mean and variance, and get log probs\n        scheduler_output = scheduler.step(pred, t, latents, eta, variance_noise=0)\n        t_1 = t - scheduler.config.num_train_timesteps // num_inference_steps\n        variance = scheduler._get_variance(t, t_1)\n        std_dev_t = eta * variance ** (0.5)\n        prev_sample_mean = scheduler_output.prev_sample # this is the mean and not full sample since variance is 0\n        prev_sample = prev_sample_mean + torch.randn_like(prev_sample_mean) * std_dev_t # get full sample by adding noise\n        log_probs.append(calculate_log_probs(prev_sample, prev_sample_mean, std_dev_t).mean(dim=tuple(range(1, prev_sample_mean.ndim))))\n\n        all_step_preds.append(prev_sample)\n        latents = prev_sample\n    \n    return latents, torch.stack(all_step_preds), torch.stack(log_probs)\n\nWe can get everything we need for the loss function now (intermediate timesteps, log_probs, rewards):\n\nper_prompt_stat_tracker = PerPromptStatTracker(buffer_size=32, min_count=16)\nprompts = next(iter(train_dl))\npipe.text_encoder.to('cuda')\npipe.vae.to('cuda')\npreds, all_step_preds, log_probs = sd_sample(prompts, pipe, 512, 512, 7.5, 50, 1, 'cuda')\nimgs = decoding_fn(preds,pipe)\nrewards = aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)\nadvantages = torch.from_numpy(per_prompt_stat_tracker.update(np.array(prompts), rewards.squeeze().cpu().detach().numpy())).float().to('cuda')\n\n\n\n\n\n\n    \n      \n      0.00% [0/50 00:00<?]\n    \n    \n\n\nHere‚Äôs a function to compute the loss function:\n\ndef compute_loss(x_t, original_log_probs, advantages, clip_advantages, clip_ratio, prompts, pipe, num_inference_steps, guidance_scale, eta, device):\n    scheduler = pipe.scheduler\n    unet = pipe.unet\n    text_embeddings = pipe._encode_prompt(prompts,device, 1, do_classifier_free_guidance=guidance_scale > 1.0).detach()\n    scheduler.set_timesteps(num_inference_steps, device=device)\n    loss_value = 0.\n    for i, t in enumerate(progress_bar(scheduler.timesteps)):\n        clipped_advantages = torch.clip(advantages, -clip_advantages, clip_advantages).detach()\n        \n        input = torch.cat([x_t[i].detach()] * 2)\n        input = scheduler.scale_model_input(input, t)\n\n        # predict the noise residual\n        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n        # perform guidance\n        pred_uncond, pred_text = pred.chunk(2)\n        pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n        # compute the \"previous\" noisy sample mean and variance, and get log probs\n        scheduler_output = scheduler.step(pred, t, x_t[i].detach(), eta, variance_noise=0)\n        t_1 = t - scheduler.config.num_train_timesteps // num_inference_steps\n        variance = scheduler._get_variance(t, t_1)\n        std_dev_t = eta * variance ** (0.5)\n        prev_sample_mean = scheduler_output.prev_sample\n        current_log_probs = calculate_log_probs(x_t[i+1].detach(), prev_sample_mean, std_dev_t).mean(dim=tuple(range(1, prev_sample_mean.ndim)))\n\n        # calculate loss\n\n        ratio = torch.exp(current_log_probs - original_log_probs[i].detach()) # this is the importance ratio of the new policy to the old policy\n        unclipped_loss = -clipped_advantages * ratio # this is the surrogate loss\n        clipped_loss = -clipped_advantages * torch.clip(ratio, 1. - clip_ratio, 1. + clip_ratio) # this is the surrogate loss, but with artificially clipped ratios\n        loss = torch.max(unclipped_loss, clipped_loss).mean() # we take the max of the clipped and unclipped surrogate losses, and take the mean over the batch\n        loss.backward() # perform backward here, gets accumulated for all the timesteps\n\n        loss_value += loss.item()\n    return loss_value\n\n\nloss = compute_loss(all_step_preds, log_probs, advantages, 10, 1e-4, prompts, pipe, 50, 7.5, 1, 'cuda')\nprint(loss)\n\n\n\n\n\n\n    \n      \n      100.00% [50/50 00:29<00:00]\n    \n    \n\n\n2.6168066263198853"
  },
  {
    "objectID": "posts/ddpo.html#complete-training-loop",
    "href": "posts/ddpo.html#complete-training-loop",
    "title": "Reinforcement Learning for Diffusion Models from Scratch",
    "section": "Complete training loop",
    "text": "Complete training loop\nNow that we can calculate the loss function, we can construct the full training loop. For a single epoch we:\n\nSample from the diffusion model num_samples_per_epochtimes, collecting the intermediate noisy images and log probs.\nPass the samples to the reward model and get reward, which we normalize to get advantage.\nFor num_inner_epochs times, we go over each sample compute the loss, backpropagate, and update our diffusion model.\n\nLet‚Äôs define all our hyperparameters. We will be training Stable Diffusion v1.4 on ImageNet animal prompts as defined earlier, using the LAION Aesthetic classifier as our reward model.\nNote that if we set num_inner_epochs to a high amount, this would be very data-efficient since we would be repeatedly using the previously generated trajectories, but we would probably significantly diverge from the original policy that we used to get those trajectories (or it would at least get clipped frequently in the loss). So we set num_inner_epochs=1. This is still pretty efficient using DDPOIS because otherwise with DDPOSF you‚Äôd need to resample after every iteration (when model is updated) instead of after num_samples_per_epoch=128 that we have here.\n\nnum_samples_per_epoch = 128\nnum_epochs = 50\nnum_inner_epochs = 1\nnum_timesteps = 50\nbatch_size = 4\nimg_size = 512\nlr = 5e-6\nclip_advantages = 10.0\nclip_ratio = 1e-4\ncfg = 5.0\n\nOkay let‚Äôs set everything up:\n\n# group all reward function stuff\ndef reward_fn(imgs, device):\n    clip_model.to(device)\n    aesthetic_model.to(device)\n    rewards = aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)\n    clip_model.to('cpu')\n    aesthetic_model.to('cpu')\n    return rewards\n\n\n# a function to sample from the model and calculate rewards\ndef sample_and_calculate_rewards(prompts, pipe, image_size, cfg, num_timesteps, decoding_fn, reward_fn, device):\n    preds, all_step_preds, log_probs = sd_sample(prompts, pipe, image_size, image_size, cfg, num_timesteps, 1, device)\n    imgs = decoding_fn(preds,pipe)    \n    rewards = reward_fn(imgs, device)\n    return imgs, rewards, all_step_preds, log_probs\n\nHere we create our dataset, which is just randomly chosen prompts:\n\ntrain_set = PromptDataset(imagenet_animal_prompts, num_samples_per_epoch)\ntrain_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\nper_prompt_stat_tracker = PerPromptStatTracker(buffer_size=32, min_count=16)\nsample_prompts = next(iter(train_dl)) # sample a batch of prompts to use for visualization\n\n\npipe.unet.enable_gradient_checkpointing() # more performance optimization\n\n\noptimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=lr, weight_decay=1e-4) # optimizer\n\nNow that we are set up, let‚Äôs start training! You can see the training loop is quite simple!\n\nfor epoch in master_bar(range(num_epochs)):\n    all_step_preds, log_probs, advantages, all_prompts, all_rewards = [], [], [], [], []\n\n    # sampling `num_samples_per_epoch` images and calculating rewards\n    for i, prompts in enumerate(progress_bar(train_dl)):\n        batch_imgs, rewards, batch_all_step_preds, batch_log_probs = sample_and_calculate_rewards(prompts, pipe, img_size, cfg, num_timesteps, decoding_fn, reward_fn, 'cuda')\n        batch_advantages = torch.from_numpy(per_prompt_stat_tracker.update(np.array(prompts), rewards.squeeze().cpu().detach().numpy())).float().to('cuda')\n        all_step_preds.append(batch_all_step_preds)\n        log_probs.append(batch_log_probs)\n        advantages.append(batch_advantages)\n        all_prompts += prompts\n        all_rewards.append(rewards)\n    \n    all_step_preds = torch.cat(all_step_preds, dim=1)\n    log_probs = torch.cat(log_probs, dim=1)\n    advantages = torch.cat(advantages)\n    all_rewards = torch.cat(all_rewards)\n\n    # inner loop\n    for inner_epoch in progress_bar(range(num_inner_epochs)):\n        # chunk them into batches\n        all_step_preds_chunked = torch.chunk(all_step_preds, num_samples_per_epoch // batch_size, dim=1)\n        log_probs_chunked = torch.chunk(log_probs, num_samples_per_epoch // batch_size, dim=1)\n        advantages_chunked = torch.chunk(advantages, num_samples_per_epoch // batch_size, dim=0)\n        \n        # chunk the prompts (list of strings) into batches\n        all_prompts_chunked = [all_prompts[i:i + batch_size] for i in range(0, len(all_prompts), batch_size)]\n        \n        for i in progress_bar(range(len(all_step_preds_chunked))):\n            optimizer.zero_grad()\n\n            loss = compute_loss(all_step_preds_chunked[i], log_probs_chunked[i], \n                                advantages_chunked[i], clip_advantages, clip_ratio, all_prompts_chunked[i], pipe, num_timesteps, cfg, 1, 'cuda'\n                                ) # loss.backward happens inside\n            \n            torch.nn.utils.clip_grad_norm_(pipe.unet.parameters(), 1.0) # gradient clipping\n            optimizer.step()\n\nThat‚Äôs it! Let‚Äôs see what results we get with this code."
  },
  {
    "objectID": "posts/ddpo.html#results",
    "href": "posts/ddpo.html#results",
    "title": "Reinforcement Learning for Diffusion Models from Scratch",
    "section": "Results",
    "text": "Results\n(These results were obtained with this script that included W&B tracking)\nHere is the loss curve from training Stable Diffusion v1.4 with the LAION Aesthetic classifier reward model on ImageNet animal prompts: \nAs you can see, it‚Äôs quite noisy but clearly does decrease. That said, I‚Äôve observed that typically the loss curve can be quite uninformative. Instead, as expected, the reward curve is a better indicator of the performance:\n\nHere you can see a clear increase in average reward over training, which is what we want! So what do the samples look like? Let‚Äôs see:\n\nI‚Äôd argue these images are definitely more aesthetic! It works! A few observations:\n\nSometimes the prompt isn‚Äôt being followed, as seen with the wolf spider example. This is since the reward model does not take into consideration the prompt and only looks at the image, so if generating something that is slightly unrelated to the prompt gives a better reward score then the model will do so. A reward model explicitly taking in the prompt as well and ensuring prompt alignment would be needed. One such model that could be used is PickScore.\nSometimes the RL-trained diffusion model generates pencil/charcoal drawings of the animals, which was observed in the original DDPO paper as well.\nAnother common pattern that the RL-trained diffusion model demonstrates is the generation of narrow depth-of-focus images, which of course clearly looks more artistic and aesthetic."
  },
  {
    "objectID": "posts/ddpo.html#drlx---a-library-for-performing-rlhf-training-with-diffusion-models",
    "href": "posts/ddpo.html#drlx---a-library-for-performing-rlhf-training-with-diffusion-models",
    "title": "Reinforcement Learning for Diffusion Models from Scratch",
    "section": "DRLX - A library for performing RLHF training with diffusion models",
    "text": "DRLX - A library for performing RLHF training with diffusion models\nIn order to make RLHF for diffusion models easy-to-use and accessible, I have co-developed a library with Shahbuland Matiana at CarperAI called DRLX. It implements DDPO, complete with W&B experiment tracking, distributed GPU training support, and other features. We also will be implementing other RL algorithms and adding more features in the coming weeks. Here I provide a quick overview, but check out our blog post for more information!\nLet‚Äôs see how to do the same DDPO training of Stable Diffusion 1.4 with LAION aesthetic classifier on ImageNet prompts. First we‚Äôll do our imports:\n\nfrom drlx.trainer.ddpo_trainer import DDPOTrainer\nfrom drlx.configs import DRLXConfig\nfrom drlx.reward_modelling.aesthetics import Aesthetics\nfrom drlx.pipeline.imagenet_animal_prompts import ImagenetAnimalPrompts\n\nDRLX has a simple-to-use config system, where the model information and hyperparameters are described in a YAML file. See the config file for this example here.\n\nconfig = DRLXConfig.load_yaml(\"configs/ddpo_sd_imagenet.yml\")\n\nDRLX has PromptPipeline to implement the prompts we pass into the diffusion model. We already have a prompt pipeline for ImageNet animal prompts implemented in the library:\n\npipe = ImagenetAnimalPrompts(prefix='', postfix='', num=config.train.num_samples_per_epoch)\n\nAll we have to do is instantiate our DDPOTrainer and call train(), passing in our reward model. We have a RewardModel class that can be subclassed to implement the desired reward function, and the LAION aesthetic classifier is already provided as the Aesthetics lass:\n\ntrainer = DDPOTrainer(config)\ntrainer.train(pipe, Aesthetics())\n\nIt‚Äôs that simple!"
  },
  {
    "objectID": "posts/ddpo.html#conclusion",
    "href": "posts/ddpo.html#conclusion",
    "title": "Reinforcement Learning for Diffusion Models from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nNote that much of what we discuss here for how RL is applied to diffusion models also applies to language models. Specifically, the autoregressive generation of text from a language model can be viewed as a trajectory from an MDP. The state is the previous tokens, the action is the next token to be predicted, and the policy is of course the language model. The PPO algorithm that we described is the most common RL algorithm for RLHF training of language models. Overall, this hints to a deeper connection between diffusion models and autoregressive language models and how ideas can be transferred from one domain to another. For example, recently it was demonstrated how classifier-free guidance could be applied to language models. There may continue to be interesting ideas to apply from diffusion models to language models or vice versa.\nThis paper only is the start of applying RL to diffusion models. DDPOIS is only a baseline, and there are changes that can easily be made to improve the performance, such as value function baselines, reward discounting, KL regularization, etc. Additionally, alternative RLHF algorithms like direct preference optimization could be explored. We plan to explore these directions further via DRLX."
  },
  {
    "objectID": "posts/ddpo.html#acknowledgements",
    "href": "posts/ddpo.html#acknowledgements",
    "title": "Reinforcement Learning for Diffusion Models from Scratch",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThank you to Shahbuland Matiana, ‚Ä¶ for providing feedback on my blog post. Thank you to Costa Huang who helped me debug my code."
  }
]