<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Dr. Tanishq Abraham</title>
<link>https://www.tanishq.ai/blog/</link>
<atom:link href="https://www.tanishq.ai/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Dr. Tanishq Abraham&#39;s blog.</description>
<generator>quarto-1.6.40</generator>
<lastBuildDate>Tue, 04 Feb 2025 08:00:00 GMT</lastBuildDate>
<item>
  <title>Debunking DeepSeek Delusions</title>
  <dc:creator>Tanishq Mathew Abraham, Ph.D.</dc:creator>
  <link>https://www.tanishq.ai/blog/posts/deepseek-delusions.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>On January 20th, 2025, a Chinese AI company called <a href="https://www.deepseek.com/">DeepSeek</a> open-sourced and <a href="https://api-docs.deepseek.com/news/news250120">released</a> their reasoning model, R1. What’s different about this model vs.&nbsp;all the other open-source LLMs are a couple things:</p>
<ol type="1">
<li>Performance is actually as good as OpenAI’s o1, which is a frontier model, marking the first time open-source has truly caught up to closed-source</li>
<li>This was done with a relatively low training budget compared to other frontier models</li>
<li>The easy-to-use UI, combined with a good UX with visible chain-of-thought in their website and app led to millions of new users</li>
</ol>
<p>Given that DeepSeek is a Chinese company, the U.S. and its AGI companies have a variety of “national security concerns”. Rampant misinformation has been spreading about the model due to this. The goal of this blog post is to counteract many of the extremely bad AI-related takes about DeepSeek since its release and provide a balanced take as an AI researcher who works at the forefront of generative AI.</p>
<p>Let’s get started!</p>
</section>
<section id="myth-1-deepseek-is-a-chinese-company-that-came-out-of-nowhere-deeply-suspicious" class="level2">
<h2 class="anchored" data-anchor-id="myth-1-deepseek-is-a-chinese-company-that-came-out-of-nowhere-deeply-suspicious">Myth 1: DeepSeek is a Chinese company that came out of nowhere, deeply suspicious!</h2>
<p>Completely false, pretty much any generative AI researcher had already heard of DeepSeek by January 2025. DeepSeek even <a href="https://api-docs.deepseek.com/news/news1120">previewed</a> R1 a couple months before its full release!</p>
<p>Anybody spreading this myth is likely someone who doesn’t work in AI and it is preposterous and extremely pretentious to assume that you know everything about what’s going on in a field if you are not actively a part of it.</p>
<p>DeepSeek’s first open-source models were <a href="https://x.com/deepseek_ai/status/1720106723518918839">released</a> in November 2023, which were state-of-the-art coding LLMs (DeepSeek-Coder). As you can see in the below graph, DeepSeek continued shipping over the course of a year to reach R1:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/1738087100006.jpg" class="img-fluid figure-img"></p>
<figcaption>deepseek progress plot</figcaption>
</figure>
</div>
<p>So this isn’t some overnight success, and there’s nothing suspicious about their rate of progress. With everything moving so fast in AI and with the clearly cracked team they have, this much progress in a year seems quite reasonable to me.</p>
<p>If you are wondering what other companies are under the radar to the broader public but bullish in AI circles, I would look into <a href="https://qwenlm.github.io/">Qwen</a> (Alibaba), <a href="https://github.com/01-ai/Yi">YI</a> (01.AI), <a href="https://mistral.ai/">Mistral</a>, <a href="https://cohere.com">Cohere</a>, <a href="https://allenai.org/">AI2</a>. I will note that none of them have the consistent shipping of SOTA models like DeepSeek, but they all have the potential to release stellar models, as they have demonstrated in the past.</p>
</section>
<section id="myth-2-the-model-does-not-cost-6-million-to-make-the-chinese-are-lying-about-it" class="level2">
<h2 class="anchored" data-anchor-id="myth-2-the-model-does-not-cost-6-million-to-make-the-chinese-are-lying-about-it">Myth 2: The model does not cost $6 million to make, the Chinese are lying about it</h2>
<p>Okay this is an interesting one. The claim is that DeepSeek is lying about the true cost of model training in order to avoid admitting they had illegal under-the-table dealings to obtain compute they shouldn’t have access to (due to export controls).</p>
<p>First of all it’s worth understanding where this $6 million figure comes from. It’s mentioned in the DeepSeek-V3 <a href="https://arxiv.org/pdf/2412.19437v1">paper</a> that released a month before the DeepSeek-R1 paper:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/deepseek_cost.png" class="img-fluid figure-img"></p>
<figcaption>deepseek cost</figcaption>
</figure>
</div>
<p>DeepSeek-V3 is the base model of DeepSeek-R1, which means DeepSeek-R1 is DeepSeek-V3 with some additional reinforcement learning training. So in some sense the cost is already inaccurate simply because there’s an additional cost for the RL training that’s not accounted for. But that would likely only cost a few hundred thousand dollars.</p>
<p>Okay then, so is the $5.5 million claim of the DeepSeek-V3 paper incorrect? <a href="https://x.com/EMostaque/status/1882965806134514000">Numerous</a> <a href="https://x.com/tomgoldsteincs/status/1884651376854122774">analyses</a> <a href="https://x.com/arankomatsuzaki/status/1884676245922934788">based</a> on GPU cost, dataset size, and model size achieve similar ballpark estimates. Note that while DeepSeek V3/R1 is a 671B parameter model, it is a <a href="https://en.wikipedia.org/wiki/Mixture_of_experts">mixture-of-experts</a> <a href="https://huggingface.co/blog/moe">model</a> which means any function call/forward pass of the model only uses ~37B parameters and this is the value used in calculations for training cost.</p>
<p>However, note that DeepSeek is reporting an estimated cost based on current market prices for these GPUs. We don’t actually know how much their 2048 H800 GPU cluster (note: not H100s, a common misconception and confusion!) costs. Typically, contiguous GPU clusters cost less when bought together, so it may even be cheaper.</p>
<p>But here’s the thing, this is the cost for the <em>final</em> run. There are numerous experiments and ablations that are done at smaller scales to get to the final run which can cost a significant amount and this is not reported here.</p>
<p>On top of that, there are probably numerous other costs, like researcher salaries. SemiAnalysis <a href="https://semianalysis.com/2025/01/31/deepseek-debates/">reports</a> that DeepSeek research salaries are rumored to be on the order of $1 million. This is comparable to the higher end of salaries at AGI frontier labs like OpenAI or Anthropic.</p>
<p>Typically when costs of training different models have been reported and compared, they have always focused on the final training run cost. But due to the poor discourse and misinformation spreading, people have been arguing that the additional costs discredit the cheap costs of DeepSeek and the efficient nature of their operation. This is wildly unfair. The additional costs both in terms of ablations/experiments and researcher salaries at other AGI frontier labs are quite significant but these are not typically mentioned in such discussions!</p>
</section>
<section id="myth-3-its-so-cheap-all-the-us-agi-companies-have-been-wasting-their-money-this-is-extremely-bearish-for-nvidia" class="level2">
<h2 class="anchored" data-anchor-id="myth-3-its-so-cheap-all-the-us-agi-companies-have-been-wasting-their-money-this-is-extremely-bearish-for-nvidia">Myth 3: It’s so cheap, all the US AGI companies have been wasting their money, this is extremely bearish for NVIDIA</h2>
<p>Okay I consider this to be another fairly dumb take. DeepSeek definitely was significantly more efficient in training compared to many other LLM. And yes, it’s very much possible many US frontier labs were being inefficient with their compute. However, that does not necessarily imply that having more compute is a bad thing.</p>
<p>Honestly, whenever I hear a take like this, it’s clear to me that they don’t understand scaling laws and they don’t understand the mindset of AGI company CEOs (and anyone who is treated as an expert in AI should understand such things). Let me dispense some alpha on this topic.</p>
<p>Scaling laws have demonstrated that as long as we continue to put more compute into the model, we get better and better performance. Of course, the exact approach and aspect of the AI being scaled has changed over time: first it was with model size, then with dataset size, now with inference-time compute and synthetic data. Nevertheless, the overall trend of more compute=better performance seems to be holding since the original Transformer in 2017.</p>
<p>More efficient models means you can squeeze more performance for a given compute budget, but more compute will still be better. More efficient models means you can do more with less amount of compute, but you can do even more with more compute!</p>
<p>Now you may have your own opinions on scaling laws. You may think there is a plateau coming. You may argue past performance is not indicative of future results, as they say in finance. But that frankly doesn’t matter much if you want to understand the moves the largest AGI companies are making. All of the largest AGI companies are betting on scaling laws to hold long enough to reach AGI and ASI. This is their whole-hearted belief. And if they deeply believe this, then the only logical move is to obtain more compute.</p>
<p>(Personally, I am quite “scaling-pilled” but am open to evidence that suggests otherwise)</p>
<p>Now you may argue that NVIDIA GPUs are going to be obsolete soon, look at the performance of AMD, Cerebras, Graphcore, TPUs, Trainium, blah blah blah. There’s a million of these AI-specific hardware products that are all trying to compete with NVIDIA. And one of them might win in the future. In which case, maybe these AGI companies will switch to them. But this is completely orthogonal to DeepSeek’s success.</p>
<p>(Personally, I don’t see very strong evidence that other companies will topple NVIDIA’s domination of AI accelerator chips, given NVIDIA’s current market domination and continued level of innovation.)</p>
<p>So overall, I see no reason why DeepSeek means you should be bearish on NVIDIA. You may be bearish on NVIDIA for other reasons which may very well be justifiable and correct, but DeepSeek does not seem like the right justification to me.</p>
</section>
<section id="myth-4-deepseek-didnt-make-any-meaningful-innovations-and-are-copying-american-companies" class="level2">
<h2 class="anchored" data-anchor-id="myth-4-deepseek-didnt-make-any-meaningful-innovations-and-are-copying-american-companies">Myth 4: DeepSeek didn’t make any meaningful innovations and are copying American companies</h2>
<p>Wrong. There are numerous innovations in the design of the language model and how it was trained, some more significant than others. Here are a few (not a comprehensive list, read the DeepSeek-V3 and DeepSeek-R1 papers for more details):</p>
<ol type="1">
<li>Multi-latent attention - LLMs are usually Transformers which utilizes what is known as a multi-head attention (MHA) mechanism. The DeepSeek team developed a variant of the MHA mechanism that is both more memory-efficient and yields better performance.</li>
<li>GRPO with verifiable rewards - The AI community has been trying to replicate o1 since its release. Since OpenAI had been quite closed about how it works, the community had to explore a variety of different approaches for achieving o1-like results. There were various directions like Monte Carlo Tree Search (the approach used by Google DeepMind to <a href="https://en.wikipedia.org/wiki/AlphaGo#Algorithm">win at Go</a>) which turned out to be less promising than initially expected. On the other hand, DeepSeek demonstrated a very simple reinforcement learning (RL) pipeline can actually achieve o1-like results. On top of that, they developed their own variant of the common <a href="https://en.wikipedia.org/wiki/Proximal_policy_optimization">PPO</a> RL algorithm called GRPO that is more efficient and better-performing. I think many in the AI community have been wondering, why didn’t we try this before already?</li>
<li>DualPipe - When training an AI model over many GPUs there’s a lot of efficiency aspects to consider. You need to figure out how the model and dataset is split across all the GPUs, how the data flows through the GPUs, etc. You need to reduce any transfer of data between GPUs too because it’s very slow, it’s better to process as much as you can on each individual GPU. Anyway there are many ways to set up this sort of multi-GPU training, and the DeepSeek team designed a new approach that is significantly more efficient and faster called DualPipe.</li>
</ol>
<p>We are extremely lucky that DeepSeek has completely open-sourced and written in great detail these innovations, unlike American AGI companies. Now, everyone can benefit and improve their own training of AI models by utilizing these advances.</p>
</section>
<section id="myth-5-deepseek-is-sucking-knowledge-from-chatgpt" class="level2">
<h2 class="anchored" data-anchor-id="myth-5-deepseek-is-sucking-knowledge-from-chatgpt">Myth 5: DeepSeek is “sucking knowledge” from ChatGPT</h2>
<p>It has been claimed by <a href="https://x.com/tsarnick/status/1884352911192514975">David Sacks</a> (AI and crypto czar for the US government) and <a href="https://www.ft.com/content/a0dfedd1-5255-4fa9-8ccc-1fe01de87ea6">OpenAI</a> that DeepSeek is “sucking knowledge” from ChatGPT with a technique called distillation.</p>
<p>First of all, the term distillation is being used very weirdly here. Typically distillation refers to training on full probabilities (logits) of all the possible next words (tokens) but this info isn’t even exposed by ChatGPT.</p>
<p>But okay, let’s say we’re talking about training on text generated by ChatGPT, despite that not being the typical use of the term.</p>
<p>OpenAI and its employees are claiming that DeepSeek themselves generated text with ChatGPT and trained on it. They have provided no evidence for this but if this is true, then DeepSeek has clearly violated ChatGPT Terms of Service. I think the legal ramifications of this, especially for a Chinese firm, is unclear, but I don’t know much about that.</p>
<p>Note that this is only if DeepSeek themselves generated the data to train on. If DeepSeek used ChatGPT-generated data available from other sources (there are many public datasets at this point), my understanding is that this form of “distillation” or synthetic data training is not prohibited by the TOS.</p>
<p>That said, in my opinion this doesn’t take away from the achievements of DeepSeek. Rather than the efficiency side of DeepSeek, what was more impressive to me as a researcher was their replication of o1. And I highly doubt performing “distillation” of ChatGPT would help in any way, simply because the o1 CoT thinking process was never exposed publicly, so how would DeepSeek be able to learn it?</p>
<p>Additionally, many LLMs do perform training on ChatGPT (and other LLM) synthetic data, plus there’s naturally going to be AI text in any new Internet scrapes anyway.</p>
<p>Overall, the argument that DeepSeek’s model performs well because it simply distilled ChatGPT ignores the reality of their engineering, efficiency and architectural innovations, as detailed in DeepSeek’s technical report.</p>
</section>
<section id="should-we-be-worried-about-chinas-dominance-in-ai" class="level2">
<h2 class="anchored" data-anchor-id="should-we-be-worried-about-chinas-dominance-in-ai">Should we be worried about China’s dominance in AI?</h2>
<p>Maybe a little bit? Frankly, not much really changed in terms of the Chinese-US AI race between now and 2 months ago. Rather, the reaction from outsiders has been quite dramatic and this may indeed affect the overall AI landscape through changes in funding, regulation, etc.</p>
<p>The Chinese have <a href="https://x.com/iScienceLuvr/status/1787563145625546950">always been competitive</a> in the AI space, but DeepSeek makes them impossible to ignore now.</p>
<p>The typical argument regarding open-source has been that because China is behind we shouldn’t openly share our technology for them to catch up. But clearly China has already caught up, and they frankly did a while back, and they are actually leading on open-source, so it’s unclear if closing off our technology actually helps significantly.</p>
<p>Note that companies like OpenAI, Anthropic, and Google DeepMind definitely have models better than DeepSeek R1. For example, the benchmark results for OpenAI’s <a href="https://venturebeat.com/ai/openai-confirms-new-frontier-models-o3-and-o3-mini/">o3 model</a> are quite impressive and they likely already have another subsequent model wrapping up development.</p>
<p>On top of that, with significant additional investment like <a href="https://openai.com/index/announcing-the-stargate-project/">Project Stargate</a> and OpenAI’s <a href="https://www.reuters.com/technology/artificial-intelligence/openai-talks-investment-round-valuing-it-up-340-billion-wsj-reports-2025-01-30/">upcoming funding round</a>, OpenAI and other American frontier labs will have plenty of compute to be able to maintain their lead.</p>
<p>Of course, China will be pouring lots of additional capital into AI development. So overall, the competition is heating up! But I think the path continues to be quite promising for American AGI frontier labs to remain at the top.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>On one hand, some AI folks, especially some at OpenAI, are trying to underhype DeepSeek. On the other hand, the reaction to DeepSeek from some pundits and self-proclaimed experts is exaggerated and even dangerous. No, it’s not over for OpenAI/Anthropic/Meta/Google/xAI/NVIDIA/etc. No, DeepSeek is (probably) not lying about what they did. That said, DeepSeek deserves the recognition and R1 is an impressive model.</p>
<p>Finally, I want to note that there is so much more nuance and details regarding what is discussed here. But I hope this article served as a useful jumping-off point for your own exploration of these topics. If other sources are sharing these falsehoods with no nuance, you can safely disregard them. But there are all kinds of more in-depth discussions from folks like <a href="https://x.com/teortaxesTex">Teortaxes</a>, <a href="https://semianalysis.com/">SemiAnalysis</a>, etc., be sure to check them out!</p>
</section>
<section id="acknowledgements" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h2>
<p>Thanks to Paul Scotti for his feedback.</p>


</section>

 ]]></description>
  <category>LLMs</category>
  <category>deep learning</category>
  <category>reinforcement learning</category>
  <guid>https://www.tanishq.ai/blog/posts/deepseek-delusions.html</guid>
  <pubDate>Tue, 04 Feb 2025 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Reinforcement Learning for Diffusion Models from Scratch</title>
  <dc:creator>Tanishq Mathew Abraham, Ph.D.</dc:creator>
  <link>https://www.tanishq.ai/blog/posts/ddpo.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Over the past year we have seen the rise of generative AI that has mainly come in two forms:</p>
<ol type="1">
<li><p>Text-to-image generation powered by diffusion models like Stable Diffusion and DALL-E 2</p></li>
<li><p>Language models like ChatGPT and LLaMA-2</p></li>
</ol>
<p>It turns out one of the key ingredients for the mainstream success of language models is the use of Reinforcement Learning from Human Feedback (RLHF) where language models are trained with human feedback to produce outputs that users are more likely to prefer. This has enabled these language models to more easily follow instructions, making these models significantly more accessible. Therefore the question arises if RLHF can be applied to diffusion models. This is a natural question to ask, since text-to-image diffusion models also struggle to follow prompts and tend to need prompt engineering skills in order to get desired results. A <a href="https://arxiv.org/abs/2305.13301">paper in May 2023</a> by the Levine Lab at UC Berkeley explored how the RLHF paradigm can be applied to diffusion models, resulting in an algorithm called DDPO. Here we’ll walk through a simple implementation of this DDPO algorithm. Let’s get started!</p>
<p>First let’s start with some basic imports:</p>
<div id="a0a4aced-7a93-4ac0-bd08-717701c9d050" class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> requests</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pathlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Path</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> clip <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># pip install git+https://github.com/openai/CLIP.git</span></span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> math</span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> wandb</span>
<span id="cb1-11"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nn</span>
<span id="cb1-12"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> diffusers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StableDiffusionPipeline, DDIMScheduler</span>
<span id="cb1-13"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> PIL <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Image</span>
<span id="cb1-14"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastprogress <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> progress_bar, master_bar</span></code></pre></div>
</div>
<p>Let’s load our Stable Diffusion model. Let’s also enable some performance optimizations (TF32 support, attention slicing, memory-efficient xformers attention) that will make it faster to work with our Stable Diffusion model for training.</p>
<div id="4b85fe72-d26c-4d14-8c4f-dd7f3108bf8e" class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">torch.backends.cuda.matmul.allow_tf32 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb2-2">pipe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StableDiffusionPipeline.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CompVis/stable-diffusion-v1-4"</span>).to(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span>)</span>
<span id="cb2-3">pipe.enable_attention_slicing()</span>
<span id="cb2-4">pipe.enable_xformers_memory_efficient_attention()</span>
<span id="cb2-5">pipe.text_encoder.requires_grad_(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb2-6">pipe.vae.requires_grad_(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
<p>We’re using the <a href="https://huggingface.co/docs/diffusers">diffusers</a> library, which provides a simple-to-use interface for sampling the Stable Diffusion model using their pipeline:</p>
<div id="bdba1e9a-e696-4e70-b341-110d87ebd679" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a photograph of an astronaut riding a horse"</span></span>
<span id="cb3-2">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe(prompt).images[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 50/50 [00:03&lt;00:00, 15.99it/s]</code></pre>
</div>
</div>
<div id="04789a42-52a5-4b88-a389-7b23bf4c860e" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">img</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Okay then, we want to improve the images coming out of our model. In order to do so we should have some sort of score for the image that we can later optimize for. This score could represent how aesthetic the image is. This is frankly something that is quite subjective, and there is no mathematical equation for the aestheticness of an image. Instead we will use <a href="https://github.com/LAION-AI/aesthetic-predictor">LAION’s aesthetic predictor</a>, which was trained on thousands of human aesthetic ratings of AI-generated images and is a linear model on top of CLIP features. Below is the standard inference code for the aesthetic predictor model:</p>
<div id="ef6ad119-1fc4-46cd-9d23-37062553d484" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> MLP(nn.Module):</span>
<span id="cb6-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, input_size, xcol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'emb'</span>, ycol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'avg_rating'</span>):</span>
<span id="cb6-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb6-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.input_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> input_size</span>
<span id="cb6-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.xcol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xcol</span>
<span id="cb6-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.ycol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ycol</span>
<span id="cb6-7">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb6-8">            nn.Linear(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.input_size, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>),</span>
<span id="cb6-9">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#nn.ReLU(),</span></span>
<span id="cb6-10">            nn.Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>),</span>
<span id="cb6-11">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>),</span>
<span id="cb6-12">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#nn.ReLU(),</span></span>
<span id="cb6-13">            nn.Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>),</span>
<span id="cb6-14">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>),</span>
<span id="cb6-15">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#nn.ReLU(),</span></span>
<span id="cb6-16">            nn.Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>),</span>
<span id="cb6-17"></span>
<span id="cb6-18">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>),</span>
<span id="cb6-19">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#nn.ReLU(),</span></span>
<span id="cb6-20"></span>
<span id="cb6-21">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb6-22">        )</span>
<span id="cb6-23"></span>
<span id="cb6-24">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb6-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers(x)</span></code></pre></div>
</div>
<div id="8a6f8393-2ada-4ceb-9375-4a35dac3808c" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> load_aesthetic_model_weights(cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"."</span>):</span>
<span id="cb7-2">    weights_fname <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sac+logos+ava1-l14-linearMSE.pth"</span></span>
<span id="cb7-3">    loadpath <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> os.path.join(cache, weights_fname)</span>
<span id="cb7-4"></span>
<span id="cb7-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> os.path.exists(loadpath):</span>
<span id="cb7-6">        url <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb7-7">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://github.com/christophschuhmann/"</span></span>
<span id="cb7-8">            <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"improved-aesthetic-predictor/blob/main/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>weights_fname<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">?raw=true"</span></span>
<span id="cb7-9">        )</span>
<span id="cb7-10">        r <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> requests.get(url)</span>
<span id="cb7-11"></span>
<span id="cb7-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(loadpath, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"wb"</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f:</span>
<span id="cb7-13">            f.write(r.content)</span>
<span id="cb7-14"></span>
<span id="cb7-15">    weights <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(loadpath, map_location<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.device(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cpu"</span>))</span>
<span id="cb7-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> weights</span></code></pre></div>
</div>
<div id="88269a5d-c812-464f-99fa-ec59f2819976" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> aesthetic_model_normalize(a, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb8-2">    l2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.atleast_1d(np.linalg.norm(a, order, axis))</span>
<span id="cb8-3">    l2[l2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb8-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.expand_dims(l2, axis)</span></code></pre></div>
</div>
<p>We need the CLIP model, whose features will be passed into our aesthetic predictor:</p>
<div id="da80c62c-f0cb-49f9-8a86-72b42377c080" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">clip_model, preprocess <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clip.load(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ViT-L/14"</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span>)</span></code></pre></div>
</div>
<div id="833fa803-27af-4d77-8d46-2c10490238bf" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">aesthetic_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MLP(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">768</span>)</span></code></pre></div>
</div>
<div id="13c0a71f-8233-403d-a3fa-f72675c10671" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">aesthetic_model.load_state_dict(load_aesthetic_model_weights())</span>
<span id="cb11-2">aesthetic_model.cuda()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>MLP(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): Dropout(p=0.2, inplace=False)
    (2): Linear(in_features=1024, out_features=128, bias=True)
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=64, bias=True)
    (5): Dropout(p=0.1, inplace=False)
    (6): Linear(in_features=64, out_features=16, bias=True)
    (7): Linear(in_features=16, out_features=1, bias=True)
  )
)</code></pre>
</div>
</div>
<div id="0f506689-6306-4764-932f-9ccb16e3c2ad" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> preprocess(img).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).cuda()</span>
<span id="cb13-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad(): image_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clip_model.encode_image(image)</span></code></pre></div>
</div>
<div id="c005cc63-246b-407a-a96a-480c947d6681" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">im_emb_arr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aesthetic_model_normalize(image_features.cpu().detach().numpy())</span>
<span id="cb14-2">prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aesthetic_model(torch.from_numpy(im_emb_arr).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().cuda())</span></code></pre></div>
</div>
<div id="e7e15d26-19ed-4d79-b241-2d34a8c82413" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Aesthetic score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>prediction<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Aesthetic score: tensor([[5.1999]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>Just like, that, we get the aesthetic score given with this predictor. Let’s package this code into a function:</p>
<div id="1775ff1d-745d-400d-bcf8-715dfac66725" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model):</span>
<span id="cb17-2">    image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> preprocess(img).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).cuda()</span>
<span id="cb17-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad(): image_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clip_model.encode_image(image)</span>
<span id="cb17-4">    im_emb_arr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aesthetic_model_normalize(image_features.cpu().detach().numpy())</span>
<span id="cb17-5">    prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aesthetic_model(torch.from_numpy(im_emb_arr).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().cuda())</span>
<span id="cb17-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> prediction</span></code></pre></div>
</div>
<div id="6a85b538-8f9d-4fc3-954f-102d52941a7b" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a horse"</span></span>
<span id="cb18-2">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe(prompt).images[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb18-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Aesthetic score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb18-4">img</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code> 34%|███▍      | 17/50 [00:00&lt;00:01, 19.25it/s]100%|██████████| 50/50 [00:02&lt;00:00, 19.03it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Aesthetic score: 5.421084403991699</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-16-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="b12e6e91-a9d1-40f9-8658-a2ca85c8d305" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a beautiful, exquisite portrait of a horse, 4k, unreal engine"</span></span>
<span id="cb21-2">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe(prompt).images[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb21-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Aesthetic score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb21-4">img</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code> 62%|██████▏   | 31/50 [00:01&lt;00:00, 19.53it/s]100%|██████████| 50/50 [00:02&lt;00:00, 19.17it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Aesthetic score: 5.927961826324463</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="16">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-17-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ca070867-5c4f-496b-9013-0fa9dac8a6b2" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a very ugly photograph of a donkey"</span></span>
<span id="cb24-2">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe(prompt).images[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb24-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Aesthetic score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb24-4">img</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code> 58%|█████▊    | 29/50 [00:01&lt;00:01, 19.56it/s]100%|██████████| 50/50 [00:02&lt;00:00, 19.16it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Aesthetic score: 5.116610050201416</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="17">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-18-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>You can see if you prompt for something “ugly” we do get an image with a lower score. But to be honest, it’s not that much lower. So I found another “ugly donkey” image online to score instead:</p>
<div id="bb80d763" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> requests</span>
<span id="cb27-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> io <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BytesIO</span>
<span id="cb27-3">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Image.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(BytesIO(requests.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://i.redd.it/8wbqtdequzv41.jpg"</span>).content)).resize((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>))</span>
<span id="cb27-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Aesthetic score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>aesthetic_scoring(img, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb27-5">img</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Aesthetic score: 4.846365928649902</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="19">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Okay that’s definitely much lower.</p>
<p>As you can see, for Stable Diffusion-generated images, variation of the score doesn’t tend to be much. There seems to be a sort of average aesthetic score that most images fluctuate around (~5.3). Can we increase the average aesthetic score of the images produced by Stable Diffusion? Reinforcement learning (RL) will help us do this.</p>
</section>
<section id="what-if-we-could-optimize-the-aesthetic-score" class="level2">
<h2 class="anchored" data-anchor-id="what-if-we-could-optimize-the-aesthetic-score">What if we could optimize the aesthetic score?</h2>
<p>Now that we have some sort of measure of quality of our image, our aesthetic score, we can optimize for it. In the RL literature, this measure of quality that we are optimizing for is referred to as the <strong>reward</strong>. The goal of RL algorithms is to optimize the reward. We will see how DDPO does this for diffusion models.</p>
<p>Before we go down the RL route though, it is worth examining if there are alternative approaches. Diffusion models, after all, are an extremely versatile framework, and people have been incorporating different constraints and forms of guidance during sampling in order to achieve desired results. Let’s do a quick refresher about diffusion models and how guidance is applied.</p>
<section id="diffusion-model-refresher" class="level3">
<h3 class="anchored" data-anchor-id="diffusion-model-refresher">Diffusion model refresher</h3>
<p>A diffusion model is described by a forward and reverse process. The forward process is when we start out with a clean image <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_0"> and repeatedly add Gaussian noise <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_t%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmathbf%7B0%7D,%20%5Cmathbf%7BI%7D)"> (<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> being the identity matrix)) to give use noisier and noisier images <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_t">. This is described by the following:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20q(%5Cmathbf%7Bx%7D_t%20%7C%20%5Cmathbf%7Bx%7D_%7Bt-1%7D)%20=%20%5Cmathcal%7BN%7D(%5Cmathbf%7Bx%7D_t;%20%5Csqrt%7B1-%5Cbeta_t%7D%5Cmathbf%7Bx%7D_%7Bt-1%7D,%20%5Cbeta_t%5Cmathbf%7BI%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cbeta_t"> is a predefined monotonically increasing variance schedule. The forward process runs for a total of <img src="https://latex.codecogs.com/png.latex?T"> timesteps and finally ends with pure noise <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_T">. The reverse process starts with pure noise <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_T"> and uses a neural network to repeatedly denoise the image giving us <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_t">. The end of the reverse process gives us back our samples <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_0">. This is described as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p_%5Ctheta(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bx%7D_t)%20=%20%5Cmathcal%7BN%7D(%5Cmathbf%7Bx%7D_%7Bt-1%7D;%20%5Cmu_%5Ctheta(%5Cmathbf%7Bx%7D_t,%20t),%20%5Ctilde%7B%5Cbeta%7D_t%20%5Cmathbf%7BI%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7B%5Cbeta%7D_t"> is the variance schedule for the reverse schedule and <img src="https://latex.codecogs.com/png.latex?%5Cmu_%5Ctheta(%5Cmathbf%7Bx%7D_t,%20t)"> is the denoising neural network. Note that the denoising neural network can be reparameterized to predict the noise <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_t"> in the image. So instead of predicting the denoised image <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7Bx%7D%7D_0"> directly, we can predict the noise in the image and subtract it out to get <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7Bx%7D%7D_0">. We train the reparameterized denoising neural network <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_%5Ctheta(%5Cmathbf%7Bx%7D_t,%20t)"> in the reverse diffusion process with a simple MSE loss:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L_%7Bsimple%7D%20=%20%5Cmathbb%7BE%7D_%7Bt,%5Cmathbf%7Bx%7D_t,%20%5Cepsilon_t%7D%20%7C%7C%20%5Cepsilon_t%20-%20%5Cepsilon_%5Ctheta(%5Cmathbf%7Bx%7D_t,%20t)%20%7C%7C%20"></p>
<p>In practice, training and sampling is quite simple. During each training step, a random image <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_0"> and timestep <img src="https://latex.codecogs.com/png.latex?t"> is selected, the forward process starts from <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_0"> till timestep <img src="https://latex.codecogs.com/png.latex?t"> to get <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_t"> using the noise <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_t">, this is passed into our denoising model, and the MSE between the <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_t"> used to calculate <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_t"> and the predicted <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_%5Ctheta(%5Cmathbf%7Bx%7D_t,%20t)"> is optimized. During sampling, we start out with random Gaussian noise <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_T%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmathbf%7B0%7D,%20%5Cmathbf%7BI%7D)"> and the denoising neural network is repeatedly applied to give us <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_%7Bt-1%7D"> until we reach our sample <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_0">.</p>
<p>It is also worth noting the score matching intuition behind diffusion models. Specifically, <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_%5Ctheta(%5Cmathbf(x)_t,%20t)"> turns out to actually be an estimate (up to a factor) of the “score function” <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Cmathbf%7Bx%7D%20%5Clog%20p(%5Cmathbf%7Bx%7D)">. Basically, what this means is that when we sample from a diffusion model, we are iteratively taking steps in the direction of this score function, which is this gradient of the likelihood. So the sampling is very much like an optimization problem.</p>
<p>If any of this is unfamiliar to you, I recommend checking out the <a href="https://course.fast.ai/Lessons/part2.html">fast.ai course</a> on the subject (I am somewhat biased though given I co-taught the class!).</p>
<p>Let’s now discuss how additional constraints and guidance can be added during diffusion sampling. Basically, we want to model <img src="https://latex.codecogs.com/png.latex?p(%5Cmathbf%7Bx%7D%20%7C%20%5Cmathbf%7By%7D)"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> is some sort of condition or constraint (for example a class condition). In diffusion models, we could instead try to estimate <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Cmathbf%7Bx%7D%20%5Clog%20p(%5Cmathbf%7Bx%7D%20%7C%20%5Cmathbf%7By%7D)"> and use this in sampling. This can be expressed differently using Bayes’ Rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Bgathered%7D%0Ap(%5Cmathbf%7Bx%7D%20%5Cmid%20%20%5Cmathbf%7By%7D)=%5Cfrac%7Bp(%20%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)%20%5Ccdot%20p(%5Cmathbf%7Bx%7D)%7D%7Bp(%20%5Cmathbf%7By%7D)%7D%20%5C%5C%0A%5CLongrightarrow%20%5Clog%20p(%5Cmathbf%7Bx%7D%20%5Cmid%20%20%5Cmathbf%7By%7D)=%5Clog%20p(%20%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)+%5Clog%20p(%5Cmathbf%7Bx%7D)-%5Clog%20p(%20%5Cmathbf%7By%7D)%20%5C%5C%0A%5CLongrightarrow%20%5Cnabla_%5Cmathbf%7Bx%7D%20%5Clog%20p(%5Cmathbf%7Bx%7D%20%5Cmid%20%20%5Cmathbf%7By%7D)=%5Cnabla_%5Cmathbf%7Bx%7D%20%5Clog%20p(%20%5Cmathbf%7By%7D%20%5Cmid%20%5Cmathbf%7Bx%7D)+%5Cnabla_%5Cmathbf%7Bx%7D%20%5Clog%20p(%5Cmathbf%7Bx%7D)%0A%5Cend%7Bgathered%7D%20"></p>
<p>The second term is our score function that is already being estimated by our diffusion model <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_%5Ctheta(%5Cmathbf%7Bx%7D_t,%20t)">. The first term, however, is the gradient of the log likelihood of a classifier <img src="https://latex.codecogs.com/png.latex?p(%5Cmathbf%7Bx%7D%20%5Cmid%20%20%5Cmathbf%7By%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">. What this means is that during diffusion model sampling, if we use a modified <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cepsilon%7D_%5Ctheta(%5Cmathbf%7Bx%7D_t,t)"> with the classifier gradient added to it, we can get samples that adhere to the desired condition.</p>
<p>More broadly, losses can be applied to the noisy images and its gradient can be added to the score function/denoising network output to try to obtain images that better adhere to some desired constraints. This is the idea behind CLIP-guided diffusion, for example, where the similarity between the CLIP text embedding of a prompt and the CLIP image embedding of the images during sampling are maximized.</p>
<p>Can we use guidance to get diffusion models to generate more aesthetic images that better adhere to user prompts? Potentially yes, but there are many challenges that may make it undesirable.</p>
<p>Strictly speaking, the proper way to perform classifier guidance is to use a classifier trained on either the noisy images <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_t"> or the predicted denoised images <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7Bx%7D%7D_0"> (which tend to be blurry, especially early on in sampling), which is what the original classifier guidance paper demonstrates. Note, it is possible to use classifiers and other models trained on regular images and get reasonable results for guidance, even though noisy or blurry images will likely be out-of-distribution. For example CLIP isn’t trained on noisy images but is used in CLIP-guided diffusion. But often to get reasonable results, various hacks and tricks are required (in the case of CLIP-guided diffusion, the use of this technique fell out of popularity once diffusion models properly conditioned on CLIP features like Stable Diffusion were developed, and CLIP guidance applied on top of models like Stable Diffusion often showed minimal benefit).</p>
<p>Additionally, note that guidance requires the calculation of whatever guidance loss we have and autograd of that loss at each step in the sampling process. This can add a significant overhead to the sampling time compared to guidance-free sampling. The situation is made worse with latent diffusion models like Stable Diffusion, where the latents often need to be decoded to full images in order to apply the classifier/loss, resulting in additional computational overhead.</p>
<p>For these reasons, we will instead apply reinforcement learning to obtain a diffusion model after optimizing arbitrary constraints (the reward function), such as aesthetic scores. As we will see, the generated images from the starting diffusion model are passed into the reward function, so there is no concern of images being out-of-distribution. Additionally, we will obtain a diffusion model that provides higher scoring images directly, not through sampling changes like guidance.</p>
<p>Okay let’s proceed with trying to apply RL to diffusion models. First we’ll create a dataset generator - Stable Diffusion generated images given some prompts. We’ll use animal prompts:</p>
<div id="f0479e42-f32c-4992-9728-fb4cb3669617" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>wget https:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span>raw.githubusercontent.com<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>formigone<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>tf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>imagenet<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>master<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>LOC_synset_mapping.txt</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--2023-06-13 09:54:06--  https://raw.githubusercontent.com/formigone/tf-imagenet/master/LOC_synset_mapping.txt
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 31675 (31K) [text/plain]
Saving to: ‘LOC_synset_mapping.txt.14’

LOC_synset_mapping. 100%[===================&gt;]  30.93K  --.-KB/s    in 0.006s  

2023-06-13 09:54:06 (5.24 MB/s) - ‘LOC_synset_mapping.txt.14’ saved [31675/31675]
</code></pre>
</div>
</div>
<div id="ada2838f-fc8b-4ced-b51c-f2d3943b425e" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">synsets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {k:v <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k,v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [o.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">','</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' '</span>, maxsplit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOC_synset_mapping.txt'</span>).read_text().splitlines()]}</span>
<span id="cb31-2"></span>
<span id="cb31-3">imagenet_classes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(synsets.values())</span></code></pre></div>
</div>
<div id="5f26bc7c-7d4b-47b9-bebc-0d2ed61f042b" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> imagenet_animal_prompts():</span>
<span id="cb32-2">    animal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random.choice(imagenet_classes[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">397</span>])</span>
<span id="cb32-3">    prompts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>animal<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span></span>
<span id="cb32-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> prompts</span></code></pre></div>
</div>
<div id="f5fb951e-7f2e-4159-890e-74c2e2576eee" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">imagenet_animal_prompts()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>'sea urchin'</code></pre>
</div>
</div>
<p>Put into a dataset class:</p>
<div id="777ba057-48af-4999-982d-ceff953e1013" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> PromptDataset(torch.utils.data.Dataset):</span>
<span id="cb35-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, prompt_fn, num):</span>
<span id="cb35-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb35-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prompt_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prompt_fn</span>
<span id="cb35-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> num</span>
<span id="cb35-6">        </span>
<span id="cb35-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__len__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num</span>
<span id="cb35-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__getitem__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prompt_fn()</span></code></pre></div>
</div>
<p>Next let’s set up our sampling loop. For simplicity, we’ll just use the DDIM scheduler:</p>
<div id="6345395d" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">pipe.scheduler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DDIMScheduler(</span>
<span id="cb36-2">    num_train_timesteps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipe.scheduler.num_train_timesteps,</span>
<span id="cb36-3">    beta_start<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipe.scheduler.beta_start,</span>
<span id="cb36-4">    beta_end<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipe.scheduler.beta_end,</span>
<span id="cb36-5">    beta_schedule<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipe.scheduler.beta_schedule,</span>
<span id="cb36-6">    trained_betas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipe.scheduler.trained_betas,</span>
<span id="cb36-7">    clip_sample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipe.scheduler.clip_sample,</span>
<span id="cb36-8">    set_alpha_to_one<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipe.scheduler.set_alpha_to_one,</span>
<span id="cb36-9">    steps_offset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipe.scheduler.steps_offset,</span>
<span id="cb36-10">    prediction_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipe.scheduler.prediction_type</span>
<span id="cb36-11">)</span></code></pre></div>
</div>
<p>Below we have a sampling function, that also gives us intermediate timesteps. Again this is a pretty standard diffusion sampling loop, check out <a href="https://huggingface.co/blog/stable_diffusion">HuggingFace blog post</a> for more information.</p>
<div id="34d38a58" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@torch.no_grad</span>()</span>
<span id="cb37-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sd_sample(prompts, pipe, height, width, guidance_scale, num_inference_steps, eta, device):</span>
<span id="cb37-3">    scheduler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe.scheduler</span>
<span id="cb37-4">    unet <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe.unet</span>
<span id="cb37-5">    text_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe._encode_prompt(prompts,device, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, do_classifier_free_guidance<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>guidance_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>)</span>
<span id="cb37-6"></span>
<span id="cb37-7">    scheduler.set_timesteps(num_inference_steps, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb37-8">    latents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn((<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(prompts), unet.in_channels, height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>)).to(device)</span>
<span id="cb37-9"></span>
<span id="cb37-10">    all_step_preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb37-11"></span>
<span id="cb37-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(progress_bar(scheduler.timesteps)):</span>
<span id="cb37-13">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([latents] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb37-14">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler.scale_model_input(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, t)</span>
<span id="cb37-15"></span>
<span id="cb37-16">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># predict the noise residual</span></span>
<span id="cb37-17">        pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> unet(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>text_embeddings).sample</span>
<span id="cb37-18"></span>
<span id="cb37-19">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># perform guidance</span></span>
<span id="cb37-20">        pred_uncond, pred_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred.chunk(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb37-21">        pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred_uncond <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> guidance_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (pred_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pred_uncond)</span>
<span id="cb37-22"></span>
<span id="cb37-23">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute the "previous" noisy sample</span></span>
<span id="cb37-24">        scheduler_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler.step(pred, t, latents, eta)</span>
<span id="cb37-25"></span>
<span id="cb37-26">        all_step_preds.append(scheduler_output)</span>
<span id="cb37-27">        latents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler_output.prev_sample</span>
<span id="cb37-28">    </span>
<span id="cb37-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> latents, all_step_preds</span></code></pre></div>
</div>
<div id="cde37a1d" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">preds, all_step_preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sd_sample([prompt]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, pipe, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="50" style="width:300px; height:20px; vertical-align: middle;"></progress>
      0.00% [0/50 00:00&lt;?]
    </div>
    
</div>
</div>
<p>The sampling function only gives us latents, and they need to be decoded by a VAE to get the final images:</p>
<div id="92aee9bc" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@torch.no_grad</span>()</span>
<span id="cb39-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> decoding_fn(latents,pipe):</span>
<span id="cb39-3">    images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe.vae.decode(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.18215</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> latents.cuda()).sample</span>
<span id="cb39-4">    images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>).clamp(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb39-5">    images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.detach().cpu().permute(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).numpy()</span>
<span id="cb39-6">    images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>().astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"uint8"</span>)</span>
<span id="cb39-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> images</span></code></pre></div>
</div>
<div id="2f88b63f" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">Image.fromarray(decoding_fn(preds,pipe)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-29-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s again calculate the aesthetic score. We have to make a slight modification to our <code>aesthetic_scoring</code> function so it can handle batches.</p>
<div id="396d1ac2" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model):    </span>
<span id="cb41-2">    imgs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.stack([preprocess(Image.fromarray(img)).cuda() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> img <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> imgs])</span>
<span id="cb41-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad(): image_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clip_model.encode_image(imgs)</span>
<span id="cb41-4">    im_emb_arr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aesthetic_model_normalize(image_features.cpu().detach().numpy())</span>
<span id="cb41-5">    prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aesthetic_model(torch.from_numpy(im_emb_arr).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().cuda())</span>
<span id="cb41-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> prediction</span></code></pre></div>
</div>
<div id="d269cff8" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">imgs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> decoding_fn(preds,pipe)</span>
<span id="cb42-2">aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>tensor([[5.1635],
        [5.4475]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>Now we can initialize our dataset generator, which provides prompts that we sample with, generate images, and pass into the aesthetic predictor in just a few lines of code:</p>
<div id="78626aad" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">train_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PromptDataset(imagenet_animal_prompts, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb44-2">train_dl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.utils.data.DataLoader(train_set, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, num_workers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div id="1e6352fd" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">prompts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_dl))</span>
<span id="cb45-2">preds, all_step_preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sd_sample(prompts, pipe, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>)</span>
<span id="cb45-3">imgs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> decoding_fn(preds,pipe)</span>
<span id="cb45-4">rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="50" style="width:300px; height:20px; vertical-align: middle;"></progress>
      0.00% [0/50 00:00&lt;?]
    </div>
    
</div>
</div>
<div id="50f05b6c" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.where(rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> rewards.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>())[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb46-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(prompts[index])</span>
<span id="cb46-3">Image.fromarray(imgs[index])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>polecat</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="33">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-34-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="30aac35d" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.where(rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> rewards.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>())[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb48-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(prompts[index])</span>
<span id="cb48-3">Image.fromarray(imgs[index])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>garter snake</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="34">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-35-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Once again, the aesthetic predictor provides <em>rewards</em> and these are used in optimization with RL. Our goal is to maximize the reward!</p>
<p>For stability during RL training, the rewards are usually normalized. There are different ways of doing this, but the DDPO paper utilizes a simple approach, which is to normalize based on unique prompts. Basically, a queue is set up with prompts and the corresponding rewards, which some given buffer size. When a new (prompt, reward) pair is obtained, it is added to the queue. If the queue for that unique prompt is not long enough, the reward is just normalized over the whole batch. Else, it is normalized based on the statistics for that specific prompt. This is implemented below:</p>
<div id="8054210c" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> collections <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> deque</span>
<span id="cb50-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> PerPromptStatTracker:</span>
<span id="cb50-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, buffer_size, min_count):</span>
<span id="cb50-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.buffer_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> buffer_size</span>
<span id="cb50-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.min_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> min_count</span>
<span id="cb50-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb50-7"></span>
<span id="cb50-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, prompts, rewards):</span>
<span id="cb50-9">        unique <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.unique(prompts)</span>
<span id="cb50-10">        advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.empty_like(rewards)</span>
<span id="cb50-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> prompt <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> unique:</span>
<span id="cb50-12">            prompt_rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rewards[prompts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> prompt]</span>
<span id="cb50-13">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> prompt <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.stats:</span>
<span id="cb50-14">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.stats[prompt] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> deque(maxlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.buffer_size)</span>
<span id="cb50-15">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.stats[prompt].extend(prompt_rewards)</span>
<span id="cb50-16"></span>
<span id="cb50-17">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.stats[prompt]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.min_count:</span>
<span id="cb50-18">                mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rewards)</span>
<span id="cb50-19">                std <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.std(rewards) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span></span>
<span id="cb50-20">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb50-21">                mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.stats[prompt])</span>
<span id="cb50-22">                std <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.std(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.stats[prompt]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span></span>
<span id="cb50-23">            advantages[prompts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> prompt] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (prompt_rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mean) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> std</span>
<span id="cb50-24"></span>
<span id="cb50-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> advantages</span></code></pre></div>
</div>
<div id="b11d2591" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">per_prompt_stat_tracker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PerPromptStatTracker(buffer_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, min_count<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>)</span></code></pre></div>
</div>
<div id="92be4b5b" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">rewards.squeeze().cpu()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>tensor([5.4786, 5.5764], grad_fn=&lt;ToCopyBackward0&gt;)</code></pre>
</div>
</div>
<div id="7e00a133" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1">advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> per_prompt_stat_tracker.update(np.array(prompts), rewards.squeeze().cpu().detach().numpy())</span></code></pre></div>
</div>
<div id="22575f89" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1">advantages</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>array([-0.99998444,  0.9999747 ], dtype=float32)</code></pre>
</div>
</div>
<div id="6c67613e" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1">per_prompt_stat_tracker.stats</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>{'garter snake': deque([5.576433], maxlen=32),
 'polecat': deque([5.478563], maxlen=32)}</code></pre>
</div>
</div>
<p>We have the reward normalization set up. Let’s look at some of the intermediate timesteps:</p>
<div id="09ee01fc" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">Image.fromarray(decoding_fn(all_step_preds[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].prev_sample, pipe)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-42-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="d34337c6" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">Image.fromarray(decoding_fn(all_step_preds[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>].prev_sample, pipe)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<div>
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/ddpo_files/figure-html/cell-43-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As expected, we get pure noise (in the latent space, it looks somewhat different when decoded by the Stable Diffusion VAE) at the beginning of sampling, and as sampling progresses, the images starts to take shape.</p>
</section>
</section>
<section id="the-ddpo-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="the-ddpo-algorithm">The DDPO algorithm</h2>
<p>Okay let’s now dig into how reinforcement learning works and derive the DDPO objective. A reminder that what we want to do is to maximize the reward signal. We can mathematically express this as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctheta%5E%5Cstar%20=%20%5Carg%20%5Cmax_%5Ctheta%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Bx%7D_0%20%5Csim%20p_%5Ctheta(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bc%7D)%7D%20%5Br(%5Cmathbf%7Bx%7D_0%20%5Cmid%20%5Cmathbf%7Bc%7D)%5D%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the weights of our diffusion model, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bc%7D"> is some conditioning for the diffusion model, and <img src="https://latex.codecogs.com/png.latex?r(%5Ccdot)"> is our reward function.</p>
<p>It would be nice to directly maximize for <img src="https://latex.codecogs.com/png.latex?r(%5Ccdot)"> and if our model was a single evaluation of a neural network, we could simply backpropagate through the neural network and use an optimizer to update the weights. But that’s not what happens with a diffusion model! We have multiple timesteps for which we apply our denoising neural network. This constructs a <strong>trajectory</strong> as its known in the RL literature. In standard RL literature, our trajectory is composed of <strong>states</strong> and <strong>actions</strong>. A model that we are optimizing provides the next action given the current state, and this model is referred to as the <strong>policy</strong>. This framework is known as a <strong>Markov Decision Process (MDP)</strong>. Note that in the general MDP framework, a reward is usually given after each action, and we optimize over the sum of rewards over the whole trajectory.</p>
<p>We can easily describe diffusion models as an MDP, which will allow us to use standard results in RL for diffusion model optimization.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Barray%7D%7Brrr%7D%0A%5Cmathbf%7Bs%7D_t%20%5Ctriangleq%5Cleft(%5Cmathbf%7Bc%7D,%20t,%20%5Cmathbf%7Bx%7D_t%5Cright)%20&amp;%20%5Cpi%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Ctriangleq%20p%5E%7B%5Ctext%7Bdiffusion%7D%7D_%5Ctheta%5Cleft(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%5Cmid%20%5Cmathbf%7Bx%7D_t,%20%5Cmathbf%7Bc%7D%5Cright)%20&amp;%20p%5Cleft(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%5Cright)%20%5Ctriangleq%5Cleft(%5Cdelta_%7B%5Cmathbf%7Bc%7D%7D,%20%5Cdelta_%7Bt-1%7D,%20%5Cdelta_%7B%5Cmathbf%7Bx%7D_%7Bt-1%7D%7D%5Cright)%20%5C%5C%0A%5Cmathbf%7Ba%7D_t%20%5Ctriangleq%20%5Cmathbf%7Bx%7D_%7Bt-1%7D%20&amp;%20%5Crho_0%5Cleft(%5Cmathbf%7Bs%7D_0%5Cright)%20%5Ctriangleq%5Cleft(p(%5Cmathbf%7Bc%7D),%20%5Cdelta_T,%20%5Cmathcal%7BN%7D(%5Cmathbf%7B0%7D,%20%5Cmathbf%7BI%7D)%5Cright)%20&amp;%20R%5Cleft(%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%5Cright)%20%5Ctriangleq%20%5Cbegin%7Bcases%7Dr%5Cleft(%5Cmathbf%7Bx%7D_0,%20%5Cmathbf%7Bc%7D%5Cright)%20&amp;%20%5Ctext%20%7B%20if%20%7D%20t=0%20%5C%5C%0A0%20&amp;%20%5Ctext%20%7B%20otherwise%20%7D%5Cend%7Bcases%7D%0A%5Cend%7Barray%7D%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathcal%7BJ%7D(%5Ctheta)=%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p(%5Ccdot%20%5Cmid%20%5Cpi)%7D%5Cleft%5B%5Csum_%7Bt=0%7D%5ET%20R%5Cleft(%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%5Cright)%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Bx%7D_0%20%5Csim%20p_%5Ctheta(%5Ccdot%20%5Cmid%20%5Cmathbf%7Bc%7D)%7D%20%5Cleft%5Br(%5Cmathbf%7Bx%7D_0%20%5Cmid%20%5Cmathbf%7Bc%7D)%5Cright%5D%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_t"> is the state, which is just the current noisy image <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_t"> (along with the timestep and condition info). <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Ba%7D_t"> is the action, which is the slightly less noisy image <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_%7Bt-1%7D">. <img src="https://latex.codecogs.com/png.latex?%5Cpi%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%5Cright)"> is the policy that takes the current state and provides the next action, which is our diffusion model <img src="https://latex.codecogs.com/png.latex?p%5E%7B%5Ctext%7Bdiffusion%7D%7D_%5Ctheta%5Cleft(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%5Cmid%20%5Cmathbf%7Bx%7D_t,%20%5Cmathbf%7Bc%7D%5Cright)">. <img src="https://latex.codecogs.com/png.latex?%5Crho_0%5Cleft(%5Cmathbf%7Bs%7D_0%5Cright)"> is the distribution of the initial states, which in this case is the same distribution for <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_T">, a standard isotropic normal distribution, with the timestep always being <img src="https://latex.codecogs.com/png.latex?T"> and the conditioning having whatever prior distribution as in the dataset. <img src="https://latex.codecogs.com/png.latex?p%5Cleft(%5Cmathbf%7Bs%7D_%7Bt+1%7D%20%5Cmid%20%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%5Cright)"> is giving <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bs%7D_%7Bt+1%7D"> given the current state and action, and is basically just saying it always goes to the state associated with <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_%7Bt-1%7D">. <img src="https://latex.codecogs.com/png.latex?R%5Cleft(%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t%5Cright)"> is the reward after each action/state (which is zero until the very last step when the generation is complete).The entire trajectory can be represented as <img src="https://latex.codecogs.com/png.latex?%5Ctau"> and the probability density for trajectories as <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta(%5Cmathbf%7Bs%7D_0,%20%5Cmathbf%7Ba%7D_0,%20%5Ccdot%20%5Ccdot%20%5Ccdot,%20%5Cmathbf%7Bs%7D_T,%20%5Cmathbf%7Ba%7D_T)%20=%20p_%5Ctheta(%5Ctau)"> Note this <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta"> is different from the diffusion model <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta"> as it is a function of <img src="https://latex.codecogs.com/png.latex?%5Ctau">, a bit confusing! So going forward the diffusion model is referred to as <img src="https://latex.codecogs.com/png.latex?p%5E%5Ctext%7Bdiffusion%7D_%5Ctheta">.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/image.png" class="img-fluid figure-img"></p>
<figcaption>Diffusion models as a Markov Decision Process.</figcaption>
</figure>
</div>
<p>Okay, with this framework in place it becomes trivial to apply standard <strong>policy gradient</strong> optimization methods like <strong>REINFORCE</strong> and <strong>proximal policy optimization (PPO)</strong>. Let’s go over these two algorithms now. I have also written a more principled from-scratch derivation over [here]</p>
<p>We’ll start by looking at the general case of taking the gradient of the expectation over <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta(%5Cmathbf%7Bx%7D)"> of some function <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cnabla_%5Ctheta%20%5Cmathbb%7BE%7D_%7Bp_%7B%5Ctheta%7D%7D%5Bf(%5Cmathbf%7Bx%7D)%5D%0A&amp;=%20%5Cnabla_%5Ctheta%20%5Cint%20p_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%20f(%5Cmathbf%7Bx%7D)%20d%5Cmathbf%7Bx%7D%20%5C%5C%0A&amp;=%20%5Cint%20%5Cnabla_%5Ctheta%20p_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%20f(%5Cmathbf%7Bx%7D)%20d%5Cmathbf%7Bx%7D%20%5C%5C%0A&amp;=%20%5Cint%20%5Cfrac%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%7D%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%7D%20%5Cnabla_%5Ctheta%20p_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%20f(%5Cmathbf%7Bx%7D)%20d%5Cmathbf%7Bx%7D%20%5C%5C%0A&amp;=%20%5Cint%20p_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%20%5Cfrac%7B%5Cnabla_%5Ctheta%20p_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%7D%7Bp_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%7D%20f(%5Cmathbf%7Bx%7D)%20d%5Cmathbf%7Bx%7D%20%5C%5C%0A&amp;=%20%5Cint%20p_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%20%5Cnabla_%5Ctheta%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)%20f(%5Cmathbf%7Bx%7D)%20d%5Cmathbf%7Bx%7D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BE%7D_%7Bp_%7B%5Ctheta%7D%7D%20%5Cbig%5B%20%5Cnabla_%5Ctheta%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)f(%5Cmathbf%7Bx%7D)%20%5Cbig%5D%0A%5Cend%7Balign%7D%0A"></p>
<p>This is referred to as the <em>score function gradient estimator</em>.</p>
<p>Let’s think more about what this is doing. We want to calculate the gradient of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7Bp_%7B%5Ctheta%7D%7D%5Bf(%5Cmathbf%7Bx%7D)%5D"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. That is, we want to know how we can change <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> such that we get samples from <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta(%5Cmathbf%7Bx%7D"> that on average give higher <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> values. What this estimator says is that we can take the gradient <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20%5Clog%20p_%7B%5Ctheta%7D(%5Cmathbf%7Bx%7D)"> (which tells you how to change <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to increase the likelihood of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> under your distribtuion <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta(%5Cmathbf%7Bx%7D)">), and weight it with <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)">. So if this is being used in gradient ascent for example, we are placing more weight on updates that make high-scoring <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> samples more likely under our model <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta(%5Cmathbf%7Bx%7D)">.</p>
<p>When we apply this to the MDP framework and simplify, we can get our <strong>policy gradient</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cnabla_%5Ctheta%20%5Cmathcal%7BJ%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta(%5Ctau)%7D%20%5Cleft%5B%5Cleft(%5Csum%5ET_%7Bt=0%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright)%20%5Cleft(%5Csum%5ET_%7Bt=0%7DR(%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%5Cright)%20%5Cright%5D%20"></p>
<p>This gradient is referred to as the <strong>REINFORCE</strong> gradient and is only one type of policy gradient that could be used. Of course, this policy gradient is then used to update the weights of our model using gradient ascent:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctheta%20%5Cleftarrow%20%5Ctheta%20+%20%5Calpha%20%5Cnabla_%5Ctheta%20%5Cmathcal%7BJ%7D(%5Ctheta)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is some learning rate.</p>
<p>One implementation point is that the expectation is over the trajectories but of course we can’t take and sum over all possible trajectories. The expectation is estimated with just the sampled trajectories in the currenty batch. One other implementation point to mention: we could calculate our gradient and then pass that gradient to our optimizer, or we could let autograd handle the calculation of the gradient by constructing a loss function and treating it as a standard training loop. The latter is what is done in practice even though it is not explicitly mentioned often in the papers. So our loss function is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathcal%7BL%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%5Ctheta(%5Ctau)%7D%20%5Cleft%5B%20-%20%5Cleft(%5Csum%5ET_%7Bt=0%7D%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright)%20%5Cleft(%5Csum%5ET_%7Bt=0%7DR(%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%5Cright)%20%5Cright%5D%20"></p>
<p>Again, let’s reinforce the intuition behind REINFORCE gradient/loss function (pun definitely intended). You can see the loss looks very much like a negative log-likelihood loss, with the actions as our target. The diference here is that it is weighted by the reward. What this loss is doing is trying to make high-reward trajectories more likely and low-reward trajectories less likely.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.tanishq.ai/blog/posts/1 zkOBQ9Izq28yXCANTmdKtA.webp" class="img-fluid figure-img"></p>
<figcaption>Higher reward trajectories (represented by the checkmark) are encouraged by the policy gradient (represented by the higher peak).</figcaption>
</figure>
</div>
<p>Okay so we can simply plug in our diffusion model terms based on how it fits into the MDP framework, which we described earlier. We get:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cnabla_%5Ctheta%20%5Cmathcal%7BJ%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D%20%5Cleft%5B%5Cleft(%5Csum%5ET_%7Bt=0%7D%20%5Cnabla_%5Ctheta%20%5Clog%20p%5E%5Ctext%7Bdiffusion%7D_%5Ctheta%5Cleft(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%5Cmid%20%5Cmathbf%7Bc%7D,%20t,%20%5Cmathbf%7Bx%7D_t%5Cright)%20%5Cright)%20r(%5Cmathbf%7Bx%7D_0,%20%5Cmathbf%7Bc%7D)%20%5Cright%5D%20"></p>
<p>This objective and gradient estimator is referred in the paper as <strong>DDPO<sub>SF</sub></strong>.</p>
<p>One challenge with this approach is that for each optimization step, the sampling from the current iteration of the model needs to be performed, we need to re-calculate <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_t"> as it comes from the current version of the model. This can be computationally expensive and wasteful, as the samples collected with previous iterations of the model cannot be used to learn.</p>
<p>One trick to address this is known as importance sampling. This relies on the following identity (trivial to demonstrate based on the expectation definition):</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BE%7D_%7Bx%5Csim%20p(x)%7D%20%5Cleft%5Bf(x)%5Cright%5D%20=%20%5Cmathbb%7BE%7D_%7Bx%5Csim%20q(x)%7D%20%5Cleft%5B%5Cfrac%7Bp(x)%7D%7Bq(x)%7Df(x)%5Cright%5D%20"></p>
<p>Applying importance sampling gives us the following:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cnabla_%5Ctheta%20%5Cmathcal%7BJ%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%7B%5Ctheta_%7Bold%7D%7D%20%5Cleft(%5Ctau%20%5Cright)%7D%20%5Cleft%5B%5Cleft(%5Csum%5ET_%7Bt=0%7D%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%5Cright)%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%5Cright)%20%5Cright)%20%5Cleft(%5Csum%5ET_%7Bt=0%7DR(%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%5Cright)%20%5Cright%5D%20"></p>
<p>Again this can be written down as a loss function that we perform gradient descent on (sometimes referred to as the <strong>surrogate loss</strong>):</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathcal%7BL%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20p_%7B%5Ctheta_%7Bold%7D%7D%20%5Cleft(%5Ctau%20%5Cright)%7D%20%5Cleft%5B-%5Cleft(%5Csum%5ET_%7Bt=0%7D%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%5Cleft(%5Cmathbf%7Ba%7D_t%20%5Cmid%20%5Cmathbf%7Bs%7D_t%5Cright)%7D%20%5Cright)%20%5Cleft(%5Csum%5ET_%7Bt=0%7DR(%5Cmathbf%7Bs%7D_t,%20%5Cmathbf%7Ba%7D_t)%20%5Cright)%20%5Cright%5D%20"></p>
<p>Again, we can plug in the diffusion model terms based on the MDP framework and get this loss function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L(%5Ctheta)%20=%20%5Cmathbb%7BE%7D%20%5Cleft%5B%20-%20%5Csum%5ET_%7Bt=0%7D%20%5Cfrac%7Bp%5E%5Ctext%7Bdiffusion%7D_%5Ctheta(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%7D%7Bp%5E%5Ctext%7Bdiffusion%7D_%7B%5Ctheta_%7Bold%7D%7D(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%7D%20r(%5Cmathbf%7Bx%7D_0,%5Cmathbf%7Bc%7D)%20%5Cright%5D%20"></p>
<p>Minimization of this loss function is equivalent to gradient with the following gradient:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Chat%20g%20=%20%5Cmathbb%7BE%7D%20%5Cleft%5B%5Csum%5ET_%7Bt=0%7D%20%5Cfrac%7Bp%5E%5Ctext%7Bdiffusion%7D_%5Ctheta(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%7D%7Bp%5E%5Ctext%7Bdiffusion%7D_%7B%5Ctheta_%7Bold%7D%7D(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%7D%20%5Cnabla_%5Ctheta%20p%5E%5Ctext%7Bdiffusion%7D_%5Ctheta(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%20r(%5Cmathbf%7Bx%7D_0,%5Cmathbf%7Bc%7D)%20%5Cright%5D%20"></p>
<p>Note that the reward <img src="https://latex.codecogs.com/png.latex?r(%5Cmathbf%7Bx%7D_0,%5Cmathbf%7Bc%7D)"> is usually normalized, and the normalized reward is referred to the advantage <img src="https://latex.codecogs.com/png.latex?A(%5Cmathbf%7Bx%7D_0,%5Cmathbf%7Bc%7D)">. So the advantage can be negative if it’s less than average.</p>
<p>Note that we don’t want current policy diverge too much from the previous policy, otherwise we may diverge and get a bad policy. To help address this, we can apply clipping to the importance sampling ratio to the loss function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L(%5Ctheta)%20=%20%5Cmathbb%7BE%7D%20%5Cleft%5B%20-%20%5Csum%5ET_%7Bt=0%7D%20%5Cmin%20%5Cleft(%5Cfrac%7Bp%5E%5Ctext%7Bdiffusion%7D_%5Ctheta(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%7D%7Bp%5E%5Ctext%7Bdiffusion%7D_%7B%5Ctheta_%7Bold%7D%7D(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%7D%20A(%5Cmathbf%7Bx%7D_0,%5Cmathbf%7Bc%7D),%20%5Cmathrm%7Bclip%7D%20%5Cleft(%20%5Cfrac%7Bp%5E%5Ctext%7Bdiffusion%7D_%5Ctheta(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%7D%7Bp%5E%5Ctext%7Bdiffusion%7D_%7B%5Ctheta_%7Bold%7D%7D(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%7D,%201-%5Cepsilon,%201+%5Cepsilon%20%20%5Cright)%20A(%5Cmathbf%7Bx%7D_0,%5Cmathbf%7Bc%7D)%20%5Cright)%20%5Cright%5D%20"></p>
<p>So if the policy diverges too much (the ratio is either much larger or much smaller than 1) the loss function is clipped to a certain value and the gradient will be zero and no updates will be made. The below diagram (taken from <a href="https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl">here</a>) clarifies this further:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.stack.imgur.com/gasbI.png" class="img-fluid figure-img"></p>
<figcaption>Annotated PPO loss function.</figcaption>
</figure>
</div>
<p>Note that DDPO also clips the advantages themselves $ A(_0,) $ in its implementation but this is not described in the paper, so I have not included it in the loss function.</p>
<p>The loss function can be written in a way that’s numerically easier to calculate/more stable (using logs, ignoring the clipping for now):</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L(%5Ctheta)%20=%20%5Cmathbb%7BE%7D%20%5Cleft%5B%20-%20%5Csum%5ET_%7Bt=0%7D%20%5Cexp%7B%5Cleft(%5Clog%20p%5E%5Ctext%7Bdiffusion%7D_%5Ctheta(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%20-%5Clog%20p%5E%5Ctext%7Bdiffusion%7D_%7B%5Ctheta_%7Bold%7D%7D(%5Cmathbf%7Bx%7D_%7Bt-1%7D%20%7C%20%5Cmathbf%7Bc%7D,t,%5Cmathbf%7Bx%7D_t)%20%5Cright)%7D%20A(%5Cmathbf%7Bx%7D_0,%5Cmathbf%7Bc%7D)%20%5Cright%5D%20"></p>
<p>The objective and gradient estimator described here is referred in the paper as <strong>DDPO<sub>IS</sub></strong>. It is pretty much the same as <strong>proximal policy optimization (PPO)</strong>, applied to diffusion models.</p>
<p>For a more complete derivation of the DDPO objective from scratch, see <a href="https://www.tanishq.ai/blog/posts/ddpo_derivation.pdf">here</a>.</p>
<p>In order to start implementing this loss function, let’s calculate the log probs, which is easy for a normal distribution:</p>
<div id="b701de45" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> calculate_log_probs(prev_sample, prev_sample_mean, std_dev_t):</span>
<span id="cb61-2">    std_dev_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.clip(std_dev_t, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>)</span>
<span id="cb61-3">    log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>((prev_sample.detach() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prev_sample_mean) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> std_dev_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> torch.log(std_dev_t) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> math.log(math.sqrt(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> math.pi))</span>
<span id="cb61-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> log_probs</span></code></pre></div>
</div>
<p>We need to get those log probs of the original model so our sampling function should return that. Let’s update our sampling function to do that:</p>
<div id="c24c2ac4" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@torch.no_grad</span>()</span>
<span id="cb62-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sd_sample(prompts, pipe, height, width, guidance_scale, num_inference_steps, eta, device):</span>
<span id="cb62-3">    scheduler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe.scheduler</span>
<span id="cb62-4">    unet <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe.unet</span>
<span id="cb62-5">    text_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe._encode_prompt(prompts,device, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, do_classifier_free_guidance<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>guidance_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>)</span>
<span id="cb62-6"></span>
<span id="cb62-7">    scheduler.set_timesteps(num_inference_steps, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb62-8">    latents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn((<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(prompts), unet.in_channels, height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>)).to(device)</span>
<span id="cb62-9"></span>
<span id="cb62-10">    all_step_preds, log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [latents], []</span>
<span id="cb62-11"></span>
<span id="cb62-12"></span>
<span id="cb62-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(progress_bar(scheduler.timesteps)):</span>
<span id="cb62-14">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([latents] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb62-15">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler.scale_model_input(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, t)</span>
<span id="cb62-16"></span>
<span id="cb62-17">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># predict the noise residual</span></span>
<span id="cb62-18">        pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> unet(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>text_embeddings).sample</span>
<span id="cb62-19"></span>
<span id="cb62-20">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># perform guidance</span></span>
<span id="cb62-21">        pred_uncond, pred_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred.chunk(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb62-22">        pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred_uncond <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> guidance_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (pred_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pred_uncond)</span>
<span id="cb62-23"></span>
<span id="cb62-24">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute the "previous" noisy sample mean and variance, and get log probs</span></span>
<span id="cb62-25">        scheduler_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler.step(pred, t, latents, eta, variance_noise<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb62-26">        t_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> scheduler.config.num_train_timesteps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> num_inference_steps</span>
<span id="cb62-27">        variance <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler._get_variance(t, t_1)</span>
<span id="cb62-28">        std_dev_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> eta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> variance <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb62-29">        prev_sample_mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler_output.prev_sample <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is the mean and not full sample since variance is 0</span></span>
<span id="cb62-30">        prev_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prev_sample_mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> torch.randn_like(prev_sample_mean) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> std_dev_t <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get full sample by adding noise</span></span>
<span id="cb62-31">        log_probs.append(calculate_log_probs(prev_sample, prev_sample_mean, std_dev_t).mean(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, prev_sample_mean.ndim))))</span>
<span id="cb62-32"></span>
<span id="cb62-33">        all_step_preds.append(prev_sample)</span>
<span id="cb62-34">        latents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prev_sample</span>
<span id="cb62-35">    </span>
<span id="cb62-36">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> latents, torch.stack(all_step_preds), torch.stack(log_probs)</span></code></pre></div>
</div>
<p>We can get everything we need for the loss function now (intermediate timesteps, log_probs, rewards):</p>
<div id="3d8b882c" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1">per_prompt_stat_tracker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PerPromptStatTracker(buffer_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, min_count<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>)</span>
<span id="cb63-2">prompts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_dl))</span>
<span id="cb63-3">pipe.text_encoder.to(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>)</span>
<span id="cb63-4">pipe.vae.to(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>)</span>
<span id="cb63-5">preds, all_step_preds, log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sd_sample(prompts, pipe, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>)</span>
<span id="cb63-6">imgs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> decoding_fn(preds,pipe)</span>
<span id="cb63-7">rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)</span>
<span id="cb63-8">advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.from_numpy(per_prompt_stat_tracker.update(np.array(prompts), rewards.squeeze().cpu().detach().numpy())).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().to(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="50" style="width:300px; height:20px; vertical-align: middle;"></progress>
      0.00% [0/50 00:00&lt;?]
    </div>
    
</div>
</div>
<p>Here’s a function to compute the loss function:</p>
<div id="2ce6ac70" class="cell">
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_loss(x_t, original_log_probs, advantages, clip_advantages, clip_ratio, prompts, pipe, num_inference_steps, guidance_scale, eta, device):</span>
<span id="cb64-2">    scheduler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe.scheduler</span>
<span id="cb64-3">    unet <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe.unet</span>
<span id="cb64-4">    text_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipe._encode_prompt(prompts,device, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, do_classifier_free_guidance<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>guidance_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>).detach()</span>
<span id="cb64-5">    scheduler.set_timesteps(num_inference_steps, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb64-6">    loss_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.</span></span>
<span id="cb64-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(progress_bar(scheduler.timesteps)):</span>
<span id="cb64-8">        clipped_advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.clip(advantages, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>clip_advantages, clip_advantages).detach()</span>
<span id="cb64-9">        </span>
<span id="cb64-10">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([x_t[i].detach()] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb64-11">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler.scale_model_input(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, t)</span>
<span id="cb64-12"></span>
<span id="cb64-13">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># predict the noise residual</span></span>
<span id="cb64-14">        pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> unet(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, t, encoder_hidden_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>text_embeddings).sample</span>
<span id="cb64-15"></span>
<span id="cb64-16">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># perform guidance</span></span>
<span id="cb64-17">        pred_uncond, pred_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred.chunk(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb64-18">        pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pred_uncond <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> guidance_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (pred_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pred_uncond)</span>
<span id="cb64-19"></span>
<span id="cb64-20">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute the "previous" noisy sample mean and variance, and get log probs</span></span>
<span id="cb64-21">        scheduler_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler.step(pred, t, x_t[i].detach(), eta, variance_noise<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb64-22">        t_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> scheduler.config.num_train_timesteps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> num_inference_steps</span>
<span id="cb64-23">        variance <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler._get_variance(t, t_1)</span>
<span id="cb64-24">        std_dev_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> eta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> variance <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb64-25">        prev_sample_mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scheduler_output.prev_sample</span>
<span id="cb64-26">        current_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calculate_log_probs(x_t[i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].detach(), prev_sample_mean, std_dev_t).mean(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, prev_sample_mean.ndim)))</span>
<span id="cb64-27"></span>
<span id="cb64-28">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># calculate loss</span></span>
<span id="cb64-29"></span>
<span id="cb64-30">        ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(current_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> original_log_probs[i].detach()) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is the importance ratio of the new policy to the old policy</span></span>
<span id="cb64-31">        unclipped_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>clipped_advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ratio <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is the surrogate loss</span></span>
<span id="cb64-32">        clipped_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>clipped_advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> torch.clip(ratio, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> clip_ratio, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> clip_ratio) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is the surrogate loss, but with artificially clipped ratios</span></span>
<span id="cb64-33">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(unclipped_loss, clipped_loss).mean() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># we take the max of the clipped and unclipped surrogate losses, and take the mean over the batch</span></span>
<span id="cb64-34">        loss.backward() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># perform backward here, gets accumulated for all the timesteps</span></span>
<span id="cb64-35"></span>
<span id="cb64-36">        loss_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> loss.item()</span>
<span id="cb64-37">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> loss_value</span></code></pre></div>
</div>
<div id="c3df2d60" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_loss(all_step_preds, log_probs, advantages, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span>, prompts, pipe, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>)</span>
<span id="cb65-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(loss)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="50" class="" max="50" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [50/50 00:29&lt;00:00]
    </div>
    
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2.6168066263198853</code></pre>
</div>
</div>
</section>
<section id="complete-training-loop" class="level2">
<h2 class="anchored" data-anchor-id="complete-training-loop">Complete training loop</h2>
<p>Now that we can calculate the loss function, we can construct the full training loop. For a single epoch we:</p>
<ol type="1">
<li><p>Sample from the diffusion model <code>num_samples_per_epoch</code>times, collecting the intermediate noisy images and log probs.</p></li>
<li><p>Pass the samples to the reward model and get reward, which we normalize to get advantage.</p></li>
<li><p>For <code>num_inner_epochs</code> times, we go over each sample compute the loss, backpropagate, and update our diffusion model.</p></li>
</ol>
<p>Let’s define all our hyperparameters. We will be training Stable Diffusion v1.4 on ImageNet animal prompts as defined earlier, using the LAION Aesthetic classifier as our reward model.</p>
<p>Note that if we set <code>num_inner_epochs</code> to a high amount, this would be very data-efficient since we would be repeatedly using the previously generated trajectories, but we would probably significantly diverge from the original policy that we used to get those trajectories (or it would at least get clipped frequently in the loss). So we set <code>num_inner_epochs=1</code>. This is still pretty efficient using DDPO<sub>IS</sub> because otherwise with DDPO<sub>SF</sub> you’d need to resample after every iteration (when model is updated) instead of after <code>num_samples_per_epoch=128</code> that we have here.</p>
<div id="c376834d" class="cell">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">num_samples_per_epoch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span></span>
<span id="cb67-2">num_epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb67-3">num_inner_epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb67-4">num_timesteps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb67-5">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb67-6">img_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span></span>
<span id="cb67-7">lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5e-6</span></span>
<span id="cb67-8">clip_advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">10.0</span></span>
<span id="cb67-9">clip_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span></span>
<span id="cb67-10">cfg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5.0</span></span></code></pre></div>
</div>
<p>Okay let’s set everything up:</p>
<div id="374c5489" class="cell">
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># group all reward function stuff</span></span>
<span id="cb68-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> reward_fn(imgs, device):</span>
<span id="cb68-3">    clip_model.to(device)</span>
<span id="cb68-4">    aesthetic_model.to(device)</span>
<span id="cb68-5">    rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aesthetic_scoring(imgs, preprocess, clip_model, aesthetic_model_normalize, aesthetic_model)</span>
<span id="cb68-6">    clip_model.to(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cpu'</span>)</span>
<span id="cb68-7">    aesthetic_model.to(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cpu'</span>)</span>
<span id="cb68-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> rewards</span></code></pre></div>
</div>
<div id="b3ad3abb" class="cell">
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># a function to sample from the model and calculate rewards</span></span>
<span id="cb69-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sample_and_calculate_rewards(prompts, pipe, image_size, cfg, num_timesteps, decoding_fn, reward_fn, device):</span>
<span id="cb69-3">    preds, all_step_preds, log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sd_sample(prompts, pipe, image_size, image_size, cfg, num_timesteps, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, device)</span>
<span id="cb69-4">    imgs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> decoding_fn(preds,pipe)    </span>
<span id="cb69-5">    rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reward_fn(imgs, device)</span>
<span id="cb69-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> imgs, rewards, all_step_preds, log_probs</span></code></pre></div>
</div>
<p>Here we create our dataset, which is just randomly chosen prompts:</p>
<div id="ca093404" class="cell">
<div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1">train_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PromptDataset(imagenet_animal_prompts, num_samples_per_epoch)</span>
<span id="cb70-2">train_dl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.utils.data.DataLoader(train_set, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, num_workers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb70-3">per_prompt_stat_tracker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PerPromptStatTracker(buffer_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, min_count<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>)</span>
<span id="cb70-4">sample_prompts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_dl)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># sample a batch of prompts to use for visualization</span></span></code></pre></div>
</div>
<div id="74404662" class="cell">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">pipe.unet.enable_gradient_checkpointing() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># more performance optimization</span></span></code></pre></div>
</div>
<div id="eb5199fc" class="cell">
<div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.AdamW(pipe.unet.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lr, weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># optimizer</span></span></code></pre></div>
</div>
<p>Now that we are set up, let’s start training! You can see the training loop is quite simple!</p>
<div id="63326234" class="cell">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> master_bar(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_epochs)):</span>
<span id="cb73-2">    all_step_preds, log_probs, advantages, all_prompts, all_rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [], [], [], [], []</span>
<span id="cb73-3"></span>
<span id="cb73-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># sampling `num_samples_per_epoch` images and calculating rewards</span></span>
<span id="cb73-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, prompts <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(progress_bar(train_dl)):</span>
<span id="cb73-6">        batch_imgs, rewards, batch_all_step_preds, batch_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sample_and_calculate_rewards(prompts, pipe, img_size, cfg, num_timesteps, decoding_fn, reward_fn, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>)</span>
<span id="cb73-7">        batch_advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.from_numpy(per_prompt_stat_tracker.update(np.array(prompts), rewards.squeeze().cpu().detach().numpy())).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().to(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>)</span>
<span id="cb73-8">        all_step_preds.append(batch_all_step_preds)</span>
<span id="cb73-9">        log_probs.append(batch_log_probs)</span>
<span id="cb73-10">        advantages.append(batch_advantages)</span>
<span id="cb73-11">        all_prompts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> prompts</span>
<span id="cb73-12">        all_rewards.append(rewards)</span>
<span id="cb73-13">    </span>
<span id="cb73-14">    all_step_preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat(all_step_preds, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb73-15">    log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat(log_probs, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb73-16">    advantages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat(advantages)</span>
<span id="cb73-17">    all_rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat(all_rewards)</span>
<span id="cb73-18"></span>
<span id="cb73-19">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># inner loop</span></span>
<span id="cb73-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> inner_epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> progress_bar(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_inner_epochs)):</span>
<span id="cb73-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># chunk them into batches</span></span>
<span id="cb73-22">        all_step_preds_chunked <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.chunk(all_step_preds, num_samples_per_epoch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> batch_size, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb73-23">        log_probs_chunked <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.chunk(log_probs, num_samples_per_epoch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> batch_size, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb73-24">        advantages_chunked <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.chunk(advantages, num_samples_per_epoch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> batch_size, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb73-25">        </span>
<span id="cb73-26">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># chunk the prompts (list of strings) into batches</span></span>
<span id="cb73-27">        all_prompts_chunked <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [all_prompts[i:i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> batch_size] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(all_prompts), batch_size)]</span>
<span id="cb73-28">        </span>
<span id="cb73-29">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> progress_bar(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(all_step_preds_chunked))):</span>
<span id="cb73-30">            optimizer.zero_grad()</span>
<span id="cb73-31"></span>
<span id="cb73-32">            loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_loss(all_step_preds_chunked[i], log_probs_chunked[i], </span>
<span id="cb73-33">                                advantages_chunked[i], clip_advantages, clip_ratio, all_prompts_chunked[i], pipe, num_timesteps, cfg, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span></span>
<span id="cb73-34">                                ) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># loss.backward happens inside</span></span>
<span id="cb73-35">            </span>
<span id="cb73-36">            torch.nn.utils.clip_grad_norm_(pipe.unet.parameters(), <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># gradient clipping</span></span>
<span id="cb73-37">            optimizer.step()</span></code></pre></div>
</div>
<p>That’s it! Let’s see what results we get with this code.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>(These results were obtained with <a href="https://github.com/tmabraham/ddpo-pytorch/blob/main/main.py">this script</a> that included W&amp;B tracking)</p>
<p>Here is the loss curve from training Stable Diffusion v1.4 with the LAION Aesthetic classifier reward model on ImageNet animal prompts: <img src="https://www.tanishq.ai/blog/posts/image-1.png" class="img-fluid"></p>
<p>As you can see, it’s quite noisy but clearly does decrease. That said, I’ve observed that typically the loss curve can be quite uninformative. Instead, as expected, the reward curve is a better indicator of the performance:</p>
<p><img src="https://www.tanishq.ai/blog/posts/image-2.png" class="img-fluid"></p>
<p>Here you can see a clear increase in average reward over training, which is what we want! So what do the samples look like? Let’s see:</p>
<p><img src="https://www.tanishq.ai/blog/posts/image-3.png" class="img-fluid"></p>
<p>I’d argue these images are definitely more aesthetic! It works! A few observations:</p>
<ol type="1">
<li><p>Sometimes the prompt isn’t being followed, as seen with the wolf spider example. This is since the reward model does not take into consideration the prompt and only looks at the image, so if generating something that is slightly unrelated to the prompt gives a better reward score then the model will do so. A reward model explicitly taking in the prompt as well and ensuring prompt alignment would be needed. One such model that could be used is <a href="https://arxiv.org/abs/2305.01569">PickScore</a>.</p></li>
<li><p>Sometimes the RL-trained diffusion model generates pencil/charcoal drawings of the animals, which was observed in the original DDPO paper as well.</p></li>
<li><p>Another common pattern that the RL-trained diffusion model demonstrates is the generation of narrow depth-of-focus images, which of course clearly looks more artistic and aesthetic.</p></li>
</ol>
</section>
<section id="drlx---a-library-for-performing-rlhf-training-with-diffusion-models" class="level2">
<h2 class="anchored" data-anchor-id="drlx---a-library-for-performing-rlhf-training-with-diffusion-models">DRLX - A library for performing RLHF training with diffusion models</h2>
<p>In order to make RLHF for diffusion models easy-to-use and accessible, I have co-developed a library with Shahbuland Matiana at <a href="https://carper.ai">CarperAI</a> called <a href="https://github.com/carperai/drlx">DRLX</a>. It implements DDPO, complete with W&amp;B experiment tracking, distributed GPU training support, and other features. We also will be implementing other RL algorithms and adding more features in the coming weeks. Here I provide a quick overview, but check out our <a href="https://carper.ai/enhancing-diffusion-models-with-reinforcement-learning/">blog post</a> for more information!</p>
<p>Let’s see how to do the same DDPO training of Stable Diffusion 1.4 with LAION aesthetic classifier on ImageNet prompts. First we’ll do our imports:</p>
<div id="abd3840e" class="cell">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> drlx.trainer.ddpo_trainer <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DDPOTrainer</span>
<span id="cb74-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> drlx.configs <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DRLXConfig</span>
<span id="cb74-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> drlx.reward_modelling.aesthetics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Aesthetics</span>
<span id="cb74-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> drlx.pipeline.imagenet_animal_prompts <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ImagenetAnimalPrompts</span></code></pre></div>
</div>
<p>DRLX has a simple-to-use config system, where the model information and hyperparameters are described in a YAML file. See the config file for this example <a href="https://github.com/CarperAI/DRLX/blob/main/configs/ddpo_sd_imagenet.yml">here</a>.</p>
<div id="0c57f992" class="cell">
<div class="sourceCode cell-code" id="cb75" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1">config <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DRLXConfig.load_yaml(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"configs/ddpo_sd_imagenet.yml"</span>)</span></code></pre></div>
</div>
<p>DRLX has <code>PromptPipeline</code> to implement the prompts we pass into the diffusion model. We already have a prompt pipeline for ImageNet animal prompts implemented in the library:</p>
<div id="8456eb66" class="cell">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">pipe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ImagenetAnimalPrompts(prefix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span>, postfix<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>config.train.num_samples_per_epoch)</span></code></pre></div>
</div>
<p>All we have to do is instantiate our <code>DDPOTrainer</code> and call <code>train()</code>, passing in our reward model. We have a <code>RewardModel</code> class that can be subclassed to implement the desired reward function, and the LAION aesthetic classifier is already provided as the <code>Aesthetics</code> lass:</p>
<div id="3d1de700" class="cell">
<div class="sourceCode cell-code" id="cb77" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1">trainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DDPOTrainer(config)</span>
<span id="cb77-2">trainer.train(pipe, Aesthetics())</span></code></pre></div>
</div>
<p>It’s that simple!</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Note that much of what we discuss here for how RL is applied to diffusion models also applies to language models. Specifically, the autoregressive generation of text from a language model can be viewed as a trajectory from an MDP. The state is the previous tokens, the action is the next token to be predicted, and the policy is of course the language model. The PPO algorithm that we described is the most common RL algorithm for RLHF training of language models. Overall, this hints to a deeper connection between diffusion models and autoregressive language models and how ideas can be transferred from one domain to another. For example, recently it was demonstrated how classifier-free guidance <a href="https://arxiv.org/abs/2306.17806">could be applied</a> to language models. There may continue to be interesting ideas to apply from diffusion models to language models or vice versa.</p>
<p>This paper only is the start of applying RL to diffusion models. DDPO<sub>IS</sub> is only a baseline, and there are changes that can easily be made to improve the performance, such as value function baselines, reward discounting, KL regularization, etc. Additionally, alternative RLHF algorithms like <a href="https://arxiv.org/abs/2305.18290">direct preference optimization</a> could be explored. We plan to explore these directions further via DRLX.</p>
<p>Finally, I just want to say this was an interesting learning experience implementing my very first RL algorithm (and building a library based on that too). It took me a lot of reading of RL course material, blog posts, etc. looking at code implementations, wrestling with very subtle bugs, etc. but after about 2 weeks I managed to get it working. As many people in the RL field have experienced, it didn’t take much time for me to form a love-hate relationship with RL 😂 but I still think it’s a very interesting field and I’m excited to explore it further.</p>
</section>
<section id="acknowledgements" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h2>
<p>Thank you to <a href="https://costa.sh/">Costa Huang</a> who helped me debug my code and providing feedback on the blog post. Thank you to <a href="https://twitter.com/johnowhitaker">Jonathan Whitaker</a> and inox/hayley for providing feedback on my blog post. Thank you to <a href="https://twitter.com/shah_bu_land">Shahbuland Matiana</a> who I worked closely with on DRLX.</p>


</section>

 ]]></description>
  <category>deep learning</category>
  <category>diffusion models</category>
  <category>reinforcement learning</category>
  <guid>https://www.tanishq.ai/blog/posts/ddpo.html</guid>
  <pubDate>Fri, 22 Sep 2023 07:00:00 GMT</pubDate>
  <media:content url="https://www.tanishq.ai/blog/posts/image-3.png" medium="image" type="image/png" height="132" width="144"/>
</item>
<item>
  <title>Gradio + HuggingFace Spaces: A Tutorial</title>
  <dc:creator>Tanishq Mathew Abraham, Ph.D.</dc:creator>
  <link>https://www.tanishq.ai/blog/posts/2021-11-16-gradio-huggingface.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>After you train a machine learning model, the next thing to do is showcase it to the world by making a demo. Currently, the easiest way to do so is with <a href="https://gradio.app">Gradio</a>, hosting on <a href="https://huggingface.co/spaces">HuggingFace Spaces</a>. With the Gradio framework deployed on Spaces, it takes &lt;10 minutes to deploy a model! Let’s see how we can easily deploy a model for the world to try out with these platforms. We will use a classic CNN pet classifier as an example.</p>
</section>
<section id="preliminaries-training-a-pet-classifier" class="level1">
<h1>Preliminaries: Training a pet classifier</h1>
<p>Before we make a demo, we need to have a model to actually demo! Let’s quickly train a simple ResNet50 pet classifier on the Oxford Pets dataset using fastai.</p>
<div id="1d9d755d-56f3-4578-a14a-b0a787a32a7d" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-2">path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> untar_data(URLs.PETS)</span>
<span id="cb1-3">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ImageDataLoaders.from_name_re(path, get_image_files(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'images'</span>), pat<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'(.+)_\d+.jpg'</span>, item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">460</span>), batch_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>aug_transforms(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span>, min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>))</span>
<span id="cb1-4">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, models.resnet50, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>accuracy)</span>
<span id="cb1-5">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-6">learn.path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span>
<span id="cb1-7">learn.export()</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.973277</td>
<td>0.309940</td>
<td>0.905954</td>
<td>00:32</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.420781</td>
<td>0.260167</td>
<td>0.910690</td>
<td>00:34</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>And with fastai, it’s that simple! Learn more about fastai, a simple and flexible PyTorch training framework, over <a href="https://docs.fast.ai">here</a>.</p>
</section>
<section id="using-gradio" class="level1">
<h1>Using Gradio</h1>
<p>Let’s see how to make a demo web app with Gradio. First let’s load our model:</p>
<div id="35a7b3f3-7c66-4e2c-b0a0-1a6429a71eee" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_learner(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'export.pkl'</span>)</span></code></pre></div>
</div>
<p>Next, let’s define a prediction function our model:</p>
<div id="079ffc04-a87d-4599-b9f9-df6efb39fae6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.dls.vocab</span>
<span id="cb3-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> predict(img):</span>
<span id="cb3-3">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PILImage.create(img)</span>
<span id="cb3-4">    pred,pred_idx,probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.predict(img)</span>
<span id="cb3-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {labels[i]: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(probs[i]) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(labels))}</span></code></pre></div>
</div>
<p>Finally, let’s import Gradio and use it’s functionality to make an interface and launch it. Note that if you are doing this from a notebook, the Gradio demo will also show up within the notebook for you to try interactively (here I just show screenshots).</p>
<div id="82a95244-afb9-4e0b-92bc-a1f3c38da404" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gradio <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> gr</span>
<span id="cb4-2">gr.Interface(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>predict, inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>gr.inputs.Image(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>)), outputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>gr.outputs.Label(num_top_classes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)).launch(share<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<pre><code>Running on local URL:  http://127.0.0.1:7860/
Running on public URL: https://10290.gradio.app

This share link will expire in 72 hours. To get longer links, send an email to: support@gradio.app</code></pre>
<p><img src="https://www.tanishq.ai/blog/posts/gradio_frame_1.png" class="img-fluid"></p>
<pre><code>(&lt;Flask 'gradio.networking'&gt;, 'http://127.0.0.1:7860/', 'https://10290.gradio.app')</code></pre>
<p>That’s it! The actual creation of the demo takes one line!<sup>1</sup></p>
<p>All Gradio interfaces are created by constructing a <code>gradio.Interface()</code> object. As you can see in this example, the <code>Interface</code> object takes in the function that we want to make an interface for (usually an ML model inference function), Gradio input components (the number of input components should match the number of parameters of the provided function), and Gradio output components (the number of output components should match the number of values returned by the provided function). Gradio provides components for various types of input and output types. This includes: images (upload, draw, or webcam), video, audio (upload or microphone), textboxes, dataframes, timeseries, generic files, and more! So you should be able to create a Gradio demo for virtually any type of ML task you can think of!</p>
<p>After the <code>gradio.Interface()</code> object is defined, the interface is launched with the <code>launch</code> method.</p>
</section>
<section id="optional-customizing-our-gradio-app" class="level1">
<h1>Optional: customizing our Gradio app</h1>
<p>Gradio has lots of features that we can use to customize our app. Let’s go over a few of these features and add them to our demo. All of these features are arguments for the instantiation of the <code>Interface</code> class.</p>
<p>First of all, we can pass in a title and description for our app which goes at the top before our input and output components:</p>
<div id="52e1096f-0bfa-4aab-b4b9-5c126cd2344c" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">title <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Pet Breed Classifier"</span></span>
<span id="cb7-2">description <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A pet breed classifier trained on the Oxford Pets dataset with fastai. Created as a demo for Gradio and HuggingFace Spaces."</span></span></code></pre></div>
</div>
<p>We can also put a link at the bottom of our demo. Here I will link to this blog post:</p>
<div id="5669c0d2-d588-4a46-9a81-65d8adffab54" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">article<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;p style='text-align: center'&gt;&lt;a href='https://tmabraham.github.io/blog/gradio_hf_spaces_tutorial' target='_blank'&gt;Blog post&lt;/a&gt;&lt;/p&gt;"</span></span></code></pre></div>
</div>
<p>We can also provide some example inputs that people can try out. Here I have provided an example Siamese cat image, which is in the same directory as my code:</p>
<div id="de1b6b39-ad87-4658-9087-297ef2d67c0c" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">examples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'siamese.jpg'</span>]</span></code></pre></div>
</div>
<p>Another interesting feature that Gradio has is the ability for interpretation so that users can understand what parts of the input are responsible for the output. We’ll use the default interpretation function provided by Gradio but you can use your own as well:</p>
<div id="26af1ccc-345d-42c3-b632-15f559ebf561" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">interpretation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span></span></code></pre></div>
</div>
<p>Note that the default interpretation function needs <code>scikit-image</code> to be installed. More information on the interpretation feature is provided <a href="https://gradio.app/advanced_features/">here</a>.</p>
<p>Gradio also provides a screenshotting feature that can make it really easy to share your examples and results with others. It is enabled by default.</p>
<p>Finally, Gradio also supports serving of inference requests with a queue. This can be helpful when your app receives a significant amount of traffic. We’ll enable a queue here:</p>
<div id="66b253f5-8d16-4667-9147-9ab0fa746445" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">enable_queue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span></code></pre></div>
</div>
<p>You can also add custom CSS for your Gradio app but we’ll not do that here (my CSS skills are essentially non-existent! 😂). Additionally, you can set <code>live=True</code> so that it will automatically submit when you make a change to the input, but removes the Submit button so I won’t use it for now.</p>
<p>Let’s put it all together and make our interface with these additional features:</p>
<div id="feb4710b-fa55-4e18-84c9-b8d2d9cca7e3" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">gr.Interface(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>predict,inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>gr.inputs.Image(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>)),outputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>gr.outputs.Label(num_top_classes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>),title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>title,description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>description,article<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>article,examples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>examples,interpretation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>interpretation,enable_queue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>enable_queue).launch(share<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<pre><code>Running on local URL:  http://127.0.0.1:7861/
Running on public URL: https://30513.gradio.app

This share link will expire in 72 hours. To get longer links, send an email to: support@gradio.app</code></pre>
<p><img src="https://www.tanishq.ai/blog/posts/gradio_frame_2.png" class="img-fluid"></p>
<pre><code>(&lt;Flask 'gradio.networking'&gt;,
 'http://127.0.0.1:7861/',
 'https://30513.gradio.app')</code></pre>
<p>Check the Gradio <a href="https://gradio.app/docs">documentation</a> for more information on how to customize your interface.</p>
<p>Let’s put it all into one file which we name <code>app.py</code>:</p>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gradio <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> gr</span>
<span id="cb15-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb15-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> skimage</span>
<span id="cb15-4"></span>
<span id="cb15-5">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_learner(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'export.pkl'</span>)</span>
<span id="cb15-6"></span>
<span id="cb15-7">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.dls.vocab</span>
<span id="cb15-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> predict(img):</span>
<span id="cb15-9">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PILImage.create(img)</span>
<span id="cb15-10">    pred,pred_idx,probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.predict(img)</span>
<span id="cb15-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {labels[i]: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(probs[i]) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(labels))}</span>
<span id="cb15-12"></span>
<span id="cb15-13">title <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Pet Breed Classifier"</span></span>
<span id="cb15-14">description <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A pet breed classifier trained on the Oxford Pets dataset with fastai. Created as a demo for Gradio and HuggingFace Spaces."</span></span>
<span id="cb15-15">article<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;p style='text-align: center'&gt;&lt;a href='https://tmabraham.github.io/blog/gradio_hf_spaces_tutorial' target='_blank'&gt;Blog post&lt;/a&gt;&lt;/p&gt;"</span></span>
<span id="cb15-16">examples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'siamese.jpg'</span>]</span>
<span id="cb15-17">interpretation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span></span>
<span id="cb15-18">enable_queue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb15-19"></span>
<span id="cb15-20">gr.Interface(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>predict,inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>gr.inputs.Image(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>)),outputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>gr.outputs.Label(num_top_classes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>),title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>title,description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>description,article<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>article,examples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>examples,interpretation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>interpretation,enable_queue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>enable_queue).launch()</span></code></pre></div>
<p>Let’s also make a <code>requirements.txt</code> file which will allow us to install the packages that we need in whatever environment we need:</p>
<pre><code>fastai
scikit-image</code></pre>
<p>Now that we have our self-contained web app, we could deploy this on any webserver or cloud platform that we want. But let’s see how we can use HuggingFace Spaces to deploy it.</p>
</section>
<section id="using-huggingface-spaces" class="level1">
<h1>Using HuggingFace Spaces</h1>
<p><a href="https://huggingface.co/spaces">HuggingFace Spaces</a> is a free-to-use platform for hosting machine learning demos and apps. The Spaces environment provided is a CPU environment with 16 GB RAM and 8 cores. It currently supports the Gradio and Streamlit platforms. Here we will make a Space for our Gradio demo.</p>
<p>In order to be able to create a HuggingFace Space, you need to have a HuggingFace account. You can sign up for free <a href="https://huggingface.co/join">here</a>. After signing up, you can create a Space by clicking “New Space” on the navigation menu (press on your profile image).</p>
<p><img src="https://www.tanishq.ai/blog/posts/create_spaces.png" class="img-fluid"></p>
<p>Now you will be shown instructions on how to add your code to this Space from the command line to prepare the demo. Spaces are essentially git repositories (like GitHub) with an <code>app.py</code> file from which the demo is prepared.</p>
<p>So we can clone the repository to a local directory,</p>
<pre><code>git clone https://huggingface.co/spaces/tmabraham/fastai_pet_classifier</code></pre>
<p>add the <code>app.py</code>, <code>requirements.txt</code>, <code>export.pkl</code>, and <code>siamese.jpg</code> files,</p>
<pre><code>cp app.py fastai_pet_classifier/app.py
cp requirements.txt fastai_pet_classifier/requirements.txt
cp export.pkl fastai_pet_classifier/export.pkl
cp siamese.jpg fastai_pet_classifier/siamese.jpg</code></pre>
<p>Now before we commit our files, there is something we need to pay attention to. Our model file <code>export.pkl</code> is too big to be handled by <code>git</code>. So instead we need to use <a href="https://git-lfs.github.com">git-lfs</a> which you first need to install. If you are on Debian or Ubuntu, you can directly use <code>apt-get install git-lfs</code> (which installs an older version but that’s not really an issue). For other Linux distros, you can use <a href="https://gist.github.com/jph00/361a9b868aa3593f3fd8e930d0221266">this script</a> which <a href="https://twitter.com/jeremyphoward">Jeremy Howard</a> has prepared. For Windows, you can download and run the installer from <a href="https://github.com/git-lfs/git-lfs/releases">here</a>. For MacOS, you can do <code>brew install git-lfs</code>.</p>
<p>Once you have installed git-lfs, you can then initialize git-lfs in the repository for the app in the following way:</p>
<pre><code>git lfs install
git lfs track "*.pkl"
git add .gitattributes
git commit -m "update .gitattributes so git lfs will track .pkl files"</code></pre>
<p>Now, we can commit and push the changes to the Space.</p>
<pre><code>git commit -am "let's deploy to huggingface spaces"
git push</code></pre>
<p><strong>Alternatively</strong>, the files can be uploaded via the Spaces UI. When you go to your Space, under “Files and versions”, there is an “Add files” button which you can use to upload your app files.</p>
<p>After a few moments, during which the app is being built, our demo should show up on the HuggingFace Space.</p>
<p>That’s it! In a few minutes, you trained a pet classifier model with fastai, made a demo interface with Gradio, and hosted it for free on a HuggingFace Space! You can try it out right below or you can try it out on HuggingFace Spaces <a href="https://huggingface.co/spaces/tmabraham/fastai_pet_classifier">here</a>. All the files described in this post located <a href="https://huggingface.co/spaces/tmabraham/fastai_pet_classifier/tree/main">here</a>).<sup>2</sup></p>
<div id="f4cb4ce5-acd8-43a7-ab76-af3089786a55" class="cell" data-execution_count="19">
<div class="cell-output cell-output-display">

        <iframe width="900" height="900" src="https://hf.space/embed/tmabraham/fastai_pet_classifier/+" frameborder="0" allowfullscreen=""></iframe>
        
</div>
</div>
<p>If you are a more advanced user with expertise in web development, you might be interested to know that there is an API available for any Gradio interface (there is a “view the api” link at the bottom of the interface). For example, <a href="https://hf.space/embed/tmabraham/fastai_pet_classifier/api">here</a> is a link to the API docs for my interface. This provides much more flexibility, like interacting with your model very easily in code. For example, here I can take any image URL and get a pet breed prediction with my model.</p>
<div id="3c26fe7c-c3ff-44c3-84fc-219f865f3996" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> requests</span>
<span id="cb21-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gradio <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> gr</span>
<span id="cb21-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> IPython.display <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Image</span>
<span id="cb21-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> IPython.core.display <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> HTML </span>
<span id="cb21-5">image_url <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://petkeen.com/wp-content/uploads/2021/05/grey-cat.jpeg'</span></span>
<span id="cb21-6">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gr.processing_utils.encode_url_or_file_to_base64(image_url)</span>
<span id="cb21-7">r <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> requests.post(url<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://hf.space/embed/tmabraham/fastai_pet_classifier/+/api/predict/'</span>, json<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"data"</span>:[data]})</span>
<span id="cb21-8"></span>
<span id="cb21-9"></span>
<span id="cb21-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"The breed of this pet is a </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' '</span>.join(r.json()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'label'</span>].split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_'</span>)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:"</span>)</span>
<span id="cb21-11">display(Image(url<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>image_url, width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">475</span>))</span>
<span id="cb21-12"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Original JSON returned from the request: '</span>, json.dumps(r.json(), indent<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The breed of this pet is a British Shorthair:</code></pre>
</div>
<div class="cell-output cell-output-display">
<img src="https://petkeen.com/wp-content/uploads/2021/05/grey-cat.jpeg" width="475">
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Original JSON returned from the request:  {
  "data": [
    {
      "label": "British_Shorthair",
      "confidences": [
        {
          "label": "British_Shorthair",
          "confidence": 0.9997965693473816
        },
        {
          "label": "Russian_Blue",
          "confidence": 0.00019805884221568704
        },
        {
          "label": "Sphynx",
          "confidence": 2.037774265772896e-06
        }
      ]
    }
  ],
  "flag_index": null,
  "updated_state": null,
  "durations": [
    0.09037947654724121
  ],
  "avg_durations": [
    0.13969146820806688
  ]
}</code></pre>
</div>
</div>
<p>Some examples of using the API in custom websites is provided <a href="https://fastai.github.io/tinypets/">here</a> (put together by Jeremy Howard and members of the fast.ai community).</p>
<p>For more information on Gradio and HuggingFace Spaces, check the relevant docs and forums:</p>
<ul>
<li><a href="https://gradio.app/docs/">Gradio documentation</a></li>
<li><a href="https://huggingface.co/docs/hub/spaces">HuggingFace Spaces documentation</a></li>
<li><a href="https://gradio.app/guides/">Gradio Guides</a></li>
<li><a href="https://discuss.huggingface.co/">HuggingFace Forums (for Spaces and Gradio Q&amp;A)</a></li>
</ul>
<p>There are so many features of Gradio and Spaces that I haven’t mentioned here (like multiple models per demo, the Blocks feature, etc.). Additionally, both Gradio and HuggingFace Spaces are in active development and new, amazing features afe always being added by tje Gradio and HuggingFace teams! For this reason, I also recommend following <a href="https://twitter.com/huggingface">HuggingFace</a> and <a href="https://twitter.com/gradio">Gradio</a> on Twitter to hear about the latest updates and newest features.</p>
<p>I’ll end by sharing a quick example prediction by my pet classifier of our new kitten! Her name is Mimi and, as predicted by my classifier here, she is indeed a Ragdoll kitten!:</p>
<p><img src="https://www.tanishq.ai/blog/posts/gradio_mimi.png" class="img-fluid"></p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>Thanks to Zach Mueller, Ahsen Khaliq, Abhishek Thakur, and Jeremy Howard for reviewing my blog post.</p>
</section>
<section id="footnotes" class="level1">
<h1>Footnotes</h1>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>One of the developers of Gradio created a simple Python module to easily create Gradio demos for fastai <code>Learner</code> objects. Check it out <a href="https://github.com/aliabd/fastgradio">here</a>. It currently only supports image-to-label interfaces but it could likely be expanded to other tasks fairly easily.↩︎</p></li>
<li id="fn2"><p>Recently, HuggingFace <a href="https://github.com/huggingface/huggingface_hub/pull/678">added</a> direct support for pushing and loading fastai models to the HuggingFace Hub with the <code>push_to_hub_fastai</code> and <code>from_pretrained_fastai</code> functions, respectively. This can make creating Spaces much easier, since you can just load it in the Space and not have to add it to the repository with <code>git-lfs</code>. See an example of this over <a href="https://huggingface.co/spaces/espejelomar/cat_or_dog_fastai/blob/main/app.py">here</a>.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>deep learning</category>
  <guid>https://www.tanishq.ai/blog/posts/2021-11-16-gradio-huggingface.html</guid>
  <pubDate>Tue, 16 Nov 2021 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Coding with GitHub Copilot</title>
  <dc:creator>Tanishq Mathew Abraham, Ph.D.</dc:creator>
  <link>https://www.tanishq.ai/blog/posts/2021-07-14-GitHub-Copilot.html</link>
  <description><![CDATA[ 





<section id="coding-with-github-copilot" class="level1">
<h1>Coding with GitHub Copilot</h1>
<p>On July 1st, I was able to obtain access to GitHub Copilot, thanks to <a href="https://twitter.com/hamelhusain">Hamel Husain</a>. I wanted to share my experience and discoveries about this new tool. Much of the findings was demonstrated with the help of Mazen Alotaibi, Ryan Panwar, and Mark Saroufim.</p>
<section id="what-is-github-copilot" class="level2">
<h2 class="anchored" data-anchor-id="what-is-github-copilot">What is GitHub Copilot?</h2>
<section id="github-copilot-is-a-tool-that-helps-you-to-code-faster" class="level3">
<h3 class="anchored" data-anchor-id="github-copilot-is-a-tool-that-helps-you-to-code-faster">GitHub Copilot is a tool that helps you to code faster</h3>
<p>If you haven’t logged onto Twitter or Hacker News in the last couple weeks, you might not know about <a href="https://copilot.github.com">GitHub Copilot</a>. Developed out of a partnership between OpenAI and Microsoft (GitHub’s parent company), it’s an AI-based autocomplete tool that helps you to write code faster. The GitHub team has termed it “your AI pair programmer”. OpenAI CTO Greg Brockman has explained that it utilizes the currently-unreleased Codex model, which is apparently a successor to the (in)famous GPT-3 language model. It has been trained on billions of lines of code available on GitHub <sup>1</sup>.</p>
<p>Based on the demos that GitHub Copilot provided and favorable reviews from beta-testers, I was eager to give it a try, but I was also skeptical if it really was as life-changing as people claimed it was. To my surprise, it was much better than I expected.</p>
<p>Here is a demo of GitHub Copilot in action (specifically for an ML-related task):</p>
<center>
{{ tweet iScienceLuvr 1411074516411764743 }}
</center>
<p>It’s clear that GitHub Copilot understands the general PyTorch training workflow, and understands intricacies like what are the appropriate augmentations for images (resizing, random crop, normalization, etc.), making sure to put model into evaluation mode and with <code>torch.no_grad()</code> during validation, etc. These are things that sometimes we may forget to do, so it’s great that GitHub Copilot can help prevent us from making these common mistakes.</p>
<p>GitHub Copilot performs best when you provide it with comments describing what you are trying to do. It then uses the comments to generate a list of possible completions. This is highlighted in the example above, where I wrote a few lines about what I wanted to do (fine-tuning a pretrained ResNet50 on a custom dataset) and how I wanted to do that, and it mostly completed the rest of the code for me. I think this is great, because it changes the way we code. It now drives code development to focus on documentation, since writing good documentation often results in better Copilot suggestions.</p>
<p>On a related note, some have hypothesized that GitHub Copilot might also lead to more test-driven development:</p>
<center>
<img src="https://www.tanishq.ai/blog/posts/image-4.png" class="img-fluid">
</center>
<p>I also want to point out that while most demos directly use GitHub Copilot in the editor, it’s also possible to open GitHub Copilot in a separate tab and have it generate and present multiple suggestions for you. Here’s an example:</p>
<center>
<video width="528" height="310" controls="">
<source src="https://i.imgur.com/ah49d8V.mp4" type="video/mp4">
</video>
</center>
<p>I quite like this feature, because it provides various approaches for solving a particular task, and I can select which approach I want to use. For instance, in the above example, it shows various approaches for defining a ResNet50 model for fine-tuning. I typically prefer defining a class for the ResNet50, so I select that option.</p>
<p>There is another unintended consequence of GitHub Copilot that I find interesting. GitHub Copilot actually makes a pretty good autocomplete tool for regular writing. I actually discovered this when I started writing this blog post in a Markdown file in the VS Code editor. Of course, this is likely GitHub Copilot learning from README files and other documentation in various repositories, and there could be some residual general knowledge from the underlying GPT-3 model (if that is indeed the base model used) <sup>2</sup>. But I would genuinely consider writing more in Markdown files with VS Code + GitHub Copilot because some of the autocomplete suggestions are actually quite helpful.</p>
</section>
</section>
<section id="challenges-with-github-copilot" class="level2">
<h2 class="anchored" data-anchor-id="challenges-with-github-copilot">Challenges with GitHub Copilot</h2>
<p>There are several challenges that I think could preclude widespread use of GitHub Copilot:</p>
<ol type="1">
<li>Leaking of personal information</li>
<li>Limited multi-lingual capabilities</li>
<li>Copyright/licensing issues</li>
<li>Usage of outdated APIs</li>
</ol>
<p>Let’s dive into each of these issues further.</p>
<section id="personal-information-shared-by-github-copilot" class="level3">
<h3 class="anchored" data-anchor-id="personal-information-shared-by-github-copilot">Personal information shared by GitHub Copilot</h3>
<p>One aspect we discovered was that GitHub Copilot would inadvertantly share information that would be considered personal, such as people’s names, phone numbers, emails, etc. This was something Mazen and I explored further. Here are a few examples of this.</p>
<p>In a Python file, simply asking it to create a function to list author names indeed gives the name of a person that exist: <img src="https://media.discordapp.net/attachments/806360771038019669/860317676390580224/unknown.png" class="img-fluid"></p>
<p>Mark demonstrated an example when writing a bash script when an actual person’s name was suggested in an autocompletion <a href="https://www.twitch.tv/marksaroufim/clip/ScrumptiousTangiblePastaDogFace-2bOqEL6P5pYl_ALK">here</a>.</p>
<p>Interestingly, this method did not work for returning other types of information like phone numbers:</p>
<p><img src="https://media.discordapp.net/attachments/806360771038019669/860319889410752542/unknown.png" class="img-fluid"></p>
<p><img src="https://media.discordapp.net/attachments/806360771038019669/860321195526193162/unknown.png" class="img-fluid"></p>
<p>But if we just ask GitHub Copilot to autocomplete phone number in a comment at the beginning of a Python file, it does work:</p>
<p><img src="https://media.discordapp.net/attachments/806360771038019669/860322962472042536/unknown.png" class="img-fluid"></p>
<p>Mazen looked more into this number, and found out it was used in several GitHub repositories, including a programming example problem <a href="https://github.com/krelly/codewars/blob/7f9a46c845c5918856bb8e740588519dbbbd1b26/5-kyu/phone-directory/README.md">here</a>.</p>
<p>Mark also discovered that working API keys were provided by GitHub Copilot: <img src="https://media.discordapp.net/attachments/806360771038019669/861813846896017408/unknown.png" class="img-fluid"></p>
<p>Interestingly, from my experiments, I was not able to get GitHub Copilot to leak any e-mail addresses.</p>
<p>On their website, GitHub Copilot has the following information:</p>
<p><img src="https://i.imgur.com/fB33Ofo.png" class="img-fluid"></p>
<p>So this confirms that indeed private information was available in the training set that allows GitHub Copilot to leak this information. I was unable to easily get email addresses because of the rudimentary filtering that GitHub Copilot performed.</p>
</section>
<section id="multi-lingual-capabilities-of-github-copilot" class="level3">
<h3 class="anchored" data-anchor-id="multi-lingual-capabilities-of-github-copilot">Multi-lingual capabilities of GitHub Copilot</h3>
As we mentioned before, GitHub Copilot performs best when you provide it with comments explaining your intent. Therefore, Mazen and I wanted to explore how well GitHub Copilot can perform with comments in various languages. I have used Google Translate to translate my English comments to various languages and observe how well it performed. Let’s go over an example. Below, I give GitHub Copilot the prompt to “Add two numbers” and see what Python code it suggests:
<style>
    table {
    text-align: center
    }
</style>
<pre><code>&lt;/tr&gt;</code></pre><pre><code>&lt;/tr&gt;</code></pre><pre><code>&lt;/tr&gt;</code></pre><pre><code>&lt;/tr&gt;</code></pre><table>
<thead>
<tr>
<th>
English
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<video width="528" height="310" controls="">
<source src="https://i.imgur.com/tcyHz3S.mp4" type="video/mp4">
</video>
</td>

</tr></tbody>
<thead>
<tr>
<th>
Mandarin
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<video width="528" height="310" controls="">
<source src="https://i.imgur.com/4BNewWz.mp4" type="video/mp4">
</video>
</td>

</tr></tbody>
<thead>
<tr>
<th>
Spanish
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<video width="528" height="310" controls="">
<source src="https://i.imgur.com/Fko6cBm.mp4" type="video/mp4">
</video>
</td>

</tr></tbody>
<thead>
<tr>
<th>
Arabic
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<video width="528" height="310" controls="">
<source src="https://i.imgur.com/MY8rJ0A.mp4" type="video/mp4">
</video>
</td>

</tr></tbody>
</table>
<p>Of course, if you comment with English, GitHub Copilot provides a good suggestion. It gives us an adding function as well as some use-cases. But as demonstrated in these experiments, the quality of GitHub Copilot suggestions when given comments in other languages likely is correlated with the overall frequency of these languages in the training data. It’s likely that Mandarin and Spanish is more common than Arabic in the training set, so GitHub Copilot performs better with Mandarin and Spanish comments. Of course, this is a single example (although I observed similar results with other prompts). However, given that it’s well-established that biases in the training data are reflected in the output of any ML algorithm (unless it is appropriately counteracted), I think it is safe to assume that GitHub Copilot will likely be less useful for non-English-speaking users.</p>
</section>
<section id="copyrightlicensing-issues" class="level3">
<h3 class="anchored" data-anchor-id="copyrightlicensing-issues">Copyright/licensing issues</h3>
<p>Let’s move on to the elephant in the room: copyright/licensing issues. GitHub Copilot/Codex was trained on all public GitHub code, regardless of license (<a href="https://twitter.com/NoraDotCodes/status/1412741339771461635">confirmed</a> by GitHub). While <a href="https://juliareda.eu/2021/07/github-copilot-is-not-infringing-your-copyright/">some argue</a> that training on copyrighted code is not an issue, it becomes much more challenging to argue that when Copilot is <a href="https://twitter.com/mitsuhiko/status/1410886329924194309">regurgitating public code verbatim</a> <sup>3</sup>. According to GitHub, Copilot repeats code snippets verbatim about 0.1% of the time. They have also provided a more in-depth study <a href="https://github.co/copilot-research-recitation">here</a>. Thankfully they are currently developing origin tracker that tells where the verbatim code is coming from and allows you to decide whether to include proper attribution or not use that code altogether.</p>
<p>In my opinion, because of these copyright issues, GitHub Copilot in its current state is not usable for commericial purposes. I think that once the origin tracker is released, copyright issues will be resolved, although it puts the onus on the user to make sure that code is properly attributed. Of course, the easier solution would have been to avoid training on copyrighted and GPL-licensed code altogether, which would have likely prevented the significant controversy that arose, and I wonder what led to the decision to train on all public GitHub code instead of further curating the dataset.</p>
</section>
<section id="usage-of-outdated-apis" class="level3">
<h3 class="anchored" data-anchor-id="usage-of-outdated-apis">Usage of outdated APIs</h3>
<p>As an ML researcher and developer, I am typically working with the latest ML frameworks and tools. However, GitHub Copilot is trained on older codebases and does not have knowledge of these cutting-edge tools and is often unable to provide relevant suggestions.</p>
<p>I first discovered this issue when trying to write <a href="https://docs.fast.ai">fastai</a>-related code and get GitHub Copilot to provide relevant suggestions. However, since the latest version of fastai was only released in August 2020, GitHub Copilot was not able to provide any relevant suggestions and instead provided code for using older versions of fastai. This indicates that the codebases that GitHub Copilot is trained on must be at least before August 2020, if not earlier. Similarly, I discovered that GitHub Copilot was unable to provide any suggestions regarding the usage of the <a href="https://github.com/rwightman/pytorch-image-models">timm</a> library, which is one of the leading deep learning+computer vision libraries.</p>
<p>Here is a video that demonstrates this issue:</p>
<center>
<video width="528" height="310" controls="">
<source src="https://i.imgur.com/OH8rtxc.mp4" type="video/mp4">
</video>
</center>
<p>To me, this is a major concern regarding the current usability of GitHub Copilot <sup>4</sup>. If we are using cutting edge tools like PyTorch XLA, JAX, fastai, timm, GitHub Copilot has no knowledge of this and cannot provide useful suggestions. Somehow, the GitHub team needs to keep Copilot updated on newer codebases. Given that <a href="https://docs.github.com/en/github/copilot/about-github-copilot-telemetry">telemetry of GitHub Copilot usage</a> is being sent to GitHub, it’s possible that the GitHub team can further train their model on the usage of these newer codebases. Indeed, it is mentioned in the documentation that the telemetry data is used for “improving the underlying code generation models, e.g.&nbsp;by providing positive and negative examples (but always so that your private code is not used as input to suggest code for other users of GitHub Copilot)”. Additionally, a GitHub Developer Advocate has <a href="https://youtu.be/St2CMvK4hK0?t=257">mentioned</a> that “the model is being trained everyday, so the more people use it, Copilot will learn that these suggestions need to be updated”.</p>
<p>I wonder if the GitHub team might also develop a way of perhaps fine-tuning GitHub Copilot to specific use-cases. For example, there may be a specific GitHub Copilot models for fastai, JAX, etc. They would be fine-tuned on the source code of of these libraries and codebases that use these libraries. But making sure that the tool does not provide outdated suggestions would still be a challenge. I don’t think it would be possible to provide suggestions for a brand-new library that does not have enough codebases using it to train on. Additionally, for situations like fastai where there are older APIs and newer APIs, when fine-tuning a model, the codebases using the older APIs would have to be filtered out.</p>
<p>All in all, I personally think that for practical applications, it is necessary for GitHub Copilot to provide suggestions for new codebases, and doing so might be a difficult but potentially solvable challenge.</p>
</section>
</section>
<section id="how-might-github-copilot-be-commercialized" class="level2">
<h2 class="anchored" data-anchor-id="how-might-github-copilot-be-commercialized">How might GitHub Copilot be commercialized?</h2>
<p>While it is currently available for free to the beta-testers, the GitHub team has already mentioned they plan to commercialize this product. There are several ways that GitHub Copilot could be commercialized:</p>
<ol type="1">
<li>A monthly fee for personal use of a generic GitHub Copilot model</li>
<li>Enterprises paying for a model fine-tuned to their specific, private codebases</li>
<li>Separate fees for domain-specific models (ex: a GitHub Copilot model for writing machine learning code, or a GitHub Copilot model for web development)</li>
</ol>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In conclusion, GitHub Copilot, is a mind-blowing and extremely powerful tool. Additionally, it is a very interesting and practical application of AI. With the domains that it is most familiar, GitHub Copilot works exceptionally well and can write most of the code for you! It may very well change the approach and workflow many programmers have and lead to documentation-driven and test-driven development.</p>
<p>But it’s not yet ready for prime time. There are clear issues with leaking of personal information copyright/licensing issues, accessibility to foreign-language users, and its use on more cutting-edge projects. Thankfully, the GitHub team is working on these issues and I’m excited by the future of AI-augmented programming!</p>
</section>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>Thank you to <a href="https://twitter.com/hamelhusain">Hamel Husain</a> for helping to provide access to the GitHub Copilot tool and also for reviewing the blog post.</p>
<p>Thank you to <a href="https://twitter.com/sudomaze">Mazen Alotaibi</a>, <a href="https://twitter.com/RyanPanwar">Ryan Panwar</a>, and <a href="https://twitter.com/mark_saroufim">Mark Saroufim</a> for sharing their ideas to try with GitHub Copilot and also for reviewing the blog post.</p>
</section>
<section id="footnotes" class="level1">
<h1>Footnotes</h1>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The OpenAI team has recently released a <a href="https://arxiv.org/abs/2107.03374">paper</a> on the Codex model that was trained on Python code, and it is noted that the GitHub Copilot model is a descendant of the one reported in the paper. Importantly, this paper indicates that Codex model is a fine-tuned GPT-3 model. It is likely that the GitHub Copilot version is also a GPT-3 model that is instead fine-tuned on the whole GitHub dataset.↩︎</p></li>
<li id="fn2"><p>This <a href="https://www.youtube.com/watch?v=L6Nr1uc80pY">video</a> demonstrates an example of some of the more general knowledge GitHub Copilot seems to have.↩︎</p></li>
<li id="fn3"><p>Yannic Kilcher provides a nice explanation of the potential copyright/GPL licensing issues over <a href="https://www.youtube.com/watch?v=TrLrBL1U8z0">here</a>.↩︎</p></li>
<li id="fn4"><p>A related issue that many people, including myself, have observed is that sometimes recommendations are for older versions of a programming language, such as providing Python 2.7 suggestions instead of Python 3.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>programming</category>
  <guid>https://www.tanishq.ai/blog/posts/2021-07-14-GitHub-Copilot.html</guid>
  <pubDate>Wed, 14 Jul 2021 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Introducing Noisy Imagenette</title>
  <dc:creator>Tanishq Mathew Abraham, Ph.D.</dc:creator>
  <link>https://www.tanishq.ai/blog/posts/2021-03-02-noisy-imagenette.html</link>
  <description><![CDATA[ 





<p><strong>TL;DR:</strong> We introduce a dataset, Noisy Imagenette, which is a version of the Imagenette dataset with noisy labels. We hope this dataset is useful for rapid experimentation and testing of methods to address noisy label training.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="dataset-have-noisy-labels" class="level2">
<h2 class="anchored" data-anchor-id="dataset-have-noisy-labels">Dataset have noisy labels!</h2>
<p>Deep learning has led to impressive results on datasets of all types, but its success often shines when models are trained with large datasets with human-annotated labels (extreme example: GPT-3 and more recently CLIP/ALIGN/DALL-E). A major challenge when constructing these datasets is obtaining enough labels to train a neural network model. There is an inherent tradeoff between the quality of the annotations and the cost of annotation (in the form of time or money). For example, while using sources like Amazon Mechanical Turk provide cheap labeling, the use of these non-expert labeling services will often produce unreliable labels. This is what is referred to as noisy labels, as these unreliable labels are not necessarily ground truth. Unfortunately, neural networks are known to be susceptible to overfitting to noisy labels (see <a href="https://arxiv.org/abs/1611.03530">here</a>) which means alternative approaches are needed to achieve good generalization in the presence of noisy labels.</p>
</section>
<section id="prior-research-on-noisy-labels" class="level2">
<h2 class="anchored" data-anchor-id="prior-research-on-noisy-labels">Prior research on noisy labels</h2>
<p>Recently, many techniques have been presented in order to address label noise. These include novel loss functions like <a href="https://arxiv.org/abs/1906.03361">Bi-Tempered Logistic Loss</a><a href="https://www.ijcai.org/Proceedings/2020/305">Taylor Cross Entropy Loss</a>, or <a href="https://arxiv.org/abs/1908.06112">Symmetric Cross Entropy</a>. Additionally, there are many novel training techniques that have been recently developed like <a href="https://arxiv.org/abs/1911.09781">MentorMix</a>, <a href="https://arxiv.org/abs/2002.07394">DivideMix</a>, <a href="https://arxiv.org/abs/2007.00151">Early-Learning Regularization</a> and <a href="https://openreview.net/forum?id=D1E1h-K3jso">Noise-Robust Contrastive Learning</a>.</p>
<p>Most of these papers are using MNIST, SVHN, CIFAR10 or related datasets with synthetically-added noise. Other common datasets are the WebVision and Clothing1M datasets, which are real-world noisy, large-scale datasets with millions of images. Therefore there is an opportunity to develop a mid-scale dataset that allows for rapid prototyping but is complex enough to provide useful results when it comes to noisy label training.</p>
</section>
<section id="fastais-imagenette---a-dataset-for-rapid-prototyping" class="level2">
<h2 class="anchored" data-anchor-id="fastais-imagenette---a-dataset-for-rapid-prototyping">fastai’s Imagenette - a dataset for rapid prototyping</h2>
<p>The idea of mid-scale datasets for rapid prototyping has been explored in the past. For example, in 2019, fast.ai <a href="https://github.com/fastai/imagenette">released</a> the Imagenette and Imagewoof datasets (subsequently updated in 2020), subsets of Imagenet for rapid experimentation and prototyping. It can serve as a small dataset proxy for the ImageNet, or a dataset with more complexity than MNIST or CIFAR10 but still small and simple enough for benchmarking and rapid experimentation. This dataset has been used to test and establish new training techniques like <a href="https://arxiv.org/abs/1908.08681">Mish activation function</a> and <a href="https://forums.fast.ai/t/meet-ranger-radam-lookahead-optimizer/52886">Ranger optimizer</a> (see <a href="https://forums.fast.ai/t/how-we-beat-the-5-epoch-imagewoof-leaderboard-score-some-new-techniques-to-consider/53453">here</a>). The dataset also has been used in various papers (see <a href="https://arxiv.org/abs/2004.07629">here</a>, <a href="https://arxiv.org/abs/2007.15248">here</a>, <a href="https://arxiv.org/abs/1906.04887">here</a>, <a href="https://arxiv.org/abs/2101.06639">here</a>, <a href="https://arxiv.org/abs/2006.05624">here</a>, and <a href="https://www.sciencedirect.com/science/article/pii/S1047320321000134?casa_token=uL4_SoQQgKsAAAAA:CPGu3HeZVciBO5YEocTnziH7YVhbcGF0JCpB0JuJi2pqHmkaAKibhaVYe-3t07nxtpdem2lv">here</a>). Clearly, this dataset has been quite useful to machine learning researchers and practitioners for testing and comparing new methods. We believe that an analogous dataset could be useful to researchers with modest compute for testing and comparing new methods for addressing label noise.</p>
</section>
</section>
<section id="introducing-noisy-imagenette" class="level1">
<h1>Introducing Noisy Imagenette</h1>
<p>We introduce Noisy Imagenette, a version of Imagenette (and Imagewoof) that has synthetically noisy labels at different levels: 1%, 5%, 25%, and 50% incorrect labels. The Noisy Imagenette dataset already comes with the Imagenette dataset:</p>
<div id="cell-3" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-2">source <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> untar_data(URLs.IMAGENETTE)</span></code></pre></div>
</div>
<p>While the regular labels for Imagenette dataset are given as the names of the image folder, the noisy labels are provided as a separate CSV file with columns corresponding to the image filename and labels for each of the different noise levels:</p>
<div id="cell-5" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">csv_file <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(source<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'noisy_imagenette.csv'</span>)</span>
<span id="cb2-2">csv_file.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">path</th>
<th data-quarto-table-cell-role="th">noisy_labels_0</th>
<th data-quarto-table-cell-role="th">noisy_labels_1</th>
<th data-quarto-table-cell-role="th">noisy_labels_5</th>
<th data-quarto-table-cell-role="th">noisy_labels_25</th>
<th data-quarto-table-cell-role="th">noisy_labels_50</th>
<th data-quarto-table-cell-role="th">is_valid</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>train/n02979186/n02979186_9036.JPEG</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>train/n02979186/n02979186_11957.JPEG</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n03000684</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>train/n02979186/n02979186_9715.JPEG</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n03417042</td>
<td>n03000684</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>train/n02979186/n02979186_21736.JPEG</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n03417042</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>train/n02979186/ILSVRC2012_val_00046953.JPEG</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n02979186</td>
<td>n03394916</td>
<td>False</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The generation of these noisy labels are provided in <a href="https://github.com/fastai/imagenette/blob/master/noisy_labels/generate_labels.ipynb">this Jupyter notebook</a>. We have also updated fastai’s <a href="https://github.com/fastai/fastai/blob/master/nbs/examples/train_imagenette.py">train_imagenette.py</a> to utilize the new noisy labels. If you want to train on the Noisy Imagenette dataset using this script, just simply pass the <code>--pct-noise</code> argument to the script with the desired noise level.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The validation set remains clean and its labels are not changed. While technically the accuracy metric <a href="https://arxiv.org/abs/2012.04193">is robust</a> to noise, I believe it’s simpler to use a clean validation set to clearly understand see if a model is learning appropriate decision boundaries on the ground truth.</p>
</div>
</div>
<p>For the original Imagenette dataset, there are technically (3 image sizes)*(4 number of epoch levels) for both Imagenette and Imagewoof giving a total of 24 leaderboards. If we had each of these 24 leaderboards for the previously mentioned 4 noise levels (1%, 5%, 25%, 50%), that would give us 96 leaderboards! Instead, the <a href="https://github.com/fastai/imagenette">Imagenette repository</a> only maintains leaderboards for Noisy Imagenette (and not Imagewoof) for 5% and 50% noise (24 leaderboards). Just like with the regular Imagenette leaderboards, feel free to send a pull request to the Imagenette repository with your results if it beats the current top score. I have provided a <a href="https://github.com/tmabraham/noisy_imagenette/blob/main/baseline/baseline-01-18-2021.md">baseline</a> which is currently on the leaderboards, as well as a <a href="https://github.com/fastai/imagenette/blob/master/noisy_labels/extended_lb.csv">CSV file</a> with the baseline accuracy for all 96 leaderboards.</p>
</section>
<section id="backstory" class="level1">
<h1>Backstory</h1>
<p>For some background, I started looking into training with label noise because of the recent Cassava Leaf Disease Kaggle Competition (my team was able win a silver medal, see <a href="https://twitter.com/iScienceLuvr/status/1362879523650330627">here</a>), which had a really noisy dataset. One of the recent techniques I heard about was <a href="https://arxiv.org/abs/2010.01412">SAM</a>, which recently achieved a state-of-the-art score on ImageNet (only to be beaten in a few weeks by techniques/models like Meta Pseudo Labels and NFNets). However, the paper also included some improvements to noisy label training. I had a fastai implementation in-progress for the SAM optimizer (probably will describe in an upcoming blog post) and I wanted to test out its noisy label training capabilities on a dataset. I thought about corrupting the Imagenette labels and use that as my dataset for testing SAM. Jeremy Howard suggested adding it to the main Imagenette dataset and here we are!</p>
</section>
<section id="closing-remarks" class="level1">
<h1>Closing Remarks</h1>
<p>In conclusion, I hope that this Noisy Imagenette datasets serves as a useful benchmarking dataset for machine learning community when it comes to testing and comparing techniques for training on noisy labels. I hope to experiment with some of these techniques like SAM, the different loss functions, etc. and record those results over on this blog, so be sure to keep an eye on this blog, or follow me on <a href="https://twitter.com/iScienceLuvr">Twitter</a> to get the latest updates!</p>
</section>
<section id="acknowledgments" class="level1">
<h1>Acknowledgments</h1>
<p>I’d like to thank Jeremy Howard and especially Hamel Husain for adding the Noisy Imagenette dataset. I also would like to thank Hamel Husain for reviewing my blog post and providing feedback. I’d like to thank Isaac Flath for pointing out an error I originally had when generating the dataset.</p>


</section>

 ]]></description>
  <category>deep learning</category>
  <category>imagenette</category>
  <guid>https://www.tanishq.ai/blog/posts/2021-03-02-noisy-imagenette.html</guid>
  <pubDate>Tue, 02 Mar 2021 08:00:00 GMT</pubDate>
</item>
</channel>
</rss>
